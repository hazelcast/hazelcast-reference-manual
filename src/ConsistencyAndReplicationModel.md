# Consistency and Replication Model

Partitioning and replication are the two common techniques used together in distributed databases to achieve scalable, available and transparent data distribution. The data space is divided into partitions, each of which contains a distinct portion of the overall data set. For these partitions, multiple copies are created, called replicas. Partition replicas are distributed among the cluster nodes. Each node is assigned to at most single replica for a partition. In this setting, different replication techniques can be used to access the data and keep the replicas in sync on updates. The technique being used directly affects the guarantees and properties the distributed data store provides, due to the CAP principle.

For instance, primary-copy systems access to the data using an elected replica, which can be called as primary, master, etc. Changes in the data on the primary replica are also propagated to other replicas. This approach has different namings, such as _primary-copy_, _single-master_, _passive replication_. The primary-copy technique is a powerful model as it prevents conflicts, deadlocks among the replicas. However, primary replicas can become bottlenecks. On the other hand, we can have a different technique by eliminating the primary-copy and treating each replica as equal. These systems can achieve a higher level of availability as a data entry can be accessed and updated using any replica. However, it can become more difficult to keep the replicas in sync with each other.

Replication techniques also differ in how updates are propagated among replicas. One option is to update each replica as part of a single atomic transaction, called as eager replication or synchronous replication. Consensus algorithms apply this approach and achieve strong consistency on a replicated data set. The main drawback is the amount of coordination and communication required while running the replication algorithm. CP systems implement consensus algorithms under the hood. Another option is the lazy replication technique, which is also called as asynchronous replication. Lazy replication algorithms execute updates on replicas with separate transactions. They generally work with best-effort. By this way, the amount of coordination among the replicas are degraded and data can be accessed in a more performant manner. Yet, it can happen that a particular update is executed on some replicas but not in others, which will lead replicas to diverge. Such problems can be resolved with different approaches, such as _read-repair_, _write-repair_, _anti-entropy_. Lazy replication techniques are popular among AP systems.

The discussion here generally applies to any system that maintains multiple copies of a data set. It applies to Hazelcast as well. In the context of CAP principle, Hazelcast is an AP product, which employs the combination of primary-copy and lazy replication techniques. As briefly described in Data Partitioning Section, each data entry is mapped to a single Hazelcast partition and put into replicas of that partition. One of the replicas are elected as the primary replica, which is responsible for performing operations on that partition. Backup replicas stay in standby mode until the primary replica fails. Upon failure of the primary replica, one of the backup replicas are promoted to the primary role. When you read or write a map entry, you transparently talk to the Hazelcast node to which primary replica of the corresponding partition is assigned. By this way, each request always hits the most up-to-date version of a particular data entry. Backup replicas can be used to scale reads (see [Enabling Backup Reads](#enabling-backup-reads)) with no strong consistency but monotonic reads guarantee. When the primary replica receives an update request for a key, it executes the update locally, and propagates it to backup replicas. It marks each update with a logical timestamp so that backups apply them in the correct order and converge to the same state with the primary. Thanks to this approach, Hazelcast clusters offer high throughput. However, due to temporary problems in the system, such as network interruption, backup replicas can miss some updates and diverge from the primary, which is a situation called as “replication lag”. As described above, such situations are inherently possible in lazy-replication systems. Hazelcast deals with these scenarios using the anti-entropy solution. Each Hazelcast node runs a periodic task in the background. For each primary replica it is assigned, it creates a summary information and sends it to the backups. Then, each backup node compares the summary information with its own data to see if it is up-to-date with the primary. If a backup node detects a missing update, it triggers the synchronisation process with the primary.

## Invocation Lifecycle

When a write is requested, such as `map.put()` or `queue.offer()`, a write operation is submitted to the Hazelcast node that owns primary replica of the specific partition. Partition of an operation is determined based on a parameter (key of an entry or name of the data structure etc.) related to that operation depending on data structure. Target Hazelcast node is figured out by looking up a local partition assignment/ownership table, which is updated on each partition migration and broadcasted to all cluster eventually.

When a Hazelcast node receives a partition specific operation, it executes the operation and propagates it to backup replica(s) with a logical timestamp. Number of backups for each operation depends on data structure and its configuration. See [Threading Model - Operation Threading](#operation-threading) for threading details.

Two types of backup replication are available; _sync_ and _async_. Despite what their names imply, both types are still implementations of the lazy (async) replication model. The only difference between _sync_ and _async_ is, former makes the caller block until backup updates are applied by backup replicas and acknowledgments are sent back to the caller, but latter is just fire & forget. Number of sync and async backups are defined in data structure configurations and a combination of sync and async backups can be used.

When backup updates are propagated, response of the execution including number of sync backup updates is sent to the caller and after receiving the response, caller waits to receive specified number of sync backup acknowledgements for a predefined timeout. This timeout is 5 seconds by default and defined by system property `hazelcast.operation.backup.timeout.millis`. (See [System Properties](#system-properties) section.)

A backup update can be missed because of a few reasons, such as a stale partition table information on a backup replica node, network interruption, or a member crash. That’s why sync backup acks require a timeout to give up. Regardless of being a sync or async backup, if a backup update misses, the periodically running anti-entropy mechanism detects the inconsistency and synchronizes backup replicas with the primary. Also the graceful shutdown procedure ensures that all backup replicas for partitions whose primary replicas are assigned to the shutting down node will be consistent.

In some cases, although target node of an invocation is assumed to be alive by the failure detector, target may not execute the operation or send response back in time. Network splits, long pauses caused by high load, GC or IO (disk, network) can be listed as a few possible reasons. When an invocation doesn’t receive any response from the node that owns primary replica, then invocation fails with an `OperationTimeoutException`. This timeout is 2 minutes by default and defined by the system property `hazelcast.operation.call.timeout.millis`. (See [System Properties](#system-properties) section.) When timeout happens, result of invocation will be indeterminate.


## Exactly once, At least once or At most once Execution

Hazelcast, as an AP product, does not provide the exactly-once guarantee. In general, Hazelcast tends to be a at-least-once solution.

In the following failure case, exactly-once guarantee can be broken:

* When target node of a pending invocation leaves the cluster while the invocation is waiting for a response, that invocation is re-submitted to its new target due to the new partition table. It can be that, it has already been executed on the leaving node and backup updates are propagated to the backup replicas, but the response is not received by the caller. If that happens, the operation will be executed twice.

In the following failure case, invocation state will become indeterminate:

* As explained above, when an invocation does not receive a response in time, invocation will fail with an `OperationTimeoutException`. This exception does not say anything about outcome of the operation, that means operation may not be executed at all, it may be executed once or twice (due to member left case explained above).
