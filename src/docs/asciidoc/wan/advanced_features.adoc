[[wr-advanced]]
=== Advanced Features

This section describes how you can synchronize your WAN replicated clusters,
change their configurations dynamically and intercept WAN replication events using
the event filtering API.

==== Synchronizing WAN Clusters

There are two options to synchronize your WAN replicated clusters:

* Full synchronization
* Delta synchronization

The following sections describe each. 

[[synchronizing-wan-target-cluster]]
===== Full WAN Synchronization

Full WAN synchronization sends all the data of an IMap to a target cluster to align the state of target IMap with source IMap.
It is useful if two remote clusters lost their synchronizations due to overflow in the WAN queue or in restart scenarios.
This is the default synchronization option.

Full WAN Synchronization can be initiated through
link:https://docs.hazelcast.org/docs/management-center/latest/manual/html/index.html#wan-sync[Management Center^] and
Hazelcast's REST API.

Below is the URL for the REST call;

```
http://member_ip:port/hazelcast/rest/wan/sync/map
```

You need to add URL-encoded parameters to the request in the following order separated by "&";

* Cluster name
* Cluster password
* Name of the WAN replication configuration
* WAN replication publisher ID/target cluster name
* Map name to be synchronized

Assume that you have configured an IMap with a WAN replication configuration as follows:

[source,xml]
----
<hazelcast>
    ...
    <wan-replication name="london-wan-rep">
        <batch-publisher>
            <cluster-name>istanbul</cluster-name>
        </batch-publisher>
    </wan-replication>
    <map name="my-map">
        <wan-replication-ref name="london-wan-rep">
            <merge-policy>com.hazelcast.spi.merge.PassThroughMergePolicy</merge-policy>
        </wan-replication-ref>
    </map>
    ...
</hazelcast>
----

Then, an example CURL command to initiate the synchronization for "my-map" would be as follows:

```
curl -X POST -d "clusterName&clusterPassword&london-wan-rep&istanbul&my-map" --URL http://127.0.0.1:5701/hazelcast/rest/wan/sync/map
```

[NOTE]
.Synchronizing All Maps
====

You can also use the following URL in your REST call if you want to
synchronize all the maps in source and target cluster:

`+http://member_ip:port/hazelcast/rest/wan/sync/allmaps+`

You need to add the following URL-encoded parameters to
the request in the following order separated by "&";

* Cluster name
* Cluster password
* Name of the WAN replication configuration
* WAN replication publisher ID/target cluster name
====

NOTE: Synchronization for a target cluster operates only with
the data residing in the memory. Therefore, evicted entries are not
synchronized, not even if `MapLoader` is configured.

===== Delta WAN Synchronization

Hazelcast clusters connected over WAN can go out-of-sync because of various reasons such as member failures and concurrent updates.
To overcome the out-of-sync issue, Hazelcast has the default
<<synchronizing-wan-target-cluster, WAN synchronization>> feature, through which
the maps in different clusters are synced by transferring all entries from the source to the target cluster.
This may be not efficient since some of the entries have remained unchanged on both clusters and
do not require to be transferred. Also, for the entries to be transferred, they need to be copied to
on-heap on the source cluster. This may cause spikes in the heap usage, especially if using large off-heap stores.

Besides the default WAN synchronization, Hazelcast provides Delta WAN Synchronization which uses
link:https://en.wikipedia.org/wiki/Merkle_tree[Merkle tree^] for the same purpose.
It is a data structure used for efficient comparison of the difference in the contents of large data structures.
The precision of this comparison is defined by Merkle tree's depth.
Merkle tree hash exchanges can detect inconsistencies in the map data and
synchronize only the different entries when using WAN synchronization, instead of sending all the map entries.

NOTE: Currently, Delta WAN Synchronization is implemented only for Hazelcast IMap.
It will be implemented also for ICache in the future releases.

[[requirements-for-delta-wan-sync]]
====== Requirements

To be able to use Delta WAN synchronization, the following must be met:

* Source and target cluster versions must be at least Hazelcast 3.11.
* Both clusters must have the same number of partitions.
* Both clusters must use the same partitioning strategy.
* Both clusters must have the Merkle tree structure enabled.

====== Using Delta WAN Synchronization

To be able to use Delta WAN synchronization for a Hazelcast data structure:

. Configure the WAN synchronization mechanism for your WAN publisher so that
it uses the Merkle tree: If configuring declaratively, you can use the `consistency-check-strategy` sub-element of
the `sync` element. If configuring programmatically, you can use the setter of the
link:{docBaseUrl}/javadoc/com/hazelcast/config/WanSyncConfig.html[WanSyncConfig^] object.
. Bind that WAN synchronization configuration to the data structure (currently IMap):
Simply set the WAN replication reference of your map to the name of the WAN replication configuration which uses the Merkle tree.

Following is a declarative configuration example of the above:

[source,xml]
----
<hazelcast>
    ...
    <map name="myMap">
        <wan-replication-ref name="wanReplicationScheme">
        </wan-replication-ref>
    </map>
    <wan-replication name="wanReplicationScheme">
        <batch-publisher>
            <cluster-name>clusterName</cluster-name>
            <sync>
                <consistency-check-strategy>MERKLE_TREES</consistency-check-strategy>
            </sync>
        </batch-publisher>
    </wan-replication>
    ...
</hazelcast>
----

Here, the element `consistency-check-strategy` sets the strategy for
checking the consistency of data between the source and target clusters.
You must initiate the WAN synchronization (via Management Center or REST API as explained in
<<synchronizing-wan-target-cluster, Synchronizing WAN clusters>>) to let this strategy reconcile the inconsistencies.
The element `consistency-check-strategy` has currently two values:

* `NONE`: Means that there are no consistency checks. This is the default value.
* `MERKLE_TREES`: Means that WAN synchronization uses Merkle tree structure.

====== Configuring Delta WAN Synchronization

You can configure Delta WAN Synchronization declaratively using the `merkle-tree` element or programmatically using the link:{docBaseUrl}/javadoc/com/hazelcast/config/MerkleTreeConfig.html[MerkleTreeConfig^] object.

Following is a declarative configuration example showing how to
enable Delta WAN Synchronization, bind it to a Hazelcast data structure (an IMap in the below case) and specify its depth.

[source,xml]
----
<hazelcast>
    ...
    <map name="someMap">
        <merkle-tree enabled="true">
            <depth>5</depth>
        </merkle-tree>
    </map>
    ...
</hazelcast>
----

Here are the descriptions of sub-elements and attributes:

* `enabled`: Specifies whether the Merkle tree structure is enabled. Its default value is `true`.
* `mapName`: Specifies the name of the map for which the Merkle tree structure is used.
* `depth`: Specifies the depth of Merkle tree. Valid values are between 2 and 27 (exclusive). Its default value is `10`.
** A larger depth means that a data synchronization mechanism is able to pinpoint a smaller subset of
the data structure (e.g., IMap) contents in which a change has occurred.
This causes the synchronization mechanism to be more efficient.
However, keep in mind that a large depth means that the Merkle tree will consume more memory.
As the comparison mechanism is iterative, a larger depth also prolongs the comparison duration.
Therefore, it is recommended not to have large tree depths if the latency of the comparison operation is high.
** A smaller depth means that the Merkle tree is shallower and the data synchronization mechanism transfers
larger chunks of the data structure (e.g., IMap) in which a possible change has happened.
As you can imagine, a shallower Merkle tree will consume less memory.

Following is a declarative example including the Merkle tree configuration.

[source,xml]
----
<hazelcast>
    ...
    <map name="myMap">
        <wan-replication-ref name="wanReplicationScheme">
            ...
        </wan-replication-ref>
        <merkle-tree enabled="true">
            <depth>10</depth>
        </merkle-tree>
    </map>

    <wan-replication name="wanReplicationScheme">
        <batch-publisher>
            <cluster-name>clusterName</cluster-name>
            <sync>
                <consistency-check-strategy>MERKLE_TREES</consistency-check-strategy>
            </sync>
        </batch-publisher>
    </wan-replication>
    ...
</hazelcast>
----

NOTE: If you do not specifically configure the `merkle-tree` in your
Hazelcast configuration, Hazelcast uses the default Merkle tree structure values
(i.e., it is enabled by default and its default depth is 10) when there is a WAN publisher using
the Merkle tree (i.e., `consistency-check-strategy` for a WAN replication configuration is set as
`MERKLE_TREES` and there is a data structure using that WAN replication configuration).

NOTE: Merkle trees are created for each partition holding IMap data.
Therefore, increasing the partition count also
increases the efficiency of the Delta WAN Synchronization.

====== The Process

Synchronizing the maps based on Merkle trees consists of two phases:

1. _Consistency check_: Process of exchanging and comparing the hashes stored in the Merkle tree structures in the
source and target clusters. The check starts with the root node and continues recursively with the children with different
hash codes. Both sides send the children of the nodes that the other side sent, hence the comparison is done by `depth/2`
steps. After this check, the tree leaves holding different entries are identified.
2. _Synchronization_: Process of transferring the entries belong to the leaves identified by the _consistency
check_ from the source to target cluster. On the target cluster the configured merge policy is applied for each entry that
is in both the source and target clusters.

NOTE: If you only need the differences between the clusters, you can trigger the consistency check without performing
synchronization.

The two phases of the Merkle tree based synchronization can be triggered by a REST call, as it can be done with the
full synchronization.

The URL for the consistency check REST call:

```
http://member_ip:port/hazelcast/rest/wan/consistencyCheck/map
```

The URL for the synchronization REST call - the same as it is for the default synchronization:

```
http://member_ip:port/hazelcast/rest/wan/sync/map
```

See the REST call details <<synchronizing-the-clusters, here>>.


====== Memory Consumption

Since Merkle trees are built for each partition and each map, the memory overhead of the trees with high entry count and deep
trees can be significant. The trees are maintained on-heap, therefore - besides the memory consumption - garbage collection could be another
concern. Make sure the configuration is tested with realistic data size before deployed in production.

The table below shows a few examples for what the memory overhead could be.

.Merkle trees memory overhead for a member
|===
|Entries Stored |Partitions Owned |Entries per Leaf |Depth |Memory Overhead

|1M
|271
|7
|10
|57 MB

|1M
|271
|1
|13
|97 MB

|10M
|271
|72
|10
|412 MB

|10M
|271
|9
|13
|453 MB

|10M
|5009
|4
|10
|577 MB

|10M
|5009
|1
|12
|900 MB

|25M
|5009
|10
|10
|1986 MB

|25M
|5009
|1
|13
|2740 MB

|===

====== Defining the Depth

The efficiency of the Delta WAN Synchronization (WAN synchronization based on Merkle trees) is determined by the average number of entries per the tree
leaves that is proportionate to the number of entries in the map. The bigger this average the more entries are getting
synchronized for the same difference. Raising the depth decreases this average at the cost of increasing the memory overhead.

This average can be calculated for a map as `avgEntriesPerLeaf = mapEntryCount / totalLeafCount`, where `totalLeafCount =
partitionCount * 2^depth-1^`. The ideal value is 1, however this may come at significant memory overhead as shown in the
table above.

In order to specify the tree depth, a trade-off between memory consumption and effectiveness might be needed.

Even if the map is huge and the Merkle trees are configured to be relatively shallow, the Merkle tree based synchronization
may be leveraged if only a small subset of the whole map is expected to be synchronized. The table below illustrates the
efficiency of the Merkle tree based synchronization compared to the default synchronization mechanism.


.Efficiency examples
|===
|Map entry count |Depth |Memory consumption |Avg entries / leaf |Difference count |Entries synced |Efficiency

|10M
|11
|685 MB
|2
|5M
|10M
|0%

|10M
|12
|900 MB
|1
|5M
|5M
|100%

|10M
|10
|577 MB
|4
|1M
|4M
|150%

|10M
|8
|497 MB
|16
|10K
|160K
|6150%

|10M
|12
|900 MB
|1
|10K
|10K
|99900%

|===

The `Difference count` column shows the number of the entries different in the source and the target clusters.
This is the minimum number of the entries that need to be synchronized to make the clusters consistent. The `Entries synced`
column shows how many entries are synchronized in the given case, calculated as `Entries synced` = `Difference count`
* `Avg entries / leaf`.

As shown in the last two rows, the Merkle tree based synchronization transfers significantly less entries than what the
default mechanism does even with 8 deep trees. The efficiency with depth 12 is even better but consumes much more memory.

NOTE: The averages in the table are calculated with 5009 partitions.

NOTE: The average entries per leaf number above assumes perfect distribution of the entries amongst the leaves. Since this is
typically not true in real-life scenarios the efficiency can be slightly worse. The statistics section below describes how to
get the actual average for the leaves involved in the synchronization.

==== Dynamically Adding WAN Publishers

When running clusters for an extensive period, you might need to
dynamically change the configuration while the cluster is running.
This includes dynamically adding new WAN replication publishers (new target clusters) and
replicating the subsequent map and cache updates to the new publishers without any manual intervention.

You can add new WAN publishers to an existing WAN replication using
almost all of the configuration options that are available when
configuring the WAN publishers in the static configuration (including using Discovery SPI).
The new configuration is not persisted but it is replicated to all existing and new members.
Once the cluster is completely restarted, the dynamically added publisher configuration is lost and
the updates are not replicated to the target cluster anymore until added again.

If you wish to preserve the new configuration over cluster restarts, you must add
the exact same configuration to the static configuration file after dynamically adding the publisher configuration to a running cluster.

You cannot remove the existing configurations but can put the publishers into
a STOPPED state which prevents the WAN events from being enqueued in the WAN queues and
prevents the replication, rendering the publisher idle. The configurations also cannot be changed.

Synchronization can be initiated through
link:https://docs.hazelcast.org/docs/management-center/latest/manual/html/index.html#wan-sync[Management Center^] and
Hazelcast’s REST API.

Below is the URL for the REST call:

```
http://member_ip:port/hazelcast/rest/wan/addWanConfig
```

You need to add the following URL-encoded parameters to the request in the following order separated by "&";

* Cluster name
* Cluster password
* WAN replication configuration, serialized as JSON

You can, at any point, even when maps and caches are concurrently mutated, add a new WAN publisher to
an existing WAN replication configuration.
The limitation is that there must be an existing WAN replication configuration but
it can be empty, without any publishers (target clusters).
For instance, this is an example of an XML configuration to which you can dynamically add new publishers:

[source,xml]
----
<hazelcast>
    ...
    <wan-replication name="wanReplication"></wan-replication>
    <map name="my-map">
        <wan-replication-ref name="wan-replication">
            <merge-policy>com.hazelcast.spi.merge.PassThroughMergePolicy</merge-policy>
            <republishing-enabled>false</republishing-enabled>
       </wan-replication-ref>
    </map>
    ...
</hazelcast>
----

Note that the map has defined WAN replication but there is no target cluster yet.
You can then add the new WAN replication publishers (target clusters) by
performing an HTTP POST as shown below:

```
curl -X POST -d "clusterName&clusterPassword&{...}" --URL http://127.0.0.1:5701/hazelcast/rest/wan/addWanConfig

```

You can provide the full configuration as JSON as a parameter.
Any WAN configuration supported in the XML and programmatic configurations is also supported in this JSON format.
Below are some examples of JSON configuration for a WAN publisher using
the Discovery SPI and static IP configuration. Here are the integer values for `initialPublisherState`,
`queueFullBehavior` and `consistencyCheckStrategy`:

* `initialPublisherState`:
** 0: REPLICATING
** 1: PAUSED
** 2: STOPPED
* `queueFullBehavior`:
** 0: DISCARD_AFTER_MUTATION
** 1: THROW_EXCEPTION
** 2: THROW_EXCEPTION_ONLY_IF_REPLICATION_ACTIVE
* `consistencyCheckStrategy`:
** 0: NONE
** 1: MERKLE_TREES


Below is an example using Discovery SPI (AWS configuration):

```
{
   "name":"wanReplication",
   "publishers":[
      {
         "clusterName":"tokyo",
         "queueCapacity":10000,
         "queueFullBehavior":0,
         "initialPublisherState":0,
         "discovery":{
            "nodeFilterClass":null,
            "discoveryStrategy":[
               {
                  "className":"com.hazelcast.aws.AwsDiscoveryStrategy",
                  "properties":{
                     "security-group-name":"hazelcast",
                     "tag-value":"cluster1",
                     "host-header":"ec2.amazonaws.com",
                     "tag-key":"aws-test-cluster",
                     "secret-key":"my-secret-key",
                     "iam-role":"s3access",
                     "access-key":"my-access-key",
                     "hz-port":"5701-5708",
                     "region":"us-west-1"
                  }
               }
            ]
         }
      }
   ]
}
```

Below is an example with Discovery SPI (the new AWS configuration)

```
{
   "name":"wanReplication",
   "publishers":[
      {
         "clusterName":"tokyo",
         "queueCapacity":1000,
         "queueFullBehavior":0,
         "initialPublisherState":0,
         "aws":{
            "enabled":true,
            "usePublicIp":false,
            "properties":{
               "security-group-name":"hazelcast-sg",
               "tag-value":"hz-nodes",
               "host-header":"ec2.amazonaws.com",
               "tag-key":"type",
               "secret-key":"my-secret-key",
               "iam-role":"dummy",
               "access-key":"my-access-key",
               "region":"us-west-1"
            }
         },
         "sync":{
            "consistencyCheckStrategy":0
         }
      }
   ]
}
```

Below is an example with static IP configuration (with some optional attributes):

```
{
   "name":"wanReplication",
   "publishers":[
      {
         "clusterName":"tokyo",
         "queueCapacity":1000,
         "queueFullBehavior":0,
         "initialPublisherState":0,
         "responseTimeoutMillis":5000,
         "targetEndpoints":"10.3.5.1:5701, 10.3.5.2:5701",
         "batchMaxDelayMillis":3000,
         "batchSize":50,
         "snapshotEnabled":false,
         "acknowledgeType":1,
         "sync":{
            "consistencyCheckStrategy":0
         }
      }
   ]
}
```

Below is an XML configuration with two publishers and several (disabled) discovery strategy configurations:

```
{
   "name":"wanReplication",
   "publishers":[
      {
         "clusterName":"tokyo",
         "queueCapacity":1000,
         "queueFullBehavior":0,
         "initialPublisherState":0,
         "aws":{
            "enabled":true,
            "usePublicIp":false,
            "properties":{
               "security-group-name":"hazelcast-sg",
               "tag-value":"hz-nodes",
               "host-header":"ec2.amazonaws.com",
               "tag-key":"type",
               "secret-key":"my-secret-key",
               "iam-role":"dummy",
               "access-key":"my-access-key",
               "region":"us-west-1"
            }
         },
         "gcp":{
            "enabled":false,
            "usePublicIp":true,
            "properties":{
               "gcp-prop":"gcp-val"
            }
         },
         "azure":{
            "enabled":false,
            "usePublicIp":true,
            "properties":{
               "azure-prop":"azure-val"
            }
         },
         "kubernetes":{
            "enabled":false,
            "usePublicIp":true,
            "properties":{
               "k8s-prop":"k8s-val"
            }
         },
         "eureka":{
            "enabled":false,
            "usePublicIp":true,
            "properties":{
               "eureka-prop":"eureka-val"
            }
         },
         "discovery":{
            "nodeFilterClass":null,
            "discoveryStrategy":[

            ]
         },
         "sync":{
            "consistencyCheckStrategy":0
         }
      },
      {
         "clusterName":"london",
         "queueCapacity":1000,
         "queueFullBehavior":0,
         "initialPublisherState":0,
         "responseTimeoutMillis":5000,
         "targetEndpoints":"10.3.5.1:5701, 10.3.5.2:5701",
         "batchMaxDelayMillis":3000,
         "batchSize":50,
         "snapshotEnabled":false,
         "acknowledgeType":1,
         "aws":{
            "enabled":false,
            "usePublicIp":false
         },
         "gcp":{
            "enabled":false,
            "usePublicIp":false
         },
         "azure":{
            "enabled":false,
            "usePublicIp":false
         },
         "kubernetes":{
            "enabled":false,
            "usePublicIp":false
         },
         "eureka":{
            "enabled":false,
            "usePublicIp":false
         },
         "discovery":{
            "nodeFilterClass":null,
            "discoveryStrategy":[

            ]
         },
         "sync":{
            "consistencyCheckStrategy":1
         }
      }
   ]
}
```

==== Event Filtering API

WAN replication allows you to intercept WAN replication events before they are placed to
WAN event replication queues by providing a filtering API.
Using this API, you can monitor WAN replication events of each data structure
separately.

You can attach filters to your data structures using the `filter` element of
`wan-replication-ref` configuration inside `hazelcast.xml` as shown below.
You can also configure it using the programmatic configuration.

[source,xml]
----
<hazelcast>
    ...
    <map name="testMap">
        <wan-replication-ref name="test">
            <filters>
                <filter-impl>com.example.MyFilter</filter-impl>
                <filter-impl>com.example.MyFilter2</filter-impl>
            </filters>
        </wan-replication-ref>
    </map>
    ...
</hazelcast>
----

As shown in the above configuration, you can define more than one filter. Filters are called in the order that they are introduced.
A WAN replication event is only eligible to publish if it passes all the filters.

Map and Cache have different filter interfaces: `MapWanEventFilter` and
`CacheWanEventFilter`. Both of these interfaces have the method `filter` which takes the following parameters:

* `mapName`/`cacheName`: Name of the related data structure.
* `entryView`: link:{docBaseUrl}/javadoc/com/hazelcast/core/EntryView.html[EntryView^]
or link:{docBaseUrl}/javadoc/com/hazelcast/cache/CacheEntryView.html[CacheEntryView^] depending on the data structure.
* `eventType`: Enum type - `UPDATED(1)`, `REMOVED(2)` or `LOADED(3)` - depending on the event.

NOTE: `LOADED` events are filtered out and not replicated to target cluster.
