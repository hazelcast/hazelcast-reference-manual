
[[distributed-data-structures]]
== Distributed Data Structures

As mentioned in the <<hazelcast-overview, Overview section>>, Hazelcast offers
distributed implementations of many common data structures. For each of the client
languages, Hazelcast mimics as closely as possible the natural interface of the
structure. So, for example in Java, the map follows `java.util.Map` semantics.
In the descriptions below, we mention each structure's Java equivalent interface.  All of
these structures are usable from Java, .NET, C++, Node.js, Python, and Go.

* **Standard utility collections**
** <<map, Map>> is the distributed implementation of `java.util.Map`.
It lets you read from and write to a Hazelcast map with methods such as
`get` and `put`.
** <<queue, Queue>> is the distributed implementation of `java.util.concurrent.BlockingQueue`.
You can add an item in one member and remove it from another one.
** <<ringbuffer, Ringbuffer>> is implemented for reliable eventing system.
** <<set, Set>> is the distributed and concurrent implementation of `java.util.Set`.
It does not allow duplicate elements and does not preserve their order.
** <<list, List>> is similar to Hazelcast Set. The only difference is that it allows
duplicate elements and preserves their order.
** <<multimap, Multimap>> is a specialized Hazelcast map. It is a distributed data
structure where you can store multiple values for a single key.
** <<replicated-map, Replicated Map>> does not partition data. It does not spread
data to different cluster members. Instead, it replicates the data to all members.
** <<cardinality-estimator, Cardinality Estimator>> is a data structure which implements
Flajolet's HyperLogLog algorithm.
* **Topic** is the distributed mechanism for publishing messages that are delivered to
multiple subscribers. It is also known as the publish/subscribe (pub/sub) messaging model.
See the <<topic, Topic section>> for more information. Hazelcast also has a structure called
Reliable Topic which uses the same interface of Hazelcast Topic. The difference is that it is
backed up by the Ringbuffer data structure. See the <<reliable-topic, Reliable Topic section>>.
* **Concurrency utilities**
** <<lock, FencedLock>> is the distributed implementation of `java.util.concurrent.locks.Lock`.
When you use lock, the critical section that Hazelcast Lock guards is guaranteed to be
executed by only one thread in the entire cluster.
** <<isemaphore, ISemaphore>> is the distributed implementation of `java.util.concurrent.Semaphore`.
When performing concurrent activities, semaphores offer permits to control the thread counts.
** <<iatomiclong, IAtomicLong>> is the distributed implementation of
`java.util.concurrent.atomic.AtomicLong`. Most of AtomicLong's operations are available.
However, these operations involve remote calls and hence their performances differ from
AtomicLong, due to being distributed.
** <<iatomicreference, IAtomicReference>> is the distributed implementation of
`java.util.concurrent.atomic.AtomicReference`. When you need to deal with a reference
in a distributed environment, you can use Hazelcast IAtomicReference.
** <<flakeidgenerator, FlakeIdGenerator>> is used to generate cluster-wide unique identifiers.
** <<icountdownlatch, ICountdownLatch>> is the distributed implementation of
`java.util.concurrent.CountDownLatch`. Hazelcast CountDownLatch is a gate keeper for
concurrent activities. It enables the threads to wait for other threads to complete
their operations.
** <<pn-counter, PN counter>> is a distributed data structure where each Hazelcast instance
can increment and decrement the counter value and these updates are propagated to all replicas.
* <<event-journal, Event Journal>> is a distributed data structure that stores the history
of mutation actions on map or cache.


[[overview-of-hazelcast-distributed-objects]]
=== Overview of Hazelcast Distributed Objects

Hazelcast has two types of distributed objects in terms of their partitioning strategies:

. Data structures where each partition stores a part of the instance,
namely partitioned data structures.
. Data structures where a single partition stores the whole instance,
namely non-partitioned data structures.

The following are the partitioned Hazelcast data structures:

* Map
* MultiMap
* Cache (Hazelcast JCache implementation)
* Event Journal

The following are the non-partitioned Hazelcast data structures:

* Queue
* Set
* List
* Ringbuffer
* FencedLock
* ISemaphore
* IAtomicLong
* IAtomicReference
* FlakeIdGenerator
* ICountdownLatch
* Cardinality Estimator
* PN Counter

Besides these, Hazelcast also offers the Replicated Map structure as
explained in the above *Standard utility collections* list.

[[loading-and-destroying-a-distributed-object]]
==== Loading and Destroying a Distributed Object

Hazelcast offers a `get` method for most of its distributed objects. To load an object,
first create a Hazelcast instance and then use the related `get` method on this instance.
Following example code snippet creates an Hazelcast instance and a map on this instance.

[source,java]
----
HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
Map<Integer, String> customers = hazelcastInstance.getMap( "customers" );
----

As to the configuration of distributed object, Hazelcast uses the default settings
from the file `hazelcast.xml` that comes with your Hazelcast download. Of course,
you can provide an explicit configuration in this XML or programmatically according
to your needs. See the <<understanding-configuration, Understanding Configuration section>>.

Note that, most of Hazelcast's distributed objects are created lazily, i.e., a distributed object
is created once the first operation accesses it.

If you want to use an object you loaded in other places, you can safely reload it using its
reference without creating a new Hazelcast instance (`customers` in the above example).

To destroy a Hazelcast distributed object, you can use the method `destroy`. This method clears
and releases all resources of the object. Therefore, you must use it with care since a reload
with the same object reference after the object is destroyed creates a new data structure without
an error. See the following example code where one of the queues are destroyed and the other
one is accessed.

[source,java]
----
include::{javasource}/MapMember.java[tag=mapmember]
----

If you start the `Member` above, the output is as shown below:

[source,plain]
----
q1.size: 1 q2.size:1
q1.size: 0 q2.size:0
----

As you see, no error is generated and a new queue resource is created.

Hazelcast is designed to create any distributed data structure whenever it is accessed,
i.e., whenever a call is made to the data structure. Therefore, keep in mind that a
data structure is recreated when you perform an operation on it even after you have destroyed it.

[[controlling-partitions]]
==== Controlling Partitions

Hazelcast uses the name of a distributed object to determine which partition it will be put.
Let's load two queues as shown below:

[source,java]
----
HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
IQueue q1 = hazelcastInstance.getQueue("q1");
IQueue q2 = hazelcastInstance.getQueue("q2");
----

Since these queues have different names, they will be placed into different partitions.
If you want to put these two into the same partition, you use the `@` symbol as shown below:

[source,java]
----
HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
IQueue q1 = hazelcastInstance.getQueue("q1@foo");
IQueue q2 = hazelcastInstance.getQueue("q2@foo");
----

Now, these two queues will be put into the same partition whose partition key is `foo`.
Note that you can use the method `getPartitionKey` to learn the partition key of a distributed object.
It may be useful when you want to create an object in the same partition of an existing object.
See its usage as shown below:

[source,java]
----
String partitionKey = q1.getPartitionKey();
IQueue q3 = hazelcastInstance.getQueue("q3@"+partitionKey);
----

[[common-features-of-all-hazelcast-data-structures]]
==== Common Features of all Hazelcast Data Structures

* If a member goes down, its backup replica (which holds the same data) dynamically
redistributes the data, including the ownership and locks on them, to the remaining
live members. As a result, there will not be any data loss.
* There is no single cluster master that can be a single point of failure.
Every member in the cluster has equal rights and responsibilities. No single member is
superior. There is no dependency on an external 'server' or 'master'.

[[example-distributed-object-code]]
==== Example Distributed Object Code

Here is an example of how you can retrieve existing data structure instances
(map, queue, set, topic, etc.) and how you can listen for instance events,
such as an instance being created or destroyed.

[source,java]
----
include::{javasource}/ExampleDOL.java[tag=sampledol]
----

[[map]]
=== Map

Hazelcast Map (`IMap`) extends the interface `java.util.concurrent.ConcurrentMap`
and hence `java.util.Map`. It is the distributed implementation of Java map. You can
perform operations like reading and writing from/to a Hazelcast map with the well
known get and put methods.

'''
NOTE: IMap data structure can also be used by link:https://jet.hazelcast.org/[Hazelcast Jet^]
for Real-Time Stream Processing (by enabling the Event Journal on your map) and
Fast Batch Processing. Hazelcast Jet uses IMap as a source (reads data from IMap) and as a sink
(writes data to IMap). See the link:https://jet.hazelcast.org/use-cases/fast-batch-processing/[Fast Batch Processing^]
and link:https://jet.hazelcast.org/use-cases/real-time-stream-processing/[Real-Time Stream Processing^]
use cases for Hazelcast Jet. See also link:https://jet-start.sh/docs/api/sources-sinks#imap[here^]
in the Hazelcast Jet Programming Guide to learn how Jet uses IMap, i.e., how it can read from and write to IMap.


[[getting-a-map-and-putting-an-entry]]
==== Getting a Map and Putting an Entry

Hazelcast partitions your map entries and their backups, and almost evenly distribute
them onto all Hazelcast members. Each member carries approximately
"number of map entries * 2 * 1/n" entries, where **n** is the number of members in the cluster.
For example, if you have a member with 1000 objects to be stored in the cluster and then you
start a second member, each member will both store 500 objects and back up the 500 objects
in the other member.

Let's create a Hazelcast instance and fill a map named `Capitals` with key-value pairs
using the following code. Use the HazelcastInstance `getMap` method to get the map,
then use the map `put` method to put an entry into the map.

[source,java]
----
HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance();
Map<String, String> capitalcities = hzInstance.getMap( "capitals" );
    capitalcities.put( "1", "Tokyo" );
    capitalcities.put( "2", "Paris" );
    capitalcities.put( "3", "Washington" );
    capitalcities.put( "4", "Ankara" );
    capitalcities.put( "5", "Brussels" );
    capitalcities.put( "6", "Amsterdam" );
    capitalcities.put( "7", "New Delhi" );
    capitalcities.put( "8", "London" );
    capitalcities.put( "9", "Berlin" );
    capitalcities.put( "10", "Oslo" );
    capitalcities.put( "11", "Moscow" );
    ...
    capitalcities.put( "120", "Stockholm" );
----

When you run this code, a cluster member is created with a map whose entries are
distributed across the members' partitions. See the below illustration. For now,
this is a single member cluster.

image::1Node.png[Map Entries in a Single Member]

NOTE: Please note that some of the partitions do not contain any data entries since
we only have 120 objects and the partition count is 271 by default. This count is
configurable and can be changed using the system property `hazelcast.partition.count`.
See the <<system-properties, System Properties appendix>>.

[[creating-a-member-for-map-backup]]
==== Creating A Member for Map Backup

Now let's create a second member by running the above code again. This creates a
cluster with two members. This is also where backups of entries are created - remember
the backup partitions mentioned in the <<hazelcast-overview, Hazelcast Overview section>>.
The following illustration shows two members and how the data and its backup is distributed.

image::2Nodes.png[Map Entries with Backups in Two Members]

As you see, when a new member joins the cluster, it takes ownership and loads some of the
data in the cluster. Eventually, it will carry almost "(1/n `*` total-data) + backups" of the data,
reducing the load on other members.

`HazelcastInstance.getMap()` returns an instance of `com.hazelcast.map.IMap` which extends
the `java.util.concurrent.ConcurrentMap` interface. Methods like
`ConcurrentMap.putIfAbsent(key,value)` and `ConcurrentMap.replace(key,value)` can be used
on the distributed map, as shown in the example below.

[source,java]
----
include::{javasource}/dds/map/BasicMapOperations.java[tag=bmo]
----

All `ConcurrentMap` operations such as `put` and `remove` might wait if the key is locked
by another thread in the local or remote JVM. But, they will eventually return with success.
`ConcurrentMap` operations never throw a `java.util.ConcurrentModificationException`.

[[backing-up-maps]]
==== Backing Up Maps

Hazelcast distributes map entries onto multiple cluster members (JVMs). Each member
holds some portion of the data.

Distributed maps have one backup by default. If a member goes down, your data is recovered
using the backups in the cluster. There are two types of backups as described below: _sync_ and _async_.

[[creating-sync-backups]]
===== Creating Sync Backups

To provide data safety, Hazelcast allows you to specify the number of backup copies you
want to have. That way, data on a cluster member is copied onto other member(s).

To create synchronous backups, select the number of backup copies using the `backup-count` property.

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <map name="default">
        <backup-count>1</backup-count>
    </map>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  map:
    default:
      backup-count: 1
----

When this count is 1, a map entry will have its backup on one other member in the cluster.
If you set it to 2, then a map entry will have its backup on two other members.
You can set it to 0 if you do not want your entries to be backed up, e.g., if performance
is more important than backing up. The maximum value for the backup count is 6.

Hazelcast supports both synchronous and asynchronous backups. By default, backup operations
are synchronous and configured with `backup-count`. In this case, backup operations block
operations until backups are successfully copied to backup members (or deleted from backup
members in case of remove) and acknowledgements are received. Therefore, backups are updated
before a `write`(put, set, remove and their async counterparts) operation is completed,
provided that the cluster is stable. Sync backup operations have a blocking cost which may
lead to latency issues.

[[creating-async-backups]]
===== Creating Async Backups

Asynchronous backups, on the other hand, do not block operations. They are fire & forget and
do not require acknowledgements; the backup operations are performed at some point in time.

To create asynchronous backups, select the number of async backups with the `async-backup-count`
property. An example is shown below.

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <map name="default">
        <backup-count>0</backup-count>
        <async-backup-count>1</async-backup-count>
    </map>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  map:
    default:
      backup-count: 0
      async-backup-count: 1 
----

See <<consistency-and-replication-model, Consistency and Replication Model>> for more detail.

NOTE: Backups increase memory usage since they are also kept in memory.

NOTE: A map can have both sync and async backups at the same time.

[[enabling-backup-reads]]
===== Enabling Backup Reads

By default, Hazelcast has one sync backup copy. If `backup-count` is set to more than 1, then
each member will carry both owned entries and backup copies of other members. So for the `map.get(key)`
call, it is possible that the calling member has a backup copy of that key. By default, `map.get(key)`
always reads the value from the actual owner of the key for consistency.

To enable backup reads (read local backup entries), set the value of the `read-backup-data` property
to **true**. Its default value is **false** for consistency. Enabling backup reads can improve
performance but on the other hand it can cause stale reads while still preserving monotonic-reads property.

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <map name="default">
        <backup-count>0</backup-count>
        <async-backup-count>1</async-backup-count>
        <read-backup-data>true</read-backup-data>
    </map>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  map:
    default:
      backup-count: 0
      async-backup-count: 1
      read-backup-data: true      
----

This feature is available when there is at least one sync or async backup.

Please note that if you are performing a read from a backup, you should take into account that
your hits to the keys in the backups are not reflected as hits to the original keys on the primary
members. This has an impact on IMap's maximum idle seconds or time-to-live seconds expiration.
Therefore, even though there is a hit on a key in backups, your original key on the primary member may expire.

NOTE: Backup reads that are requested by Hazelcast clients are ignored since this operation
is performed on the local entries.

[[map-eviction]]
==== Map Eviction

Hazelcast maps have no restrictions on the size and may grow arbitrarily
large, by default. Unless you delete the map entries manually or use an
eviction policy, they will remain in the map. When it comes to reducing the size of
a map, there are two concepts: expiration and eviction.

Expiration puts a limit on the maximum lifetime of an entry stored inside the map.
When the entry expires it cannot be retrieved from the map any longer and at some point
in time it will be cleaned out from the map to free up the memory. You can configure the expiration,
and hence the eviction based on the expiration, using the elements `time-to-live-seconds`
and `max-idle-seconds` as described in <<configuring-map-eviction>> below.

Eviction puts a limit on the maximum size of the map. If the size of the map grows larger than
the maximum allowed size, an eviction policy decides which item to evict from the map to reduce
its size. You can configure the maximum allowed size and eviction policy using the elements
`size` and `eviction-policy` as described in <<configuring-map-eviction>> below.

Eviction and expiration can be used together. In this case, the expiration configurations
(`time-to-live-seconds` and `max-idle-seconds`) continue to work as usual cleaning out the
expired entries regardless of the map size. Note that locked map entries are not the subjects
for eviction and expiration.

Hazelcast Map uses the same eviction mechanism as our JCache implementation.
See the <<eviction-algorithm, Eviction Algorithm section>> for details.

[[understanding-map-eviction]]
===== Understanding Map Eviction

Hazelcast Map performs eviction based on partitions. For example, when you specify a size using
the `PER_NODE` attribute for `max-size` (see the <<configuring-map-eviction, Configuring Map Eviction section>>),
Hazelcast internally calculates the maximum size for every partition. Hazelcast uses the following
equation to calculate the maximum size of a partition:

```
partition-maximum-size = max-size * member-count / partition-count
```

NOTE: If the `partition-maximum-size` is less than 1 in the equation above, it will be set to 1
(otherwise, the partitions would be emptied immediately by eviction due to the exceedance of
`max-size` being less than 1).

The eviction process starts according to this calculated partition maximum size when you try
to put an entry. When entry count in that partition exceeds partition maximum size, eviction
starts on that partition.

Assume that you have the following figures as examples:

* partition count: 200
* entry count for each partition: 100
* `max-size` (PER_NODE): 20000

The total number of entries here is 20000 (partition count * entry count for each partition).
This means you are at the eviction threshold since you set the `max-size` to 20000. When you
try to put an entry:

. the entry goes to the relevant partition
. the partition checks whether the eviction threshold is reached (`max-size`)
. only one entry will be evicted.

As a result of this eviction process, when you check the size of your map, it is 19999. After
this eviction, subsequent put operations do not trigger the next eviction until the map size
is again close to the `max-size`.

NOTE: The above scenario is simply an example that describes how the eviction process works.
Hazelcast finds the most optimum number of entries to be evicted according to your cluster
size and selected policy.

[[configuring-map-eviction]]
===== Configuring Map Eviction

The following is an example declarative configuration for map eviction.

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <map name="default">
        <time-to-live-seconds>0</time-to-live-seconds>
        <max-idle-seconds>0</max-idle-seconds>
        <eviction eviction-policy="LRU" max-size-policy="PER_NODE" size="5000"/>
    </map>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  map:
    default:
      time-to-live-seconds: 0
      max-idle-seconds: 0
      eviction:
        eviction-policy: LRU
        max-size-policy: PER_NODE
        size: 5000
----

The following are the configuration element descriptions:

* `time-to-live-seconds`: Maximum time in seconds for each entry to stay in the map (TTL).
It limits the lifetime of the entries relative to the time of the last write access
performed on them. If it is not 0, the entries whose lifetime exceeds this period
(without any write access performed on them during this period) are expired and
evicted automatically. An individual entry may have its own lifetime limit by
using one of the methods accepting a TTL; see <<evicting-specific-entries, Evicting Specific Entries section>>.
If there is no TTL value provided for the individual entry, it inherits the value
set for this element. Valid values are integers between 0 and `Integer.MAX VALUE`.
Its default value is 0, which means infinite (no expiration and eviction). If it is not 0,
entries are evicted regardless of the set `eviction-policy` described below.
* `max-idle-seconds`: Maximum time in seconds for each entry to stay idle in
the map. It limits the lifetime of the entries relative to the time of the
last read or write access performed on them. The entries whose idle period
exceeds this limit are expired and evicted automatically. An entry is idle
if no `get`, `put`, `EntryProcessor.process` or `containsKey` is called on
it. Valid values are integers between 0 and `Integer.MAX VALUE`.
Its default value is 0, which means infinite.
+
NOTE: Setting this property to 1 second expires the entry after 1 second, regardless of
the operations done on that entry in-between, due to the loss of millisecond
resolution on the entry timestamps. Assume that you create a record at time = 1 second
(1000 milliseconds) and access it at wall clock time 1100 milliseconds and
then again at 1400 milliseconds. In this case, the entry is deemed as not touched.
So, setting this property to 1 second is not supported.
+
NOTE: Both `time-to-live-seconds` and `max-idle-seconds` may be used simultaneously on
the map entries. In that case, the entry is considered expired if at least one of the
policies marks it as expired.
+
* `eviction`: By default map has no eviction configured.
To make it work you have to configure it using the following attributes of
this element:
** `eviction-policy`: Eviction policy to be applied when the size of map grows larger than
the value specified by the `size` element described below.  Valid values are:
*** NONE: Default policy. If set, no items are evicted and the property `size` described
below is ignored. However, entries could still be expired if you configure
`time-to-live-seconds` and/or `max-idle-seconds`.
*** LRU: Least Recently Used.
*** LFU: Least Frequently Used.
+
Apart from the above values, you can also develop and use your own eviction policy.
See the <<custom-eviction-policy, Custom Eviction Policy section>>.
+
** `size`: Maximum size of the map. When maximum size is reached, the map is evicted
based on the policy defined. Valid values are integers between 0 and `Integer.MAX VALUE`.
Its default value is 0, which means infinite. If you want `size` to work, set the
`eviction-policy` property to a value other than `NONE`. Its attributes are described below.
** `max-size-policy`: Maximum size policy for eviction of the map. Available values are as follows:
*** `PER_NODE`: Maximum number of map entries in each cluster member. This is the default policy.
*** `PER_PARTITION`: Maximum number of map entries within each partition. Storage size depends
on the partition count in a cluster member. This attribute should not be used often. For instance,
avoid using this attribute with a small cluster. If the cluster is small, it hosts more partitions,
and therefore map entries, than that of a larger cluster. Thus, for a small cluster, eviction of
the entries decreases performance (the number of entries is large).
*** `USED_HEAP_SIZE`: Maximum used heap size in megabytes per map for each Hazelcast instance.
Please note that this policy does not work when <<setting-in-memory-format, in-memory format>>
is set to `OBJECT`, since the memory footprint cannot be determined when data is put as `OBJECT`.
*** `USED_HEAP_PERCENTAGE`: Maximum used heap size percentage per map for each Hazelcast instance.
If, for example, a JVM is configured to have 1000 MB and this value is 10, then the map entries
will be evicted when used heap size exceeds 100 MB. Please note that this policy does not work
when <<setting-in-memory-format, in-memory format>> is set to `OBJECT`, since the memory footprint
cannot be determined when data is put as `OBJECT`.
*** `FREE_HEAP_SIZE`: Minimum free heap size in megabytes for each JVM.
*** `FREE_HEAP_PERCENTAGE`: Minimum free heap size percentage for each JVM. If, for example, a JVM
is configured to have 1000 MB and this value is 10, then the map entries will be evicted when
free heap size is below 100 MB.
*** `USED_NATIVE_MEMORY_SIZE`: ([navy]*Hazelcast IMDG Pro and Enterprise*) Maximum used native memory
size in megabytes per map for each Hazelcast instance.
*** `USED_NATIVE_MEMORY_PERCENTAGE`: ([navy]*Hazelcast IMDG Pro and Enterprise*) Maximum used native
memory size percentage per map for each Hazelcast instance.
*** `FREE_NATIVE_MEMORY_SIZE`: ([navy]*Hazelcast IMDG Pro and Enterprise*) Minimum free native memory
size in megabytes for each Hazelcast instance.
*** `FREE_NATIVE_MEMORY_PERCENTAGE`: ([navy]*Hazelcast IMDG Pro and Enterprise*) Minimum free native
memory size percentage for each Hazelcast instance.

====== Fine-Tuning Map Eviction

Besides the above configuration elements and attributes you can fine-tune
the eviction related to the entry counts to be evicted using the following
Hazelcast properties:

* `hazelcast.map.eviction.batch.size`: Specifies the maximum number of map entries
that are evicted during a single eviction cycle. Its default value is 1, meaning
at most 1 entry is evicted, which is typically fine.
However, when you insert values during an eviction cycle, each iteration doubles the entry size.
In this situation more than just a single entry should be evicted.
* `hazelcast.map.eviction.sample.count`: Whenever a map eviction is required, a new sampling starts by the built-in sampler.
The sampling algorithm selects a random sample from the underlying data storage and
it results in a set of map entries. This property specifies the entry count of this sample. Its default value is 15.

See also the <<eviction-algorithm, Eviction Algorithm section>> to learn more details on evicting entries.

[[example-eviction-configurations]]
===== Example Eviction Configurations

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <map name="documents">
        <eviction eviction-policy="LRU" max-size-policy="PER_NODE" size="10000"/>
        <max-idle-seconds>60</max-idle-seconds>
    </map>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  map:
    documents:
      eviction:
        eviction-policy: LRU
        max-size-policy: PER_NODE
        size: 10000
      max-idle-seconds: 60
----

In the above example, `documents` map starts to evict its entries from a member when the
map size exceeds 10000 in that member. Then the entries least recently used will be evicted.
The entries not used for more than 60 seconds will be evicted as well.

And the following is an example eviction configuration for a map having `NATIVE` as the
in-memory format:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <map name="nativeMap">
        <in-memory-format>NATIVE</in-memory-format>
        <eviction max-size-policy="USED_NATIVE_MEMORY_PERCENTAGE" eviction-policy="LFU" size="99"/>
    </map>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  map:
    nativeMap:
      in-memory-format: NATIVE
      eviction:
        eviction-policy: LFU
        max-size-policy: USED_NATIVE_MEMORY_PERCENTAGE
        size: 99
----

[[evicting-specific-entries]]
===== Evicting Specific Entries

The eviction policies and configurations explained above apply to all the entries of a map.
The entries that meet the specified eviction conditions are evicted.

If you want to evict some specific map entries, you can use the `ttl` and `ttlUnit` parameters of
the method `map.put()`. An example code line is given below.

`myMap.put( "1", "John", 50, TimeUnit.SECONDS )`

The map entry with the key "1" will be evicted 50 seconds after it is put into `myMap`.

You may also use `map.setTTL` method to alter the time-to-live value of an existing entry.
It is done as follows:

`myMap.setTTL( "1", 50, TimeUnit.SECONDS )`

In addition to the `ttl`, you may also specify a maximum idle timeout for specific map entries
using the `maxIdle` and `maxIdleUnit` parameters:

`myMap.put( "1", "John", 50, TimeUnit.SECONDS, 40, TimeUnit.SECONDS )`

Here `ttl` is set as 50 seconds and `maxIdle` is set as 40 seconds. The entry is considered to
be evicted if at least one of these policies marks it as expired. If you want to specify only
the `maxIdle` parameter, you need to set `ttl` as 0 seconds.


[[evicting-all-entries]]
===== Evicting All Entries

To evict all keys from the map except the locked ones, use the method `evictAll()`.
If a MapStore is defined for the map, `deleteAll` is not called by `evictAll`. If
you want to call the method `deleteAll`, use `clear()`.

An example is given below.

[source,java]
----
include::{javasource}/dds/map/EvictAll.java[tag=evictall]
----

NOTE: Only EVICT_ALL event is fired for any registered listeners.

[[forced-eviction]]
===== Forced Eviction

[blue]*Hazelcast IMDG Enterprise*

Hazelcast may use forced eviction in the cases when the eviction
explained in <<understanding-map-eviction, Understanding Map Eviction>>
is not enough to free up your memory. Note that this is valid if
you are using [blue]*Hazelcast IMDG Enterprise* and you set your
in-memory format to `NATIVE`.

The forced eviction mechanism is explained below as steps in the
given order:

* When the normal eviction is not enough, forced eviction is
triggered and first it tries to evict approx. 20% of the entries
from the current partition. It retries this five times.
* If the result of above step is still not enough, forced eviction
applies the above step to all maps. This time it might perform eviction
from some other partitions too, provided that they are owned by the same
thread.
* If that is still not enough to free up your memory, it evicts not the
20% but all the entries from the current partition.
* If that is not enough, it will evict all the entries from the other
data structures; from the partitions owned by the local thread.

Finally, when all the above steps are not enough, Hazelcast throws
a native `OutOfMemoryException`.

When you have an evictable cache/map, you should safely put
entries to it without facing with any memory shortages.
Forced eviction helps to achieve this. Regular eviction
removes one entry at a time while forced eviction can remove
multiple entries, which can even be owned by another caches/maps.

[[custom-eviction-policy]]
===== Custom Eviction Policy

Apart from the policies such as LRU and LFU, which Hazelcast provides out-of-the-box,
you can develop and use your own eviction policy.

To achieve this, you need to provide an implementation of `MapEvictionPolicyComparator` as in
the following `OddEvictor` example:

[source,java]
----
include::{javasource}/dds/map/MapCustomEvictionPolicyComparator.java[tag=mcep]
----

Then you can enable your policy by setting it via the method
`MapConfig.getEvictionConfig().setComparatorClassName()`
programmatically or via XML declaratively. Following is the example
declarative configuration for the eviction policy `OddEvictor` implemented above:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <map name="test">
        ...
        <eviction comparator-class-name="com.mycompany.OddEvictor"/>
        ...
    </map>
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  map:
    test:
      eviction:
        comparator-class-name: com.mycompany.OddEvictor
----

If you Hazelcast with Spring, you can enable your policy as shown below.

[source,xml]
----
<hz:map name="test">
    <hz:map-eviction comparator-class-name="com.package.OddEvictor"/>
</hz:map>
----

[[setting-in-memory-format]]
==== Setting In-Memory Format

IMap (and a few other Hazelcast data structures, such as ICache)
has an `in-memory-format` configuration option. By default, Hazelcast
stores data into memory in binary (serialized) format. Sometimes it can
be efficient to store the entries in their object form, especially in cases
of local processing, such as entry processor and queries.

Specify the `in-memory-format` element in the configuration to set how the
data will be stored in the memory. You have the following format options:

* `BINARY` (default): The data (both the key and value) is stored in serialized
binary format. You can use this option if you mostly perform regular map
operations, such as `put` and `get`.
* `OBJECT`: The data is stored in deserialized form. This configuration is
good for maps where <<entry-processor, entry processing>> and <<how-distributed-query-works, queries>> form the majority of all
operations and the objects are complex, making the serialization cost comparatively
high. By storing objects, entry processing does not contain the deserialization
cost. Note that when you use `OBJECT` as the in-memory format, the key is still
stored in binary format and the value is stored in object format.
* `NATIVE`: ([navy]*Hazelcast IMDG Enterprise HD*) This format behaves the same as
BINARY, however, instead of heap memory, key and value are stored in the off-heap
memory.

Regular operations like `get` rely on the object instance. When the `OBJECT` format
is used and a `get` is performed, the map does not return the stored instance,
but creates a clone. Therefore, this whole `get` operation first includes a
serialization on the member owning the instance and then a deserialization on
the member calling the instance. When the `BINARY` format is used, only a
deserialization is required; `BINARY` is faster.

Similarly, a `put` operation is faster when the `BINARY` format is used. If the
format was `OBJECT`, the map would create a clone of the instance, and there would
first be a serialization and then a deserialization. When BINARY is used, only a
deserialization is needed.

NOTE: If a value is stored in `OBJECT` format, a change on a returned value does not
affect the stored instance. In this case, the returned instance is not the actual one
but a clone. Therefore, changes made on an object after it is returned will not reflect
on the actual stored data. Similarly, when a value is written to a map and the value is
stored in `OBJECT` format, it will be a copy of the `put` value. Therefore, changes made
on the object after it is stored will not reflect on the stored data.

[[using-high-density-memory-store-with-map]]
==== Using High-Density Memory Store with Map

[navy]*Hazelcast IMDG Enterprise HD*

Hazelcast instances are Java programs. In case of `BINARY` and `OBJECT` in-memory
formats, Hazelcast stores your distributed data into the heap of its server instances.
Java heap is subject to garbage collection (GC). In case of larger heaps, garbage
collection might cause your application to pause for tens of seconds (even minutes
for really large heaps), badly affecting your application performance and response times.

As the data gets bigger, you either run the application with larger heap, which would
result in longer GC pauses or run multiple instances with smaller heap which can turn
into an operational nightmare if the number of such instances becomes very high.

To overcome this challenge, Hazelcast offers High-Density Memory Store for your maps.
You can configure your map to use High-Density Memory Store by setting the in-memory
format to `NATIVE`. The following snippet is the declarative configuration example.

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <map name="nativeMap">
        <in-memory-format>NATIVE</in-memory-format>
    </map>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  map:
    nativeMap:
      in-memory-format: NATIVE
----

Keep in mind that you should have already enabled the High-Density Memory Store
usage for your cluster. See the <<configuring-high-density-memory-store, Configuring High-Density Memory Store section>>.

You can also benefit from the persistent memory technologies such as
Intel(R) Optane(TM) DC to be used by the High-Density Memory Store. See the
<<using-persistent-memory, Using Persistent Memory section>>.

[[required-configuration-changes-when-using-native]]
===== Required Configuration Changes When Using NATIVE

Note that the eviction mechanism is different for `NATIVE` in-memory format.
The new eviction algorithm for map with High-Density Memory Store is similar
to that of JCache with High-Density Memory Store and is described <<eviction-algorithm, here>>.

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <map name="nativeMap">
        <in-memory-format>NATIVE</in-memory-format>
        <eviction-percentage>25</eviction-percentage> <--! NO IMPACT with NATIVE -->
    </map>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  map:
    nativeMap:
      in-memory-format: NATIVE
      eviction-percentage: 25 # NO IMPACT with NATIVE
----

* These IMap eviction policies for `size` cannot be used: `FREE_HEAP_PERCENTAGE`,
`FREE_HEAP_SIZE`, `USED_HEAP_PERCENTAGE`, `USED_HEAP_SIZE`.
* Near Cache eviction policy `ENTRY_COUNT` cannot be used for
`max-size-policy`.

NOTE: See the <<high-density-memory-store, High-Density Memory Store section>>
for more information.

==== Metadata Policy

Hazelcast IMap offers automatic preprocessing of various data types on the
update time to make queries faster. It is currently supported only by the
<<querying-json-strings, HazelcastJsonValue>> type. When metadata creation
is on, IMap creates additional metadata about the objects of supported types
and uses this metadata during the querying. It does not affect the latency
and throughput of the object of any type except the supported types.

This feature is on by default. You can configure it using the `metadata-policy`
configuration element.

**Declarative Configuration:**

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <map name="map-a">
        <!--
        valid values for metadata-policy are:
          - OFF
          - CREATE_ON_UPDATE (default)
        -->
        <metadata-policy>OFF</metadata-policy>
    </map>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  map:
    map-a:
    # valid values for metadata-policy are:
    # - OFF
    # - CREATE_ON_UPDATE (default)
      metadata-policy: OFF
----

**Programmatic Configuration:**

[source,java]
----
MapConfig mapConfig = new MapConfig();
mapConfig.setMetadataPolicy(MetadataPolicy.OFF);
----

[[loading-and-storing-persistent-data]]
==== Loading and Storing Persistent Data

Hazelcast allows you to load and store the distributed map entries
from/to a persistent data store such as a relational database. To do this,
you can use Hazelcast's `MapStore` and `MapLoader` interfaces.

When you provide a `MapLoader` implementation and request an entry
(`IMap.get()`) that does not exist in memory, ``MapLoader``'s `load`
method loads that entry from the data store. This loaded entry is placed
into the map and will stay there until it is removed or evicted.

All loads can be listened via `EntryLoadedListener`. See the
<<listening-for-map-events, Listening for Map Events section>>
to learn how you can catch entry-based events.

When a `MapStore` implementation is provided, an entry is also put into a
user defined data store.

NOTE: Data store needs to be a centralized system that is
accessible from all Hazelcast members. Persistence to a local file system
is not supported.

NOTE: Also note that the `MapStore` interface extends the `MapLoader` interface
as you can see in the interface link:{docBaseUrl}/javadoc/com/hazelcast/map/MapStore.html[code^].

Following is a `MapStore` example.


[source,java]
----
include::{javasource}/dds/map/PersonMapStore.java[tag=personms]
----

NOTE: During the initial loading process, MapStore uses a thread different from the
partition threads that are used by the ExecutorService. After the initialization is
completed, the `map.get` method looks up any nonexistent value from the database in
a partition thread, or the `map.put` method looks up the database to return the previously
associated value for a key also in a partition thread.

Entries loaded by `MapLoader` do not have a set time-to-live property. Therefore,
they live until evicted or explicitly removed. It is possible to enforce time-to-live
on the entries by using `EntryLoader`. `EntryLoader` allows you to set
time-to-live values per key before handing the values to Hazelcast. Therefore, you can store and
load key specific time-to-live values in the external storage.

Similar to `EntryLoader`, in order to store custom expiration times associated
with the entries, you may use `EntryStore`. `EntryStore` allows you to
retrieve associated expiration date for each entry. The expiration date is an offset
from an epoch in milliseconds. Epoch is January 1, 1970 UTC which is used by
`System.currentTimeMillis()`.

NOTE: Although the expiration date is expressed in milliseconds, IMap has second granularity
when it comes to expiration. Therefore, the expiration date is rounded to the nearest lower
whole second.

`EntryLoader` and `EntryStore` extend from `MapLoader` and `MapStore`, respectively.
Therefore, all features and configuration parameters of `MapLoader` and `MapStore` apply
to them, too.

Following is an `EntryStore` example.

[source,java]
----
include::{javasource}/dds/map/PersonEntryStore.java[tag=personms]
----

NOTE: For more MapStore/MapLoader code samples,
see link:https://github.com/hazelcast/hazelcast-code-samples/tree/master/distributed-map/mapstore/src/main/java[here^].

Hazelcast supports read-through, write-through and write-behind persistence
modes, which are explained in the subsections below.

[[using-read-through-persistence]]
===== Using Read-Through Persistence

If an entry does not exist in memory when an application asks for it,
Hazelcast asks the loader implementation to load that entry from the
data store.  If the entry exists there, the loader implementation gets it,
hands it to Hazelcast, and Hazelcast puts it into memory. This is read-through
persistence mode.

As you can remember from the introduction of this section, the `IMap.get()` method
triggers the `load()` method in your MapLoader implementation if an entry does not
exist in the memory. In this case, note that the `IMap.get()` method does not create
backup copies for such entries, when the mode is read-through persistence: there is no
need for backups for these entries since if the primary entry is lost, then a read for
the key triggers the `load()` method and loads the entry from the persistence layer.

[[setting-write-through-persistence]]
===== Setting Write-Through Persistence

`MapStore` can be configured to be write-through by setting the `write-delay-seconds`
property to **0**. This means the entries are put to the data store synchronously.

In this mode, when the `map.put(key,value)` call returns:

* `MapStore.store(key,value)` is successfully called so the entry is persisted.
* In-Memory entry is updated.
* In-Memory backup copies are successfully created on other cluster members
(if `backup-count` is greater than 0).

If `MapStore` throws an exception then the exception is propagated to the original
`put` or `remove` call in the form of `RuntimeException`.

NOTE: There is a key difference in the behaviors of `map.remove(key)` and
`map.delete(key)`, i.e., the latter results in `MapStore.delete(key)` to be invoked
whereas the former only removes the entry from IMap.

[[setting-write-behind-persistence]]
===== Setting Write-Behind Persistence

You can configure `MapStore` as write-behind by setting the `write-delay-seconds`
property to a value bigger than **0**. This means the modified entries will be
put to the data store asynchronously after a configured delay.

NOTE: In write-behind mode, Hazelcast coalesces updates on a specific key by
default, which means it applies only the last update on that key. However,
you can set `MapStoreConfig.setWriteCoalescing()` to `FALSE` and you can store
all updates performed on a key to the data store.

NOTE: When you set `MapStoreConfig.setWriteCoalescing()` to `FALSE`, after you
reached per-node maximum write-behind-queue capacity, subsequent put operations
will fail with `ReachedMaxSizeException`. This exception is thrown to prevent
uncontrolled grow of write-behind queues. You can set per-node maximum capacity
using the system property `hazelcast.map.write.behind.queue.capacity`. See the
<<system-properties, System Properties appendix>> for information on this property
and how to set the system properties.


In write-behind mode, when the `map.put(key,value)` call returns:

* in-memory entry is updated
* in-memory backup copies are successfully created on the other cluster members
(if `backup-count` is greater than 0)
* the entry is marked as dirty so that after `write-delay-seconds`, it can be
persisted with `MapStore.store(key,value)` call
* and for fault tolerance, dirty entries are stored in a queue on the primary
member and also on a back-up member.

The same behavior goes for the `map.remove(key)`, the only difference is that
`MapStore.delete(key)` is called when the entry will be deleted.

If `MapStore` throws an exception, then Hazelcast tries to store the entry again.
If the entry still cannot be stored, a log message is printed and the entry is re-queued.

For batch write operations, which are only allowed in write-behind mode,
Hazelcast calls the `MapStore.storeAll(map)` and `MapStore.deleteAll(collection)`
methods to do all writes in a single call.

NOTE: If a map entry is marked as dirty, meaning that it is waiting to be
persisted to the `MapStore` in a write-behind scenario, the eviction process
forces the entry to be stored. This way you have control over the number of
entries waiting to be stored, and thus you can prevent a possible OutOfMemory
exception.

NOTE: `MapStore` or `MapLoader` implementations should not use
Hazelcast Map/Queue/MultiMap/List/Set operations. Your implementation should
only work with your data store. Otherwise, you may get into deadlock situations.

Here is an example configuration:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <map name="default">
        <map-store enabled="true" initial-mode="LAZY">
            <class-name>com.hazelcast.examples.DummyStore</class-name>
            <write-delay-seconds>60</write-delay-seconds>
            <write-batch-size>1000</write-batch-size>
            <write-coalescing>true</write-coalescing>
        </map-store>
    </map>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  map:
    default:
      map-store:
        enabled: true
        initial-mode: LAZY
        class-name: com.hazelcast.examples.DummyStore
        write-delay-seconds: 60
        write-batch-size: 1000
        write-coalescing: true
----

The following are the descriptions of MapStore configuration elements and attributes:

* `class-name`: Name of the class implementing MapLoader and/or MapStore.
* `write-delay-seconds`: Number of seconds to delay to call the
MapStore.store(key, value). If the value is zero then it is write-through,
so the `MapStore.store(key,value)` method is called as soon as the entry is
updated. Otherwise, it is write-behind; so the updates will be stored after
the `write-delay-seconds` value by calling the `Hazelcast.storeAll(map)` method.
Its default value is 0.
* `write-batch-size`: Used to create batch chunks when writing map store. In
default mode, all map entries are tried to be written in one go. To create
batch chunks, the minimum meaningful value for write-batch-size is 2. For values
smaller than 2, it works as in default mode.
* `write-coalescing`: In write-behind mode, Hazelcast coalesces updates on a
specific key by default; it applies only the last update on it. You can set this
element to `false` to store all updates performed on a key to the data store.
* `enabled`: True to enable this map-store, false to disable. Its default value
is true.
* `initial-mode`: Sets the initial load mode. LAZY is the default load mode, where
load is asynchronous. EAGER means load is blocked till all partitions are loaded.
See the <<initializing-map-on-startup, Initializing Map on Startup section>> for
more details.

===== Managing the Lifecycle of a MapLoader

With `MapLoader` (and `MapStore` which extends it), you can do the regular store and load operations.
If you need to perform other operations on create or on destroy of a `MapLoader`,
such as establishing a connection to a database or accessing to other Hazelcast maps,
you need to implement the `MapLoaderLifeCycleSupport` interface. By implementing
it, you will have the `init()` and `destroy()` methods.

The `init()` method initializes the `MapLoader` implementation. Hazelcast calls
this method when the map is first created on a Hazelcast instance. The `MapLoader`
implementation can initialize the required resources
such as reading a configuration file or creating a database connection
or accessing a Hazelcast instance.

The `destroy()` method is called during the graceful shutdown of a Hazelcast instance.
You can override this method  to cleanup the resources held by the `MapLoader` implementation, such as
closing the database connections.

In summary, you need `MapLoaderLifecycleSupport` to perform actions
on create and on destroy of a `MapLoader`.

See link:https://github.com/hazelcast/hazelcast-code-samples/blob/master/hazelcast-integration/mongodb/src/main/java/com/hazelcast/loader/MongoMapStore.java[here^] to see this interface in action.

[[storing-entries-to-multiple-maps]]
===== Storing Entries to Multiple Maps

A configuration can be applied to more than one map using wildcards
(see <<using-wildcards, Using Wildcards>>), meaning that the configuration is
shared among the maps. But `MapStore` does not know which entries to store
when there is one configuration applied to multiple maps.

To store entries when there is one configuration applied to multiple maps,
use Hazelcast's `MapStoreFactory` interface. Using the `MapStoreFactory` interface,
``MapStore``s for each map can be created when a wildcard configuration is used.
Example code is shown below.

[source,java]
----
Config config = new Config();
MapConfig mapConfig = config.getMapConfig( "*" );
MapStoreConfig mapStoreConfig = mapConfig.getMapStoreConfig();
mapStoreConfig.setFactoryImplementation( new MapStoreFactory<Object, Object>() {
    @Override
    public MapLoader<Object, Object> newMapStore( String mapName, Properties properties ) {
        return null;
    }
});
----

To initialize the `MapLoader` implementation with the given map name, configuration
properties and the Hazelcast instance, implement the
link:{docBaseUrl}/javadoc/com/hazelcast/map/MapLoaderLifecycleSupport.html[`MapLoaderLifecycleSupport` interface^]
which is described in the previous section.

[[initializing-map-on-startup]]
===== Initializing Map on Startup

To pre-populate the in-memory map when the map is first touched/used,
use the `MapLoader.loadAllKeys` API.

If `MapLoader.loadAllKeys` returns NULL, then nothing will be loaded.
Your `MapLoader.loadAllKeys` implementation can return all or some of the
keys. For example, you may select and return only the keys which are most
important to you that you want to load them while initializing the map.
`MapLoader.loadAllKeys` is the fastest way of pre-populating the map since
Hazelcast optimizes the loading process by having each cluster member load
its owned portion of the entries.

The `InitialLoadMode` configuration parameter in the class
link:{docBaseUrl}/javadoc/com/hazelcast/config/MapStoreConfig.html[MapStoreConfig^]
has two values: `LAZY` and `EAGER`. If `InitialLoadMode` is set to
`LAZY`, data is not loaded during the map creation. If it is set to
`EAGER`, all the data is loaded while the map is created and everything becomes
ready to use. Also, if you add indices to your map with the
link:{docBaseUrl}/javadoc/com/hazelcast/config/IndexConfig.html[IndexConfig^]
class or the <<indexing-queries, `addIndex`>> method, then
`InitialLoadMode` is overridden and `MapStoreConfig` behaves as if `EAGER` mode is on.

Here is the `MapLoader` initialization flow:

. When `getMap()` is first called from any member, initialization starts
depending on the value of `InitialLoadMode`. If it is set to `EAGER`,
initialization starts on all partitions as soon as the map is touched,
i.e., all partitions are loaded when `getMap` is called.  If it is set to
`LAZY`, data is loaded partition by partition, i.e., each partition is
loaded with its first touch.
. Hazelcast calls `MapLoader.loadAllKeys()` to get all your
keys on one of the members.
. That member distributes keys to all other members in batches.
. Each member loads values of all its owned keys by calling
`MapLoader.loadAll(keys)`.
. Each member puts its owned entries into the map by calling
`IMap.putTransient(key,value)`.

NOTE: If the load mode is `LAZY` and the `clear()` method is called
(which triggers `MapStore.deleteAll()`), Hazelcast removes **ONLY** the
loaded entries from your map and datastore. Since all the data is not loaded
in this case (`LAZY` mode), please note that there may still be entries
in your datastore.

NOTE: If you do not want the MapStore start to load as soon as the
first cluster member starts, you can use the system property `hazelcast.initial.min.cluster.size`.
For example, if you set its value as `3`, loading process will be
blocked until all three members are completely up.

NOTE: The return type of `loadAllKeys()` is changed from `Set` to `Iterable`
with the release of Hazelcast 3.5. MapLoader implementations from previous
releases are also supported and do not need to be adapted.

[[loading-keys-incrementally]]
===== Loading Keys Incrementally

If the number of keys to load is large, it is more efficient to
load them incrementally rather than loading them all at once. To support
incremental loading, the `MapLoader.loadAllKeys()` method returns an `Iterable`
which can be lazily populated with the results of a database query.

Hazelcast iterates over the `Iterable` and, while doing so, sends out the keys
to their respective owner members. The `Iterator` obtained from `MapLoader.loadAllKeys()`
may also implement the `Closeable` interface, in which case `Iterator` is closed
once the iteration is over. This is intended for releasing resources such as
closing a JDBC result set.

[[forcing-all-keys-to-be-loaded]]
===== Forcing All Keys To Be Loaded

The method `loadAll` loads some or all keys into a data store in order to
optimize the multiple load operations. The method has two signatures; the
same method can take two different parameter lists. One signature loads the
given keys and the other loads all keys. See the example code below.

[source,java]
----
include::{javasource}/dds/map/LoadAll.java[tag=loadall]
----

[[post-processing-objects-in-map-store]]
===== Post-Processing Objects in Map Store

In some scenarios, you may need to modify the object after storing it into
the map store.
For example, you can get an ID or version auto-generated by your database and
then need to modify your object stored in the distributed map, but not to break
the synchronization between the database and the data grid.

To post-process an object in the map store, implement the `PostProcessingMapStore`
interface to put the modified object into the distributed map. This triggers an
extra step of `Serialization`, so use it only when needed. (This is only valid
when using the `write-through` map store configuration.)

Here is an example of post processing map store:

[source,java]
----
class ProcessingStore implements MapStore<Integer, Employee>, PostProcessingMapStore {
    @Override
    public void store( Integer key, Employee employee ) {
        EmployeeId id = saveEmployee();
        employee.setId( id.getId() );
    }
}
----

NOTE: Please note that if you are using a post-processing map store in
combination with the entry processors, post-processed values will not be
carried to backups.

[[accessing-a-database-using-properties]]
===== Accessing a Database Using `Properties`

You can prepare your own `MapLoader` to access a database such as Cassandra
and MongoDB. For this, you can first declaratively specify the database properties
in your `hazelcast.xml` configuration file and then implement the
`MapLoaderLifecycleSupport` interface to pass those properties.

You can define the database properties, such as its URL and name, using the
`properties` configuration element. The following is a configuration example
for MongoDB:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <map name="supplements">
        <map-store enabled="true" initial-mode="LAZY">
            <class-name>com.hazelcast.loader.YourMapStoreImplementation</class-name>
            <properties>
                <property name="mongo.url">mongodb://localhost:27017</property>
                <property name="mongo.db">mydb</property>
                <property name="mongo.collection">supplements</property>
            </properties>
        </map-store>
    </map>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  map:
    supplements:
      map-store:
        enabled: true
        initial-mode: LAZY
        class-name: com.hazelcast.loader.YourMapStoreImplementation
        properties:
          mongo_url: mongodb://localhost:27017
          mongo.db: mydb
          mango.collection: supplements
----

After you specified the database properties in your configuration,
you need to implement the `MapLoaderLifecycleSupport` interface and
give those properties in the `init()` method, as shown below:

[source,java]
----
include::{javasource}/dds/map/YourMapStoreImplementation.java[tag=ymsi]
----

See the full example link:https://github.com/hazelcast/hazelcast-code-samples/tree/master/hazelcast-integration/mongodb[here^].

[[map-mapstore]]
===== MapStore and MapLoader Methods Triggered by IMap Operations

As it is explained in the above sections, you can configure
Hazelcast maps to be backed
by a map store to persist the entries. In this case many of the
IMap methods call
MapLoader or MapStore methods to load, store or remove data. This
section summarizes
these methods. Here are the Hazelcast IMap operations that may
trigger the MapStore or MapLoader methods:

[cols="1a,5a"]
|===
|IMap Method|Impact on the MapStore/MapLoader

|`flush()`
|If the map has a MapStore, this method flushes all the local dirty
entries. It calls the `MapStore.storeAll(Map)` or
`MapStore.deleteAll(Collection)` methods with the elements marked as dirty.

|* `put()`
* `putAll()`
* `putAsync()`
* `tryPut()`
* `putIfAbsent()`
|These methods are used to put entries to the map. They call the
`MapLoader.load(Object)` method for each entry not found in the memory
to load the value from the map store backing the map. They also call the
`MapStore.store(Object, Object)` method for each entry, if write-through
persistence mode is configured before the entry is added into the memory.

|* `set()`
* `setAsync()`
|These methods put an entry into the map without returning the old value.
They call the `MapStore.store(Object, Object)` method if write-through
persistence mode is configured before the entry is added into the memory,
to write the value into the map store.

|`remove()`
|Removes the mapping for a key from the map if it is present. It calls the
`MapLoader.load(Object)` method if no value is found with key in the memory,
to load the value from the map store backing the map. It also calls the
`MapStore.delete(Object)` method if write-through persistence mode is
configured before the value is removed from the memory, to remove the value
from the map store.

|* `removeAll()`
* `delete()`
* `removeAsync()`
* `tryRemove()`
|These methods are used to remove entries from the map for various conditions.
They call the `MapStore.delete(Object)` method if write-through persistence mode
is configured before the value is removed from the memory, to remove the value
from the map store.

| * `setTtl`
| This method updates time-to-live of an existing entry. It calls the `MapLoader.load(Object)`
method if no value is found in the memory. It also calls `EntryStore.store(Object, MetadataAwareValue)`
with the entry whose time-to-live has been updated.

|`clear()`
|It clears the map and deletes the items from the backing map store. It calls
the `MapStore.deleteAll(Collection)` method on each partition with the keys that the
given partition stores.

|`replace()`
|It replaces the entry for a key only if currently mapped to a given value.
It calls the `MapStore.store(Object, Object)` method if write-through persistence
mode is configured before the value is stored in the memory, to write the value into
the map store. 

|* `executeOnKey()`
* `executeOnKeys()`
* `submitToKey()`
* `executeOnAllEntries()`
|These methods apply the user defined entry processors to the entry or entries.
They call the `MapLoader.load(Object)` method if the value with key is not found in the
memory, to load the value from the map store backing the map. If the entry processor
updates the entry and write-through persistence mode is configured, before the value is
stored in memory, they call the `MapStore.store(Object, Object)` method to write the value
into the map store. If the entry processor updates the entry's value to null value and write-through
persistence mode is configured, before the value is removed from the memory, they call the
`MapStore.delete(Object)` method to delete the value from the map store.
|===

[[creating-near-cache-for-map]]
==== Creating Near Cache for Map

The Hazelcast distributed map supports a local Near Cache for
remotely stored entries to increase the performance of local
read operations. See the <<near-cache, Near Cache section>> for a
detailed explanation of the Near Cache feature and its configuration.

[[locking-maps]]
==== Locking Maps

Hazelcast Distributed Map (IMap) is thread-safe to meet your thread
safety requirements. When these requirements increase or you want to
have more control on the concurrency, consider the Hazelcast solutions described here.

Consider the following example:

[source,java]
----
include::{javasource}/dds/map/RacyUpdateMember.java[tag=racy]
----

If the above code is run by more than one cluster member simultaneously,
a race condition is likely. You can solve this condition with Hazelcast
using either pessimistic or optimistic locking.

[[pessimistic-locking]]
===== Pessimistic Locking

One way to solve the race issue is by using pessimistic locking -
lock the map entry until you are finished with it.

To perform pessimistic locking, use the lock mechanism provided by the
Hazelcast distributed map, i.e., the `map.lock` and `map.unlock` methods.
See the below example code.


[source,java]
----
include::{javasource}/dds/map/PessimisticUpdateMember.java[tag=pum]
----

The IMap lock will automatically be collected by the garbage collector
when the lock is released and no other waiting conditions exist on the lock.

The IMap lock is reentrant, but it does not support fairness.

WARNING: In some cases, a client application connected to your
cluster may cause the entries in a map to remain locked
after the application has been restarted (which were already locked
before such a restart). This can be due to the
reasons such as incomplete/incorrect client implementations. In these cases,
you can unlock the entries, either from the thread which locked them
using the `IMap.unlock()` method, or check if the entry is locked
using the `IMap.isLock()` method and then call `IMap.forceUnlock()`.

TIP: For the above case, as a workaround, you can also kill all the applications connected
to the cluster and use the Management Center's scripting functionality to clear the map and
release the locks (instead of using `IMap.forceUnlock()`). Keep in mind that the scripting
functionality is limited to working with maps that have primitive key types, e.g., string keys
and limited to relaying only a single string of output per member to the result panel in the Management Center.

Another way to solve the race issue is by acquiring a predictable `Lock`
object from Hazelcast. This way, every value in the map can be given a lock,
or you can create a stripe of locks.

[[optimistic-locking]]
===== Optimistic Locking

In Hazelcast, you can apply the optimistic locking strategy with the
map's `replace` method. This method compares values in object or data forms
depending on the in-memory format configuration. If the values are equal,
it replaces the old value with the new one. If you want to use your defined
`equals` method, `in-memory-format` should be `OBJECT`. Otherwise, Hazelcast
serializes objects to `BINARY` forms and compares them.

See the below example code.

NOTE: The below example code is intentionally broken.

[source,java]
----
include::{javasource}/dds/map/OptimisticMember.java[tag=om]
----

[[pessimistic-vs-optimistic-locking]]
===== Pessimistic vs. Optimistic Locking

The locking strategy you choose depends on your locking requirements.

Optimistic locking is better for mostly read-only systems. It has a
performance boost over pessimistic locking.

Pessimistic locking is good if there are lots of updates on the same
key. It is more robust than optimistic locking from the perspective of data consistency.

In Hazelcast, use `IExecutorService` to submit a task to a key owner,
or to a member or members. This is the recommended way to perform task executions,
rather than using pessimistic or optimistic locking techniques. `IExecutorService`
has fewer network hops and less data over wire, and tasks are executed very near to the data.
See the <<data-affinity, Data Affinity section>>.

[[solving-the-aba-problem]]
===== Solving the ABA Problem

The ABA problem occurs in environments when a shared resource is
open to change by multiple threads. Even if one thread sees the same value
for a particular key in consecutive reads, it does not mean that nothing
has changed between the reads. Another thread may change the value,
do work and change the value back, while the first thread thinks that nothing has changed.

To prevent these kind of problems, you can assign a version number and
check it before any write to be sure that nothing has changed between consecutive reads.
Although all the other fields are equal, the version field will prevent objects
from being seen as equal. This is the optimistic locking strategy; it is used in
environments that do not expect intensive concurrent changes on a specific key.

In Hazelcast, you can apply the <<optimistic-locking, optimistic locking>>
strategy with the map `replace` method.

[[lock-split-brain-protection-with-pessimistic-locking]]
===== Lock Split-Brain Protection with Pessimistic Locking

Locks can be configured to check the number of currently present members
before applying a locking operation. If the check fails, the lock operation
fails with a `SplitBrainProtectionException` (see the <<split-brain-protection, Split-Brain Protection section>>).
As pessimistic locking uses lock operations internally, it also uses the configured
lock split-brain protection. This means that you can configure a lock split-brain protection with the same name or a
pattern that matches the map name. Note that the split-brain protection for IMap locking actions can be
different from the split-brain protection for other IMap actions.

The following actions check for lock split-brain protection before being applied:

* `IMap.lock(K)` and `IMap.lock(K, long, java.util.concurrent.TimeUnit)`
* `IMap.isLocked()`
* `IMap.tryLock(K)`, `IMap.tryLock(K, long, java.util.concurrent.TimeUnit)` and
`IMap.tryLock(K, long, java.util.concurrent.TimeUnit, long, java.util.concurrent.TimeUnit)`
* `IMap.unlock()`
* `IMap.forceUnlock()`
* `MultiMap.lock(K)` and `MultiMap.lock(K, long, java.util.concurrent.TimeUnit)`
* `MultiMap.isLocked()`
* `MultiMap.tryLock(K)`, `MultiMap.tryLock(K, long, java.util.concurrent.TimeUnit)`
and `MultiMap.tryLock(K, long, java.util.concurrent.TimeUnit, long, java.util.concurrent.TimeUnit)`
* `MultiMap.unlock()`
* `MultiMap.forceUnlock()`

An example of declarative configuration:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <map name="myMap">
        <split-brain-protection-ref>map-actions-split-brain-protection</split-brain-protection-ref>
    </map>
    <lock name="myMap">
        <split-brain-protection-ref>map-lock-actions-split-brain-protection</split-brain-protection-ref>
    </lock>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  map:
    myMap:
      split-brain-protection-ref: map-actions-split-brain-protection
  lock:
    myMap:
      split-brain-protection-ref: map-lock-actions-split-brain-protection
----

Here the configured map uses the `map-lock-actions-split-brain-protection` for
map lock actions and the `map-actions-split-brain-protection` for other map actions.

[[accessing-entry-statistics]]
==== Accessing Map and Entry Statistics

You can retrieve the statistics of the map in your Hazelcast IMDG member
using the `getLocalMapStats()` method, which is the programmatic approach.
It returns information such as primary and backup entry count, last update
time and locked entry count. If you need the cluster-wide map statistics, you can
get the local map statistics from all members of the cluster and combine them.
Alternatively, you can see the map statistics on the
link:https://docs.hazelcast.org/docs/management-center/latest/manual/html/index.html#managing-maps[Hazelcast Management Center^].

To be able to retrieve the map statistics, the `statistics-enabled`
element under the map configuration should be set as `true`, which is the default value:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <map name="myMap">
        <statistics-enabled>true</statistics-enabled>
    </map>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  map:
    myMap:
      statistics-enabled: true
----

When this element is set to `false`, the statistics are not gathered
for the map and cannot be seen on the Hazelcast Management Center, nor retrieved
by the `getLocalMapStats()` method.

Hazelcast also keeps statistics about each map entry, such as creation time,
last update time, last access time, and number of hits and version. To access
the map entry statistics, use an `IMap.getEntryView(key)` call. Here is an example.

[source,java]
----
HazelcastInstance hz = Hazelcast.newHazelcastInstance();
EntryView entry = hz.getMap( "quotes" ).getEntryView( "1" );
System.out.println ( "size in memory  : " + entry.getCost() );
System.out.println ( "creationTime    : " + entry.getCreationTime() );
System.out.println ( "expirationTime  : " + entry.getExpirationTime() );
System.out.println ( "number of hits  : " + entry.getHits() );
System.out.println ( "lastAccessedTime: " + entry.getLastAccessTime() );
System.out.println ( "lastUpdateTime  : " + entry.getLastUpdateTime() );
System.out.println ( "version         : " + entry.getVersion() );
System.out.println ( "key             : " + entry.getKey() );
System.out.println ( "value           : " + entry.getValue() );
----

[[listening-to-map-entries-with-predicates]]
==== Listening to Map Entries with Predicates

You can listen to the modifications performed on specific map entries.
You can think of it as an entry listener with predicates. 

NOTE: See the
<<listening-for-map-events, Listening for Map Events section>> for
information on the listeners for Hazelcast maps and how to use them.

NOTE: The default backwards-compatible event publishing strategy only publishes
`UPDATED` events when map entries are updated to a value that
matches the predicate with which the listener was registered.
This implies that when using the default event publishing strategy,
your listener is not notified about an entry whose
value is updated from one that matches the predicate to a new value
that does not match the predicate.

Since version 3.7, when you configure Hazelcast members with property
`hazelcast.map.entry.filtering.natural.event.types` set to `true`,
handling of entry updates conceptually treats value transition as entry,
update or exit with regards to the predicate value space.
The following table compares how a listener is notified about an update
to a map entry value under the default
backwards-compatible Hazelcast behavior (when property
`hazelcast.map.entry.filtering.natural.event.types` is not set or is set
to `false`) versus when set to `true`:

|===

|| Default | `hazelcast.map.entry.filtering.natural.event.types = true`

| When old value matches predicate, new value does not match predicate
| No event is delivered to entry listener
| `REMOVED` event is delivered to entry listener

| When old value matches predicate, new value matches predicate
| `UPDATED` event is delivered to entry listener
| `UPDATED` event is delivered to entry listener

| When old value does not match predicate, new value does not match predicate
| No event is delivered to entry listener
| No event is delivered to entry listener

| When old value does not match predicate, new value matches predicate
| `UPDATED` event is delivered to entry listener
| `ADDED` event is delivered to entry listener
|===

As an example, let's listen to the changes made on an employee
with the surname "Smith". First, let's create the `Employee` class.

[source,java]
----
include::{javasource}/dds/map/Employee.java[tag=emp]
----

Then, let's create a listener with predicate by adding a listener
that tracks `ADDED`, `UPDATED` and `REMOVED` entry events with the `surname` predicate.

[source,java]
----
include::{javasource}/dds/map/ListenerWithPredicate.java[tag=lwp]
----


And now, let's play with the employee "smith" and see how that employee is listened to.

[source,java]
----
include::{javasource}/dds/map/Modify.java[tag=modify]
----


When you first run the class `ListenerWithPredicate` and then run `Modify`,
an output similar to the one below appears.

```
entryAdded:EntryEvent {Address[192.168.178.10]:5702} key=1,oldValue=null,
value=Person{name= smith }, event=ADDED, by Member [192.168.178.10]:5702
```

NOTE: See the <<continuous-query-cache, Continuous Query Cache section>>
for more information.

[[removing-map-entries-in-bulk-with-predicates]]
==== Removing Map Entries in Bulk with Predicates

You can remove all map entries that match your predicate. For this,
Hazelcast offers the method `removeAll()`. Its syntax is as follows:

[source,java]
----
void removeAll(Predicate<K, V> predicate);
----

Normally the map entries matching the predicate are found with a full scan
of the map. If the entries are indexed, Hazelcast uses the index search to find them.
With index, you can expect that finding the entries is faster.


NOTE: When `removeAll()` is called, ALL entries in the caller member's
Near Cache are also removed.

[[adding-interceptors]]
==== Adding Interceptors

You can add intercept operations and execute your own business logic
synchronously blocking the operations. You can change the returned value
from a `get` operation, change the value in `put`, or `cancel` operations
by throwing an exception.

Interceptors are different from listeners. With listeners, you take an action
after the operation has been completed. Interceptor actions are synchronous and
you can alter the behavior of operation, change its values, or totally cancel it.

Map interceptors are chained, so adding the same interceptor multiple times to the
same map can result in duplicate effects. This can easily happen when the interceptor
is added to the map at member initialization, so that each member adds the same interceptor.
When you add the interceptor in this way, be sure to implement the `hashCode()`
method to return the same value for every instance of the interceptor.
It is not strictly necessary, but it is a good idea to also implement `equals()`
as this ensures that the map interceptor can be removed reliably.

The IMap API has two methods for adding and removing an interceptor to the map:
`addInterceptor` and `removeInterceptor`. See also the
link:{docBaseUrl}/javadoc/com/hazelcast/map/MapInterceptor.html[`MapInterceptor` interface^]
to learn about the methods used to intercept the changes in a map.

The following is an example usage.

[source,java]
----
include::{javasource}/dds/map/MapInterceptorMember.java[tag=mim]
----

[[preventing-out-of-memory-exceptions]]
==== Preventing Out of Memory Exceptions

It is very easy to trigger an out of memory exception (OOME) with query-based map methods,
especially with large clusters or heap sizes. For example, on a cluster with five members
having 10 GB of data and 25 GB heap size per member, a single call of `IMap.entrySet()`
fetches 50 GB of data and crashes the calling instance.

A call of `IMap.values()` may return too much data for a single member.
This can also happen with a real query and an unlucky choice of predicates,
especially when the parameters are chosen by a user of your application.

To prevent this, you can configure a maximum result size limit for query based operations.
This is not a limit like `SELECT * FROM map LIMIT 100`, which you can achieve by a
<<filtering-with-paging-predicates, Paging Predicate>>. A maximum result size limit
for query based operations is meant to be a last line of defense to prevent your members
from retrieving more data than they can handle.

The Hazelcast component which calculates this limit is the `QueryResultSizeLimiter`.

[[setting-query-result-size-limit]]
===== Setting Query Result Size Limit

If the `QueryResultSizeLimiter` is activated, it calculates a result size limit per partition.
Each `QueryOperation` runs on all partitions of a member, so it collects result entries
as long as the member limit is not exceeded. If that happens, a
`QueryResultSizeExceededException` is thrown and propagated to the calling instance.

This feature depends on an equal distribution of the data on the cluster members to
calculate the result size limit per member. Therefore, there is a minimum value defined
in `QueryResultSizeLimiter.MINIMUM_MAX_RESULT_LIMIT`. Configured values below the minimum
will be increased to the minimum.

[[local-pre-check]]
===== Local Pre-check

In addition to the distributed result size check in the `QueryOperations`,
there is a local pre-check on the calling instance. If you call the method from a client,
the pre-check is executed on the member that invokes the `QueryOperations`.

Since the local pre-check can increase the latency of a `QueryOperation`,
you can configure how many local partitions should be considered for the pre-check,
or you can deactivate the feature completely.

[[scope-of-result-size-limit]]
===== Scope of Result Size Limit

Besides the designated query operations, there are other operations that use predicates internally.
Those method calls throw the `QueryResultSizeExceededException` as well.
See the following matrix for the methods that are covered by the query result size limit.

image::Map-QueryResultSizeLimiterScope.png[Methods Covered by Query Result Size Limit]

[[configuring-query-result-size]]
===== Configuring Query Result Size

The query result size limit is configured via the following system properties.

* `hazelcast.query.result.size.limit`: Result size limit for query operations on maps.
This value defines the maximum number of returned elements for a single query result.
If a query exceeds this number of elements, a QueryResultSizeExceededException is thrown.
* `hazelcast.query.max.local.partition.limit.for.precheck`: Maximum value of local partitions
to trigger local pre-check for `Predicates#alwaysTrue()` query operations on maps.

See the <<system-properties, System Properties appendix>> to see the full descriptions
of these properties and how to set them.

[[queue]]
=== Queue

Hazelcast distributed queue is an implementation of `java.util.concurrent.BlockingQueue`.
Being distributed, Hazelcast distributed queue enables all cluster members to interact with it.
Using Hazelcast distributed queue, you can add an item in one cluster member and remove it from another one.

[[getting-a-queue-and-putting-items]]
==== Getting a Queue and Putting Items

Use the Hazelcast instance's `getQueue` method to get the queue, then use the queue's
`put` method to put items into the queue.

[source,java]
----
include::{javasource}/dds/queue/ExampleQueue.java[tag=samplequeue]
----

FIFO ordering applies to all queue operations across the cluster. The user objects
(such as `MyTask` in the example above) that are enqueued or dequeued have to be `Serializable`.

Hazelcast distributed queue performs no batching while iterating over the queue.
All items are copied locally and iteration occurs locally.

Hazelcast distributed queue uses `ItemListener` to listen to the events that occur
when items are added to and removed from the queue. See the <<listening-for-item-events,
Listening for Item Events section>> for information on how to create an item listener
class and register it.

[[creating-an-example-queue]]
==== Creating an Example Queue

The following example code illustrates a distributed queue that connects a producer and consumer.

[[putting-items-on-the-queue]]
===== Putting Items on the Queue

Let's `put` one integer on the queue every second, 100 integers total.

[source,java]
----
include::{javasource}/dds/queue/ProducerMember.java[tag=producer]
----


`Producer` puts a **-1** on the queue to show that the ``put``s are finished.

[[taking-items-off-the-queue]]
===== Taking Items off the Queue

Now, let's create a `Consumer` class to `take` a message from this queue, as shown below.

[source,java]
----
include::{javasource}/dds/queue/ConsumerMember.java[tag=consumer]
----

As seen in the above example code, `Consumer` waits five seconds before it consumes
the next message. It stops once it receives **-1**. Also note that `Consumer`
puts **-1** back on the queue before the loop is ended.

When you first start `Producer` and then start `Consumer`, items produced on the
queue will be consumed from the same queue.

[[balancing-the-queue-operations]]
===== Balancing the Queue Operations

From the above example code, you can see that an item is produced every second and
consumed every five seconds. Therefore, the consumer keeps growing. To balance the
produce/consume operation, let's start another consumer. This way, consumption is
distributed to these two consumers, as seen in the example outputs below.

The second consumer is started. After a while, here is the first consumer output:

```
...
Consumed 13
Consumed 15
Consumer 17
...
```

Here is the second consumer output:

```
...
Consumed 14
Consumed 16
Consumer 18
...
```

In the case of a lot of producers and consumers for the queue, using a list of
queues may solve the queue bottlenecks. In this case, be aware that the order of the
messages sent to different queues is not guaranteed. Since in most cases strict ordering
is not important, a list of queues is a good solution.

NOTE: The items are taken from the queue in the same order they were put on the queue.
However, if there is more than one consumer, this order is not guaranteed.

[[itemids-when-offering-items]]
===== ItemIDs When Offering Items

Hazelcast gives an `itemId` for each item you offer, which is an incrementing sequence
identification for the queue items. You should consider the following to understand the
`itemId` assignment behavior:

* When a Hazelcast member has a queue and that queue is configured to have at least one
backup, and that member is restarted, the `itemId` assignment resumes from the last known
highest `itemId` before the restart; `itemId` assignment does not start from the beginning for the new items.
* When the whole cluster is restarted, the same behavior explained in the above
consideration applies if your queue has a persistent data store (`QueueStore`).
If the queue has `QueueStore`, the `itemId` for the new items are given, starting
from the highest `itemId` found in the IDs returned by the method `loadAllKeys`.
If the method `loadAllKeys` does not return anything, the ``itemId``s starts
from the beginning after a cluster restart.
* The above two considerations mean there are no duplicated ``itemId``s in the memory
or in the persistent data store.

[[setting-a-bounded-queue]]
==== Setting a Bounded Queue

A bounded queue is a queue with a limited capacity. When the bounded queue is full,
no more items can be put into the queue until some items are taken out.

To turn a Hazelcast distributed queue into a bounded queue, set the capacity limit
with the `max-size` property. You can set the `max-size` property in the configuration,
as shown below. The `max-size` element specifies the maximum size of the queue.
Once the queue size reaches this value, `put` operations are blocked until the
queue size goes below `max-size`, which happens when a consumer removes items from the queue.

Let's set **10** as the maximum size of our example queue in <<creating-an-example-queue,
Creating an Example Queue>>.

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <queue name="queue">
        <max-size>10</max-size>
    </queue>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  queue:
    queue:
      max-size: 10
----

When the producer is started, ten items are put into the queue and then the queue
will not allow more `put` operations. When the consumer is started, it will remove
items from the queue. This means that the producer can `put` more items into the
queue until there are ten items in the queue again, at which point the `put` operation
again becomes blocked.

In this example code, the producer is five times faster than the consumer.
It will effectively always be waiting for the consumer to remove items before
it can put more on the queue. For this example code, if maximum throughput is the goal,
it would be a good option to start multiple consumers to prevent the queue from filling up.

[[queueing-with-persistent-datastore]]
==== Queueing with Persistent Datastore

Hazelcast allows you to load and store the distributed queue items from/to a persistent
datastore using the interface `QueueStore`. If queue store is enabled, each item added to
the queue is also stored at the configured queue store. When the number of items in the
queue exceeds the memory limit, the subsequent items are persisted in the queue store,
they are not stored in the queue memory.

The `QueueStore` interface enables you to store, load and delete queue items with methods like
`store`, `storeAll`, `load` and `delete`. The following example class includes all of the `QueueStore` methods.

[source,java]
----
include::{javasource}/dds/queue/TheQueueStore.java[tag=qs]
----

`Item` must be serializable. The following is an example queue store configuration.

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <queue name="queue">
        <max-size>10</max-size>
        <queue-store>
            <class-name>com.hazelcast.QueueStoreImpl</class-name>
            <properties>
                <property name="binary">false</property>
                <property name="memory-limit">1000</property>
                <property name="bulk-load">500</property>
            </properties>
        </queue-store>
    </queue>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  queue:
    queue:
      max-size: 10
      queue-store:
        class-name: com.hazelcast.QueueStoreImpl
        properties:
          binary: false
          memory-limit: 1000
          bulk-load: 500
----

The following are the descriptions for each queue store property:

* **Binary**: By default, Hazelcast stores the queue items in serialized form,
and before it inserts the queue items into the queue store, it deserializes them.
If you are not reaching the queue store from an external application, you might
prefer that the items be inserted in binary form. Do this by setting the `binary`
property to true: then you can get rid of the deserialization step, which is a performance
optimization. The `binary` property is false by default.
* **Memory Limit**: This is the number of items after which Hazelcast stores items
only to the datastore. For example, if the memory limit is 1000, then the 1001st item
is put only to the datastore. This feature is useful when you want to avoid out-of-memory
conditions. If you want to always use memory, you can set it to `Integer.MAX_VALUE`.
The default number for `memory-limit` is 1000.
* **Bulk Load**: When the queue is initialized, items are loaded from `QueueStore`
in bulks. Bulk load is the size of these bulks. The default value of `bulk-load` is 250.

[[split-brain-protection-for-queue]]
==== Split-Brain Protection for Queue

Queues can be configured to check for a minimum number of available members before
applying queue operations (see the <<split-brain-protection, Split-Brain Protection section>>).
This is a check to avoid performing successful queue operations on all parts of a cluster during a network partition.

The following is a list of methods, grouped by the protection types, that support split-brain protection checks:

* WRITE, READ_WRITE
** `Collection.addAll()`
** `Collection.removeAll()`, `Collection.retainAll()`
** `BlockingQueue.offer()`, `BlockingQueue.add()`, `BlockingQueue.put()`
** `BlockingQueue.drainTo()`
** `IQueue.poll()`, `Queue.remove()`, `IQueue.take()`
** `BlockingQueue.remove()`
* READ, READ_WRITE
** `Collection.clear()`
** `Collection.containsAll()`, `BlockingQueue.contains()`
** `Collection.isEmpty()`
** `Collection.iterator()`, `Collection.toArray()`
** `Queue.peek()`, `Queue.element()`
** `Collection.size()`
** `BlockingQueue.remainingCapacity()`

[[configuring-queue]]
==== Configuring Queue

The following are examples of queue configurations. It includes the
`QueueStore` configuration, which is explained in the <<queueing-with-persistent-datastore,
Queueing with Persistent Datastore>> section.

**Declarative Configuration:**

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <queue name="default">
        <max-size>0</max-size>
        <backup-count>1</backup-count>
        <async-backup-count>0</async-backup-count>
        <empty-queue-ttl>-1</empty-queue-ttl>
        <item-listeners>
            <item-listener>com.hazelcast.examples.ItemListener</item-listener>
        </item-listeners>
        <statistics-enabled>true</statistics-enabled>
        <queue-store>
            <class-name>com.hazelcast.QueueStoreImpl</class-name>
            <properties>
                <property name="binary">false</property>
                <property name="memory-limit">10000</property>
                <property name="bulk-load">500</property>
            </properties>
        </queue-store>
        <split-brain-protection-ref>splitbrainprotection-name</split-brain-protection-ref>
    </queue>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  queue:
    default:
      max-size: 0
      backup-count: 1
      async-backup-count: 0
      empty-queue-ttl: -1
      item-listeners:
        - include-value: true
          class-name: com.hazelcast.examples.ItemListener
      statistics-enabled: true
      queue-store:
        class-name: com.hazelcast.QueueStoreImpl
        properties:
          binary: false
          memory-limit: 1000
          bulk-load: 500
      split-brain-protection-ref: splitbrainprotection-name
----

**Programmatic Configuration:**

[source,java]
----
include::{javasource}/dds/queue/QueueConfiguration.java[tag=queueconf]
----

Hazelcast distributed queue has one synchronous backup by default.
By having this backup, when a cluster member with a queue goes down,
another member having the backup of that queue will continue. Therefore,
no items are lost. You can define the number of synchronous backups for a
queue using the `backup-count` element in the declarative configuration.
A queue can also have asynchronous backups: you can define the number of
asynchronous backups using the `async-backup-count` element.

To set the maximum size of the queue, use the `max-size` element.
To purge unused or empty queues after a period of time, use the `empty-queue-ttl` element.
If you define a value (time in seconds) for the `empty-queue-ttl` element,
then your queue will be destroyed if it stays empty or unused for the time in seconds that you give.

The following is the full list of queue configuration elements with their descriptions:

* `max-size`: Maximum number of items in the queue. It is used to set an
upper bound for the queue. You will not be able to put more items when the
queue reaches to this maximum size whether you have a queue store configured or not.
* `backup-count`: Number of synchronous backups. Queue is a non-partitioned
data structure, so all entries of a queue reside in one partition. When this
parameter is '1', it means there will be one backup of that queue in another
member in the cluster. When it is '2', two members will have the backup.
* `async-backup-count`: Number of asynchronous backups.
* `empty-queue-ttl`: Used to purge unused or empty queues. If you define a
value (time in seconds) for this element, then your queue will be destroyed
if it stays empty or unused for that time.
* `item-listeners`: Adds listeners (listener classes) for the queue items.
You can also set the attribute `include-value` to `true` if you want the item
event to contain the item values. You can set `local` to `true` if you want to
listen to the items on the local member.
* `queue-store`: Includes the queue store factory class name and the properties
*binary*, *memory limit* and *bulk load*. See the <<queueing-with-persistent-datastore, Queueing with Persistent Datastore section>>.
* `statistics-enabled`: Specifies whether the statistics gathering is enabled for your queue.
If set to `false`, you cannot collect statistics in your implementation
(using `getLocalQueueStats()`) and also
link:https://docs.hazelcast.org/docs/management-center/latest/manual/html/index.html#monitoring-queues[Hazelcast Management Center^]
will not show them. Its default value is `true`.
* `split-brain-protection-ref` : Name of the split-brain protection configuration that you want this queue to use.

=== Priority Queue

Priority queue is a regular blocking queue which orders items using a comparator.
Items in this queue do not necessarily follow the FIFO or LIFO order;
you assign a comparator which defines the order in which items will be stored in the queue.
Items with higher priority get polled first, regardless of when they have been added.

Its configuration is same as the regular queue as explained in <<configuring-queue>>
except the additional comparator configuration element. A declarative example
is shown below:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<queue name="default">
    <max-size>10</max-size>
    <backup-count>1</backup-count>
    <item-listeners>
      <item-listener include-value="true">com.hazelcast.examples.ItemListener</item-listener>
    </item-listeners>
    <queue-store>
      <class-name>com.hazelcast.QueueStoreImpl</class-name>
      <properties>
        <property name="binary">false</property>
        <property name="memory-limit">10000</property>
        <property name="bulk-load">500</property>
      </properties>
    </queue-store>
    <priority-comparator-class-name>com.hazelcast.collection.impl.queue.model.PriorityElementComparator</priority-comparator-class-name>
</queue>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
queue:
  default:
    statistics-enabled: true
    max-size: 10
    backup-count: 1
    item-listeners:
      - include-value: true
        class-name: com.hazelcast.examples.ItemListener
    queue-store:
        class-name: com.hazelcast.QueueStoreImpl
        properties:
          binary: false
          memory-limit: 1000
          bulk-load: 500
    priority-comparator-class-name: com.hazelcast.collection.impl.queue.model.PriorityElementComparator
----


The `priority-comparator-class-name` element is the
fully-qualified comparator's class name to be used for the priority queue.
If you do not provide a value, then the queue behaves as a regular FIFO queue.

NOTE: When you provide a comparator, Hazelcast ignores the queue store
`memory-limit` configuration value.

[[multimap]]
=== MultiMap

Hazelcast `MultiMap` is a specialized map where you can store multiple values
under a single key. Just like any other distributed data structure implementation in
Hazelcast, `MultiMap` is distributed and thread-safe.

Hazelcast `MultiMap` is not an implementation of `java.util.Map` due to the difference
in method signatures. It supports most features of Hazelcast Map except for indexing,
predicates and MapLoader/MapStore. The entries are almost evenly
distributed onto all cluster members. However, this distribution is based on the entry keys:
if there are multiple entries having the same key but different values,
such entries are stored on the same member, otherwise they are distributed among the members.
When a new member joins the cluster, the same
ownership logic used in the distributed map applies.

[[getting-a-multimap-and-putting-an-entry]]
==== Getting a MultiMap and Putting an Entry

The following example creates a MultiMap and puts items into it:

[source,java]
----
include::{javasource}/dds/multimap/ExampleMultiMap.java[tag=mm]
----

We use the `getMultiMap` method to create the MultiMap and then use the `put`
method to put an entry into it.

Now let's print the entries in this MultiMap using the following code:

[source,java]
----
include::{javasource}/dds/multimap/PrintMember.java[tag=pm]
----

After you run `ExampleMultiMap`, run `PrintMember`. You will see the key **`a`** has
two values, as shown below:

`b -> [3]`

`a -> [2, 1]`

Hazelcast MultiMap uses `EntryListener` to listen to events which occur when
entries are added to, updated in or removed from the MultiMap. See the
<<listening-for-multimap-events, Listening for MultiMap Events section>>
for information on how to create an entry listener class and register it.

[[configuring-multimap]]
==== Configuring MultiMap

When using MultiMap, the collection type of the values can be either **Set** or **List**.
Configure the collection type with the `valueCollectionType` parameter. If you choose
`Set`, duplicate and null values are not allowed in your collection and ordering is irrelevant.
If you choose `List`, ordering is relevant and your collection can include duplicate but not null values.

You can also enable statistics for your MultiMap with the `statisticsEnabled` parameter.
If you enable `statisticsEnabled`, statistics can be retrieved with `getLocalMultiMapStats()` method.


NOTE: Currently, eviction is not supported for the MultiMap data structure.


The following are the example MultiMap configurations.

**Declarative Configuration:**

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <multimap name="default">
        <backup-count>0</backup-count>
        <async-backup-count>1</async-backup-count>
        <value-collection-type>SET</value-collection-type>
        <entry-listeners>
            <entry-listener include-value="false" local="false" >com.hazelcast.examples.EntryListener</entry-listener>
        </entry-listeners>
        <split-brain-protection-ref>split-brain-protection-name</split-brain-protection-ref>
    </multimap>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  multimap:
    default:
      backup-count: 0
      async-backup-count: 1
      value-collection-type: SET
      entry-listeners:
        - class-name: com.hazelcast.examples.EntryListener
          include-value: false
          local: false
      split-brain-protection-ref: split-brain-protection-name
----

**Programmatic Configuration:**

[source,java]
----
include::{javasource}/dds/multimap/MultiMapConfiguration.java[tag=mmc]
----


The following are the configuration elements and their descriptions:

* `backup-count`: Defines the number of synchronous backups. For example,
if it is set to 1, backup of a partition will be
placed on one other member. If it is 2, it will be placed on two other members.
* `async-backup-count`: The number of asynchronous backups. Behavior is the
same as that of the `backup-count` element.
* `statistics-enabled`: Specifies whether the statistics gathering is enabled for your MultiMap.
If set to `false`, you cannot collect statistics in your implementation
(using `getLocalMultiMapStats()`) and also
link:https://docs.hazelcast.org/docs/management-center/latest/manual/html/index.html#monitoring-multimaps[Hazelcast Management Center^]
will not show them. Its default value is `true`.
* `value-collection-type`: Type of the value collection. It can be `SET` or `LIST`.
* `entry-listeners`: Lets you add listeners (listener classes) for the map entries.
You can also set the attribute
`include-value` to `true` if you want the item event to contain the entry values.
You can set
`local` to `true` if you want to listen to the entries on the local member.
* `split-brain-protection-ref`: Name of the split-brain protection configuration that you want this MultiMap to use.
See the <<split-brain-protection-for-multimap-and-transactionalmultimap, Split-Brain
Protection for MultiMap and TransactionalMultiMap section>>.

[[split-brain-protection-for-multimap-and-transactionalmultimap]]
==== Split-Brain Protection for MultiMap and TransactionalMultiMap

MultiMap & TransactionalMultiMap can be configured to check for a minimum number of
available members before applying their operations (see the <<split-brain-protection, Split-Brain Protection section>>).
This is a check to avoid performing successful queue operations on all parts of a cluster during a network partition.

The following is a list of methods that support split-brain protection checks. The list is grouped by the protection types.

MultiMap:

* WRITE, READ_WRITE:
** `clear`
** `forceUnlock`
** `lock`
** `put`
** `remove`
** `tryLock`
** `unlock`
* READ, READ_WRITE:
** `containsEntry`
** `containsKey`
** `containsValue`
** `entrySet`
** `get`
** `isLocked`
** `keySet`
** `localKeySet`
** `size`
** `valueCount`
** `values`

TransactionalMultiMap:

* WRITE, READ_WRITE:
** `put`
** `remove`
* READ, READ_WRITE:
** `size`
** `get`
** `valueCount`

**Configuring Split-Brain Protection**

Split-brain protection for MultiMap can be configured programmatically using
the method link:{docBaseUrl}/javadoc/com/hazelcast/config/MultiMapConfig.html[setSplitBrainProtectionName()^],
or declaratively using the element `split-brain-protection-ref`. Following is an example declarative configuration:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <multimap name="default">
        <split-brain-protection-ref>splitbrainprotection-name</split-brain-protection-ref>
    </multimap>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  multimap:
    default:
      split-brain-protection-ref: splitbrainprotection-name
----

The value of `split-brain-protection-ref` should be the split-brain protection configuration name which you configured
under the `split-brain-protection` element as explained in the <<split-brain-protection, Split-Brain Protection section>>.

[[set]]
=== Set

Hazelcast Set (`ISet`) is a distributed and concurrent implementation of `java.util.Set`.
It has the following features:

* Hazelcast Set does not allow duplicate elements.
* Hazelcast Set does not preserve the order of elements.
* Hazelcast Set is a non-partitioned data structure: all the data that belongs to
a set lives on one single partition in that member.
* Hazelcast Set cannot be scaled beyond the capacity of a single machine.
Since the whole set lives on a single partition, storing a large amount of
data on a single set may cause memory pressure. Therefore, you should use multiple
sets to store a large amount of data. This way, all the sets are spread across the
cluster, sharing the load.
* A backup of Hazelcast Set is stored on a partition of another member in the cluster
so that data is not lost in the event of a primary member failure.
* All items are copied to the local member and iteration occurs locally.
* The equals method implemented in Hazelcast Set uses a serialized byte version of
objects, as opposed to `java.util.HashSet`.

[[getting-a-set-and-putting-items]]
==== Getting a Set and Putting Items

Use the ``HazelcastInstance``s `getSet` method to get the Set, then use the `add` method to put items into it.

[source,java]
----
include::{javasource}/dds/set/ExampleSet.java[tag=set]
----

Hazelcast Set uses `ItemListener` to listen to events that occur when items are
added to and removed from the Set. See the <<listening-for-item-events, Listening for Item Events section>>
for information on how to create an item listener class and register it.

[[configuring-set]]
==== Configuring Set

The following are the example Hazelcast Set configurations.


**Declarative Configuration:**

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <set name="default">
        <statistics-enabled>false</statistics-enabled>
        <backup-count>1</backup-count>
        <async-backup-count>0</async-backup-count>
        <max-size>10</max-size>
        <item-listeners>
            <item-listener>com.hazelcast.examples.ItemListener</item-listener>
        </item-listeners>
        <split-brain-protection-ref>splitbrainprotection-name</split-brain-protection-ref>
    </set>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  set:
    default:
      statistics-enabled: false
      backup-count: 1
      async-backup-count: 0
      max-size: 10
      item-listeners:
        - class-name: com.hazelcast.examples.ItemListener
      split-brain-protection-ref: splitbrainprotection-name
----

**Programmatic Configuration:**

[source,java]
----
include::{javasource}/dds/set/SetConfiguration.java[tag=sc]
----

Hazelcast Set configuration has the following elements:

* `statistics-enabled`: True (default) if statistics gathering is
enabled on the Set, false otherwise.
* `backup-count`: Count of synchronous backups. Set is a non-partitioned
data structure, so all entries of a Set reside in one partition. When this
parameter is '1', it means there will be one backup of that Set in another
member in the cluster. When it is '2', two members will have the backup.
* `async-backup-count`: Count of asynchronous backups.
* `max-size`: The maximum number of entries for this Set. It can be any number
between 0 and Integer.MAX_VALUE. Its default value is 0, meaning there is no capacity constraint.
* `item-listeners`: Lets you add listeners (listener classes) for the list items.
You can also set the attributes `include-value` to `true` if you want the item event
to contain the item values. You can set `local` to `true` if you want to listen to
the items on the local member.
* `split-brain-protection-ref`: Name of the split-brain protection configuration that you want this Set to use.
See the <<split-brain-protection-for-iset-and-transactionalset, Split-Brain Protection for ISet and TransactionalSet section>>.

[[split-brain-protection-for-iset-and-transactionalset]]
==== Split-Brain Protection for ISet and TransactionalSet

ISet & TransactionalSet can be configured to check for a minimum number of
available members before applying queue operations (see the <<split-brain-protection, Split-Brain Protection section>>).
This is a check to avoid performing successful queue operations on all parts of a cluster during a network partition.

The following is a list of methods, grouped by the protection types, that support
split-brain protection checks:

ISet:

* WRITE, READ_WRITE:
** `add`
** `addAll`
** `clear`
** `remove`
** `removeAll`
* READ, READ_WRITE:
** `contains`
** `containsAll`
** `isEmpty`
** `iterator`
** `size`
** `toArray`


TransactionalSet:

* WRITE, READ_WRITE:
** `add`
** `remove`
* READ, READ_WRITE:
** `size`


**Configuring Split-Brain Protection**

Split-brain protection for ISet can be configured programmatically using
the method link:{docBaseUrl}/javadoc/com/hazelcast/config/SetConfig.html[setSplitBrainProtectionName()^],
or declaratively using the element `split-brain-protection-ref`. The following is an example declarative configuration:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <set name="default">
        <split-brain-protection-ref>splitbrainprotection-name</split-brain-protection-ref>
    </set>
    ...
</hazelcast>

----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  set:
    default:
      split-brain-protection-ref: splitbrainprotection-name
----

The value of `split-brain-protection-ref` should be the split-brain protection configuration name which you
configured under the `split-brain-protection` element as explained in the <<split-brain-protection, Split-Brain Protection section>>.

[[list]]
=== List

Hazelcast List (`IList`) is similar to Hazelcast Set, but it also
allows duplicate elements.

* Besides allowing duplicate elements, Hazelcast List preserves the order of elements.
* Hazelcast List is a non-partitioned data structure where values and each
backup are represented by their own single partition.
* Hazelcast List cannot be scaled beyond the capacity of a single machine.
* All items are copied to local and iteration occurs locally.


'''
NOTE: While IMap and ICache are the recommended data structures to be used by
link:https://jet.hazelcast.org/[Hazelcast Jet^], IList can also be used by it for unit
testing or similar non-production situations. See link:https://docs.hazelcast.org/docs/jet/latest/manual/#imdg-list[here^]
in the Hazelcast Jet Reference Manual to learn how Jet can use IList, e.g., how it can fill
IList with data, consume it in a Jet job and drain the results to another IList.
See also the link:https://jet.hazelcast.org/use-cases/fast-batch-processing/[Fast Batch Processing^]
and link:https://jet.hazelcast.org/use-cases/real-time-stream-processing/[Real-Time Stream Processing^]
use cases for Hazelcast Jet.

[[getting-a-list-and-putting-items]]
==== Getting a List and Putting Items

Use the ``HazelcastInstance``s `getList` method to get the List,
then use the `add` method to put items into it.

[source,java]
----
include::{javasource}/dds/list/ExampleList.java[tag=list]
----

Hazelcast List uses `ItemListener` to listen to events that occur when
items are added to and removed from the List. See the <<listening-for-item-events,
Listening for Item Events section>> for information on how to create an item listener
class and register it.

[[configuring-list]]
==== Configuring List

The following are the example Hazelcast List configurations.

**Declarative Configuration:**

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <list name="default">
        <statistics-enabled>false</statistics-enabled>
        <backup-count>1</backup-count>
        <async-backup-count>0</async-backup-count>
        <max-size>10</max-size>
        <item-listeners>
            <item-listener>
                com.hazelcast.examples.ItemListener
            </item-listener>
        </item-listeners>
        <split-brain-protection-ref>splitbrainprotection-name</split-brain-protection-ref>
    </list>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  list:
    default:
      statistics-enabled: false
      backup-count: 1
      async-backup-count: 0
      max-size: 10
      item-listeners:
        - class-name: com.hazelcast.examples.ItemListener
      split-brain-protection-ref: splitbrainprotection-name
----

**Programmatic Configuration:**

[source,java]
----
include::{javasource}/dds/list/ListConfiguration.java[tag=lc]
----

Hazelcast List configuration has the following elements:

* `statistics-enabled`: True (default) if statistics gathering is
enabled on the list, false otherwise.
* `backup-count`: Number of synchronous backups. List is a non-partitioned
data structure, so all entries of a List reside in one partition. When this
parameter is '1', there will be one backup of that List in another member in
the cluster. When it is '2', two members will have the backup.
* `async-backup-count`: Number of asynchronous backups.
* `max-size`: The maximum number of entries for this List.
* `item-listeners`: Lets you add listeners (listener classes) for the list items.
You can also set the attribute `include-value` to `true` if you want the item event
to contain the item values. You can set the attribute `local` to `true` if you want
to listen the items on the local member.
* `split-brain-protection-ref`: Name of the split-brain protection configuration that you want this List to use.
See the <<split-brain-protection-for-ilist-and-transactionallist, Split-Brain Protection for IList and TransactionalList section>>.

[[split-brain-protection-for-ilist-and-transactionallist]]
==== Split-Brain Protection for IList and TransactionalList

IList & TransactionalList can be configured to check for a minimum
number of available members before applying queue operations (see the <<split-brain-protection, Split-Brain Protection section>>).
This is a check to avoid performing successful queue operations on all parts of a cluster during a network partition.

The following is a list of methods, grouped by the protection types, that support split-brain protection checks:

IList:

* WRITE, READ_WRITE:
** `add`
** `addAll`
** `clear`
** `remove`
** `removeAll`
** `set`
* READ, READ_WRITE:
** `add`
** `contains`
** `containsAll`
** `get`
** `indexOf`
** `isEmpty`
** `iterator`
** `lastIndexOf`
** `listIterator`
** `size`
** `subList`
** `toArray`


TransactionalList:

* WRITE, READ_WRITE:
** `add`
** `remove`
* READ, READ_WRITE:
** `size`


**Configuring Split-Brain Protection**

Split-brain protection for IList can be configured programmatically using
the method link:{docBaseUrl}/javadoc/com/hazelcast/config/ListConfig.html[setSplitBrainProtectionName()^],
or declaratively using the element `split-brain-protection-ref`. Following is an example declarative configuration:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <list name="default">
        <split-brain-protection-ref>splitbrainprotection-name</split-brain-protection-ref>
    </list>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  list:
    default:
      split-brain-protection-ref: splitbrainprotection-name
----

The value of `split-brain-protection-ref` should be the split-brain protection configuration name which you
configured under the `split-brain-protection` element as explained in the <<split-brain-protection, Split-Brain Protection section>>.

[[ringbuffer]]
=== Ringbuffer

Hazelcast Ringbuffer is a replicated but not partitioned data structure that
stores its data in a ring-like structure. You can
think of it as a circular array with a given capacity. Each Ringbuffer has a
tail and a head. The tail is where the items are
added and the head is where the items are overwritten or expired. You can reach
each element in a Ringbuffer using a sequence
ID, which is mapped to the elements between the head and tail (inclusive) of the Ringbuffer.

[[getting-a-ringbuffer-and-reading-items]]
==== Getting a Ringbuffer and Reading Items

Reading from Ringbuffer is simple: get the Ringbuffer with the
HazelcastInstance `getRingbuffer` method, get its current head with
the `headSequence` method and start reading. Use the method `readOne` to
return the item at the
given sequence; `readOne` blocks if no item is available. To read the next item,
increment the sequence by one.

[source,java]
----
include::{javasource}/dds/ringbuffer/ExampleRB.java[tag=rb]
----

By exposing the sequence, you can now move the item from the Ringbuffer
as long as the item is still available. If the item is not available
any longer, `StaleSequenceException` is thrown.

[[adding-items-to-a-ringbuffer]]
==== Adding Items to a Ringbuffer

Adding an item to a Ringbuffer is also easy with the Ringbuffer `add` method:

[source,java]
----
Ringbuffer<String> ringbuffer = hz.getRingbuffer("ExampleRB");
ringbuffer.add("someitem");
----

Use the method `add` to return the sequence of the inserted item; the
sequence value is always unique. You can use this as a
very cheap way of generating unique IDs if you are already using Ringbuffer.

[[iqueue-vs-ringbuffer]]
==== IQueue vs. Ringbuffer

Hazelcast Ringbuffer can sometimes be a better alternative than an
Hazelcast IQueue. Unlike IQueue, Ringbuffer does not remove the items, it only
reads items using a certain position. There are many advantages to this
approach as described below:

* The same item can be read multiple times by the same thread. This is
useful for realizing semantics of read-at-least-once or
read-at-most-once.
* The same item can be read by multiple threads. Normally you could use an
IQueue per thread for the same semantic, but this is
less efficient because of the increased remoting. A take from an IQueue is
destructive, so the change needs to be applied for backup
also, which is why a `queue.take()` is more expensive than a `ringBuffer.read(...)`.
* Reads are extremely cheap since there is no change in the Ringbuffer.
Therefore no replication is required.
* Reads and writes can be batched to speed up performance. Batching can
dramatically improve the performance of Ringbuffer.

[[configuring-ringbuffer-capacity]]
==== Configuring Ringbuffer Capacity

By default, a Ringbuffer is configured with a `capacity` of 10000 items.
This creates an array with a size of 10000. If
a `time-to-live` is configured, then an array of longs is also created that
stores the expiration time for every item.
In a lot of cases you may want to change this `capacity` number to something
that better fits your needs.

Below is a declarative configuration example of a Ringbuffer with a `capacity` of 2000 items.

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <ringbuffer name="rb">
        <capacity>2000</capacity>
    </ringbuffer>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  ringbuffer:
    rb:
      capacity: 2000
----

Currently, Hazelcast Ringbuffer is not a partitioned data structure;
its data is stored in a single partition and the replicas
 are stored in another partition. Therefore, create a Ringbuffer that can
safely fit in a single cluster member.

[[backing-up-ringbuffer]]
==== Backing Up Ringbuffer

Hazelcast Ringbuffer has a single synchronous backup by default. You can control
the Ringbuffer backup just like most of the other Hazelcast
distributed data structures by setting the synchronous and asynchronous backups:
`backup-count` and `async-backup-count`. In the example below, a Ringbuffer is configured with no
synchronous backups and one asynchronous backup:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <ringbuffer name="rb">
        <backup-count>0</backup-count>
        <async-backup-count>1</async-backup-count>
    </ringbuffer>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  ringbuffer:
    rb:
      backup-count: 0
      async-backup-count: 1
----

An asynchronous backup probably gives you better performance. However, there
is a chance that the item added will be lost
when the member owning the primary crashes before the backup could complete.
You may want to consider batching
methods if you need high performance but do not want to give up on consistency.

[[configuring-ringbuffer-time-to-live]]
==== Configuring Ringbuffer Time-To-Live

You can configure Hazelcast Ringbuffer with a time-to-live in seconds. Using
this setting, you can control how long the items remain in
the Ringbuffer before they are expired. By default, the time-to-live is set to 0,
meaning that unless the item is overwritten,
it will remain in the Ringbuffer indefinitely. If you set a time-to-live and an item
is added, then, depending on the Overflow Policy,
either the oldest item is overwritten, or the call is rejected.

In the example below, a Ringbuffer is configured with a time-to-live of 180 seconds.

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <ringbuffer name="rb">
        <time-to-live-seconds>180</time-to-live-seconds>
    </ringbuffer>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  ringbuffer:
    rb:
      time-to-live-seconds: 180
----

[[setting-ringbuffer-overflow-policy]]
==== Setting Ringbuffer Overflow Policy

Using the overflow policy, you can determine what to do if the oldest item
in the Ringbuffer is not old enough to expire when
 more items than the configured Ringbuffer capacity are being added. The below
options are currently available:

* `OverflowPolicy.OVERWRITE`: The oldest item is overwritten.
* `OverflowPolicy.FAIL`: The call is aborted. The methods that make use of the
OverflowPolicy return `-1` to indicate that adding
the item has failed.

Overflow policy gives you fine control on what to do if the Ringbuffer is full.
You can also use the overflow policy to apply
a back pressure mechanism. The following example code shows the usage of an exponential backoff.

[source,java]
----
include::{javasource}/dds/ringbuffer/Writer.java[tag=writer]
----

[[ringbuffer-with-persistent-datastore]]
==== Ringbuffer with Persistent Datastore

Hazelcast allows you to load and store the Ringbuffer items from/to a persistent
datastore using the interface `RingbufferStore`. If a Ringbuffer store is enabled,
each item added to the Ringbuffer will also be stored at the configured Ringbuffer store.

If the Ringbuffer store is configured, you can get items with sequences which are
no longer in the actual Ringbuffer but are only in the Ringbuffer store. This is
probably much slower but still allows you to continue consuming items from the
Ringbuffer even if they are overwritten with newer items in the Ringbuffer.

When a Ringbuffer is being instantiated, it checks if the Ringbuffer store is
configured and requests the latest sequence in the Ringbuffer store. This is to
enable the Ringbuffer to start with sequences larger than the ones in the Ringbuffer
store. In this case, the Ringbuffer is empty but you can still request older items
from it (which will be loaded from the Ringbuffer store).

The Ringbuffer store stores items in the same format as the Ringbuffer. If the
`BINARY` in-memory format is used, the Ringbuffer store must implement the interface
`RingbufferStore<byte[]>` meaning that the Ringbuffer receives items in the binary format.
If the `OBJECT` in-memory format is used, the Ringbuffer store must implement the interface
`RingbufferStore<K>`, where `K` is the type of item being stored (meaning that the Ringbuffer
store receives the deserialized object).

When adding items to the Ringbuffer, the method `storeAll` allows you to store items in batches.

The following example class includes all of the `RingbufferStore` methods.

[source,java]
----
include::{javasource}/dds/ringbuffer/TheRingbufferObjectStore.java[tag=rbstore]
----

`Item` must be serializable. The following is an example of a Ringbuffer with the
Ringbuffer store configured and enabled.

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <ringbuffer name="default">
        <capacity>10000</capacity>
        <time-to-live-seconds>30</time-to-live-seconds>
        <backup-count>1</backup-count>
        <async-backup-count>0</async-backup-count>
        <in-memory-format>BINARY</in-memory-format>
        <ringbuffer-store>
            <class-name>com.hazelcast.RingbufferStoreImpl</class-name>
        </ringbuffer-store>
    </ringbuffer>
    ...
</hazelcast>

----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  ringbuffer:
    default:
      capacity: 10000
      time-to-live-seconds: 30
      backup-count: 1
      async-backup-count: 0
      in-memory-format: BINARY
      ringbuffer-store:
        class-name: com.hazelcast.RingbufferStoreImpl
----

The following are the explanations for the Ringbuffer store configuration elements:

* `class-name: Name of the Ringbuffer store factory class.

[[configuring-ringbuffer-in-memory-format]]
==== Configuring Ringbuffer In-Memory Format

You can configure Hazelcast Ringbuffer with an in-memory format that controls the
format of the Ringbuffer's stored items. By default, `BINARY` in-memory format is used,
meaning that the object is stored in a serialized form. You can select the `OBJECT` in-memory
format, which is useful when filtering is
applied or when the `OBJECT` in-memory format has a smaller memory footprint than `BINARY`.

In the declarative configuration example below, a Ringbuffer is configured with the
`OBJECT` in-memory format:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <ringbuffer name="rb">
        <in-memory-format>OBJECT</in-memory-format>
    </ringbuffer>
    ...
</hazelcast>

----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  ringbuffer:
    rb:
      in-memory-format: OBJECT
----

[[configuring-split-brain-protection-for-ringbuffer]]
==== Configuring Split-Brain Protection for Ringbuffer

Ringbuffer can be configured to check for a minimum number of available members before
applying Ringbuffer operations. This is a check to avoid performing successful Ringbuffer
operations on all parts of a cluster during a network partition and can be configured
using the element `split-brain-protection-ref`. You should set this element's value as the quorum's name,
which you configured under the `split-brain-protection` element as explained in the <<split-brain-protection,
Split-Brain Protection section>>. Following is an example snippet:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <ringbuffer name="rb">
        <split-brain-protection-ref>splitbrainprotection-name</split-brain-protection-ref>
    </ringbuffer>
    ...
</hazelcast>

----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  ringbuffer:
    rb:
      split-brain-protection-ref: splitbrainprotection-name
----

The following is a list of methods, grouped by the protection types, that support split-brain protection checks:

* WRITE, READ_WRITE:
** `add`
** `addAllAsync`
** `addAsync`
* READ, READ_WRITE:
** `capacity`
** `headSequence`
** `readManyAsync`
** `readOne`
** `remainingCapacity`
** `size`
** `tailSequence`

[[adding-batched-items]]
==== Adding Batched Items

In the previous examples, the method `ringBuffer.add()` is used to add an item to the Ringbuffer.
The problems with this method
are that it always overwrites and that it does not support batching. Batching can have a huge
impact on the performance. You can use the method `addAllAsync` to support batching.

See the following example code.

[source,java]
----
List<String> items = Arrays.asList("1","2","3");
CompletionStage<Long> s = rb.addAllAsync(items, OverflowPolicy.OVERWRITE);
// block until all items are added
s.toCompletableFuture().join();
----

In the above case, three strings are added to the Ringbuffer using the policy
`OverflowPolicy.OVERWRITE`. See the <<setting-ringbuffer-overflow-policy, Overflow
Policy section>> for more information.

[[reading-batched-items]]
==== Reading Batched Items

In the previous example, the `readOne` method read items from the Ringbuffer.
`readOne` is simple but not very efficient for the following reasons:

* `readOne` does not use batching.
* `readOne` cannot filter items at the source; the items need to be retrieved
before being filtered.

The method `readManyAsync` can read a batch of items and can filter items at the source.

See the following example code.

[source,java]
----
CompletionStage<ReadResultSet<E>> readManyAsync(
    long startSequence,
    int minCount,
    int maxCount,
    IFunction<E, Boolean> filter);
----

The meanings of the `readManyAsync` arguments are given below:

* `startSequence`: Sequence of the first item to read.
* `minCount`: Minimum number of items to read. If you do not want to block,
set it to 0. If you want to block for at least one item,
set it to 1.
* `maxCount`: Maximum number of the items to retrieve. Its value cannot exceed 1000.
* `filter`: A function that accepts an item and checks if it should be returned.
If no filtering should be applied, set it to `null`.

A full example is given below.

[source,java]
----
long sequence = rb.headSequence();
for(;;) {
    CompletionStage<ReadResultSet<String>> f = rb.readManyAsync(sequence, 1, 10, null);
    CompletionStage<Integer> readCountStage = f.thenApplyAsync(rs -> {
        for (String s : rs) {
            System.out.println(s);
        }
        return rs.readCount();
    });
    sequence += readCountStage.toCompletableFuture().join();
}
----

Please take a careful look at how your sequence is being incremented. You cannot
always rely on the number of items being returned
if the items are filtered out.

There is not any filtering applied in the above example. The following example shows
how you can apply a filter when reading batched items. First, let's create our filter
as shown below:

[source,java]
----
public class FruitFilter implements IFunction<String, Boolean> {
    public FruitFilter() {}

    public Boolean apply(String s) {
        return s.startsWith("a");
    }
}
----

So, the `FruitFilter` checks whether a String object starts with the letter "a".
You can see this filter in action in the below example:

[source,java]
----
HazelcastInstance hz = Hazelcast.newHazelcastInstance();
Ringbuffer<String> rb = hz.getRingbuffer("rb");

rb.add("apple");
rb.add("orange");
rb.add("pear");
rb.add("peach");
rb.add("avocado");

long sequence = rb.headSequence();
CompletableFuture<ReadResultSet<String>> f = rb.readManyAsync(sequence, 2, 5, new FruitFilter()).toCompletableFuture();

ReadResultSet<String> rs = f.join();
for (String s : rs) {
    System.out.println(s);
}
----

[[using-async-methods]]
==== Using Async Methods

Hazelcast Ringbuffer provides asynchronous methods for more powerful operations
like batched writing or batched reading with filtering.
To wait for the result of the operation in a blocking way, obtain a `CompletableFuture`
from the returned `CompletionStage` by invoking `CompletionStage#toCompletableFuture()` method, then
use either `CompletableFuture#get()` or `CompletableFuture#join()`.

See the following example code.

[source,,java]
----
CompletionStage<Long> f = ringbuffer.addAsync(item, OverflowPolicy.FAIL);
f.toCompletableFuture().get();
----

However, you can also use `CompletionStage` API to add subsequent dependent computation
stages which will be executed when the operation  has completed. This way the thread used for
the call is not blocked until the response is returned.

See the below code as an example of when you want to
get notified when a batch of reads has completed.

[source,java]
----
CompletionStage<ReadResultSet<String>> stage = rb.readManyAsync(sequence, min, max, someFilter);
stage.whenCompleteAsync((response, throwable) -> {
    if (throwable == null) {
         for (String s : response) {
             System.out.println("Received:" + s);
         }
    } else {
        throwable.printStackTrace();
    }
});
----

[[ringbuffer-configuration-examples]]
==== Ringbuffer Configuration Examples

The following shows the declarative configuration of a Ringbuffer called `rb`.
The configuration is modeled after the Ringbuffer defaults.

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <ringbuffer name="rb">
        <capacity>10000</capacity>
        <backup-count>1</backup-count>
        <async-backup-count>0</async-backup-count>
        <time-to-live-seconds>0</time-to-live-seconds>
        <in-memory-format>BINARY</in-memory-format>
        <split-brain-protection-ref>splitbrainprotection-name</split-brain-protection-ref>
    </ringbuffer>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  ringbuffer:
    rb:
      capacity: 10000
      backup-count: 1
      async-backup-count: 0
      time-to-live-seconds: 0
      in-memory-format: BINARY
      split-brain-protection-ref: splitbrainprotection-name
----

You can also configure a Ringbuffer programmatically. The following is a programmatic
version of the above declarative configuration.

[source,java]
----
include::{javasource}/dds/ringbuffer/RBConfiguration.java[tag=rbc]
----

[[topic]]
=== Topic

Hazelcast provides a distribution mechanism for publishing messages that are delivered to multiple subscribers. This is
also known as a publish/subscribe (pub/sub) messaging model. Publishing and subscribing operations are cluster wide.
When a member subscribes to a topic, it is actually registering for messages published by any member in the cluster,
including the new members that joined after you add the listener.

NOTE: Publish operation is async. It does not wait for operations to run in
remote members; it works as fire and forget.

[[getting-a-topic-and-publishing-messages]]
==== Getting a Topic and Publishing Messages

Use the HazelcastInstance's `getTopic` method to get the topic, then use the topic's
`publish` method to publish your messages. The following is an example publisher:

[source,java]
----
include::{javasource}/dds/topic/TopicPublisher.java[tag=tp]
----

And here is an example subscriber:

[source,java]
----
include::{javasource}/dds/topic/TopicSubscriber.java[tag=tsub]
----

Hazelcast Topic uses the `MessageListener` interface to listen for events that occur
when a message is received. See the <<listening-for-topic-messages, Listening for Topic Messages section>>
for information on how to create a message listener class and register it.

[[getting-topic-statistics]]
==== Getting Topic Statistics

Topic has two statistic variables that you can query. These values are incremental and local to the member.

[source,java]
----
include::{javasource}/dds/topic/TopicStats.java[tag=ts]
----

`getPublishOperationCount` and `getReceiveOperationCount` returns the total
number of published and received messages since the start of this member, respectively.
Note that these values are not backed up, so if the member goes down, these values will be lost.

You can disable this feature with topic configuration. See the <<configuring-topic, Configuring Topic section>>.

NOTE: These statistics values can be also viewed in Management Center. See the
link:https://docs.hazelcast.org/docs/management-center/latest/manual/html/index.html#monitoring-topics[Monitoring Topics section^]
in Hazelcast Management Center Reference Manual.

[[understanding-topic-behavior]]
==== Understanding Topic Behavior

Each cluster member has a list of all registrations in the cluster.
When a new member is registered for a topic, it sends a registration message
to all members in the cluster. Also, when a new member joins the cluster, it
receives all registrations made so far in the cluster.

The behavior of a topic varies depending on the value of the configuration
parameter `globalOrderEnabled`.

[[ordering-messages-as-published]]
===== Ordering Messages as Published

If `globalOrderEnabled` is disabled, messages are not ordered and listeners
(subscribers) process the messages in the order that the messages are published.
If cluster member M publishes messages *m1, m2, m3, ..., mn* to a topic **T**,
then Hazelcast makes sure that all of the subscribers of topic **T** receive and
process *m1, m2, m3, ..., mn* in the given order.

Here is how it works: Let's say that we have three members (*member1*, *member2* and
*member3*) and that *member1* and *member2* are registered to a topic named `news`.
Note that all three members know that *member1* and *member2* are registered to `news`.

In this example, *member1* publishes two messages: `a1` and `a2`. *Member3* publishes
two messages: `c1` and `c2`. When *member1* and *member3* publish a message, they check
their local list for registered members, discover that *member1* and *member2* are
in their lists, and then they fire messages to those members. One possible order of
the messages received could be the following.

*member1* -> `c1`, `a1`, `a2`, `c2`

*member2* -> `c1`, `c2`, `a1`, `a2`

[[ordering-messages-for-members]]
===== Ordering Messages for Members

If `globalOrderEnabled` is enabled, all members listening to the same topic
get its messages in the same order.

Here is how it works. Let's say that we have three members (*member1*, *member2* and
*member3*) and that *member1* and *member2* are registered to a topic named `news`.
Note that all three members know that *member1* and *member2* are registered to `news`.

In this example, *member1* publishes two messages: `a1` and `a2`. *Member3* publishes
two messages: `c1` and `c2`. When a member publishes messages over the topic `news`,
it first calculates which partition the `news` ID corresponds to. Then it sends an
operation to the owner of the partition for that member to publish messages. Let's assume
that `news` corresponds to a partition that *member2* owns. *member1* and *member3* first
sends all messages to *member2*. Assume that the messages are published in the following order:

*member1* -> `a1`, `c1`, `a2`, `c2`

*member2* then publishes these messages by looking at registrations in its local list.
It sends these messages to *member1* and *member2* (it makes a local dispatch for itself).

*member1* -> `a1`, `c1`, `a2`, `c2`

*member2* -> `a1`, `c1`, `a2`, `c2`

This way we guarantee that all members see the events in the same order.

[[keeping-generated-and-published-order-the-same]]
===== Keeping Generated and Published Order the Same

In both cases, there is a `StripedExecutor` in EventService that is responsible for
dispatching the received message. For all events in Hazelcast, the order that events
are generated and the order they are published to the user are guaranteed to be the
same via this `StripedExecutor`.

In `StripedExecutor`, there are as many threads as are specified in the property
`hazelcast.event.thread.count` (default is five). For a specific event source (for a
particular topic name), *hash of that source's name % 5* gives the ID of the responsible
thread. Note that there can be another event source (entry listener of a map, item listener
of a collection, etc.) corresponding to the same thread. In order not to make other messages
to block, heavy processing should not be done in this thread. If there is time-consuming work
that needs to be done, the work should be handed over to another thread. See the
<<getting-a-topic-and-publishing-messages, Getting a Topic and Publishing Messages section>>.

[[configuring-topic]]
==== Configuring Topic

To configure a topic, set the topic name, decide on statistics and global ordering,
and set the message listeners.
The following are the default values:

* `global-ordering` is **false**, meaning that by default, there is no guarantee of global order.
* `statistics` is **true**, meaning that by default, statistics are calculated.

You can see the example configuration snippets below.

**Declarative Configuration:**

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <topic name="yourTopicName">
        <global-ordering-enabled>true</global-ordering-enabled>
        <statistics-enabled>true</statistics-enabled>
        <message-listeners>
            <message-listener>MessageListenerImpl</message-listener>
        </message-listeners>
    </topic>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  topic:
    yourTopicName:
      global-ordering-enabled: true
      statistics-enabled: true
      message-listeners:
        - MessageListenerImpl
----

**Programmatic Configuration:**

[source,java]
----
include::{javasource}/dds/topic/TopicConfiguration.java[tag=tc]
----

Topic configuration has the following elements:

* `statistics-enabled`: Specifies whether the statistics gathering is enabled for your topic.
If set to `false`, you cannot collect statistics in your implementation
(using `getLocalTopicStats()`) and also
link:https://docs.hazelcast.org/docs/management-center/latest/manual/html/index.html#monitoring-topics[Hazelcast Management Center^]
will not show them. Its default value is `true`.
* `global-ordering-enabled`: Default is `false`, meaning there is no global order guarantee.
* `message-listeners`: Lets you add listeners (listener classes) for the topic messages.

Besides the above elements, there are the following system properties
that are topic related but not topic specific:

* `hazelcast.event.queue.capacity` with a default value of 1,000,000
* `hazelcast.event.queue.timeout.millis` with a default value of 250
* `hazelcast.event.thread.count` with a default value of 5

For the descriptions of these parameters, see the <<global-event-configuration, Global Event Configuration section>>.

[[reliable-topic]]
=== Reliable Topic

Reliable Topic uses the same `ITopic` interface
as a regular topic. The main difference is that Reliable Topic
is backed up by the Ringbuffer data structure. The following are the advantages of this approach:

* Events are not lost since the Ringbuffer is configured with one
synchronous backup by default.
* Each Reliable `ITopic` gets its own Ringbuffer; if a topic has a
very fast producer, it will not lead to problems at topics that run at a slower pace.
* Since the event system behind a regular `ITopic` is shared with other
data structures, e.g., collection listeners,
  you can run into isolation problems. This does not happen with the Reliable `ITopic`.

Here is an example for a publisher using Reliable Topic:

[source,java]
----
include::{javasource}/dds/reliabletopic/PublisherMember.java[tag=publisher]
----

And the following is an example for the subscriber:

[source,java]
----
include::{javasource}/dds/reliabletopic/SubscribedMember.java[tag=sm]
----

When you create a Reliable Topic, Hazelcast automatically creates a
Ringbuffer for it. You may configure this Ringbuffer by adding a Ringbuffer config
with the same name as the Reliable Topic. For instance, if you have a
Reliable Topic with the name "sometopic", you should add a Ringbuffer config
with the name "sometopic" to configure the backing Ringbuffer. Some of the
things that you may configure are the capacity, the time-to-live for the topic
messages, and you can even add a Ringbuffer store which allows you to have a persistent topic.
By default, a Ringbuffer does not have any TTL (time-to-live) and it has a limited
capacity; you may want to change that configuration. The following is an example
configuration for the "sometopic" given above.

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <!-- This is the ringbuffer that is used by the 'sometopic' Reliable-topic. As you can see the
         ringbuffer has the same name as the topic. -->
    <ringbuffer name="sometopic">
        <capacity>1000</capacity>
        <time-to-live-seconds>5</time-to-live-seconds>
    </ringbuffer>
    <reliable-topic name="sometopic">
        <topic-overload-policy>BLOCK</topic-overload-policy>
    </reliable-topic>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  ringbuffer:
    sometopic:
      capacity: 1000
      time-to-live-seconds: 5
  reliable-topic:
    sometopic:
      topic-overload-policy: BLOCK
----

See the <<configuring-reliable-topic, Configuring Reliable Topic section>>
below for the descriptions of all Reliable Topic configuration elements.

By default, the Reliable `ITopic` uses a shared thread pool. If you need a
better isolation, you can configure a custom executor on the
`ReliableTopicConfig`.

Because the reads on a Ringbuffer are not destructive, batching is easy to apply.
`ITopic` uses read batching and reads
ten items at a time (if available) by default. See <<reading-batched-items, Reading Batched Items>>
for more information.

[[slow-consumers]]
==== Slow Consumers

The Reliable `ITopic` provides control and a way to deal with slow consumers.
It is unwise to keep events for a slow consumer in memory
indefinitely since you do not know when the slow consumer is going to catch up.
You can control the size of the Ringbuffer by using its capacity. For the cases
when a Ringbuffer runs out of its capacity, you can specify the following policies
for the `TopicOverloadPolicy` configuration:

* `DISCARD_OLDEST`: Overwrite the oldest item, even if a TTL is set.
In this case the fast producer supersedes a slow consumer.
* `DISCARD_NEWEST`: Discard the newest item.
* `BLOCK`: Wait until the items are expired in the Ringbuffer.
* `ERROR`: Immediately throw `TopicOverloadException` if there is no space in the Ringbuffer.

[[configuring-reliable-topic]]
==== Configuring Reliable Topic

The following are example Reliable Topic configurations.


**Declarative Configuration:**

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <reliable-topic name="default">
        <statistics-enabled>true</statistics-enabled>
        <message-listeners>
            <message-listener>
                ...
            </message-listener>
        </message-listeners>
        <read-batch-size>10</read-batch-size>
        <topic-overload-policy>BLOCK</topic-overload-policy>
    </reliable-topic>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  reliable-topic:
    default:
      statistics-enabled: true
      message-listeners:
        - ...
      read-batch-size: 10
      topic-overload-policy: BLOCK
----

**Programmatic Configuration:**

[source,java]
----
Config config = new Config();
ReliableTopicConfig rtConfig = config.getReliableTopicConfig( "default" );
rtConfig.setTopicOverloadPolicy( TopicOverloadPolicy.BLOCK )
    .setReadBatchSize( 10 )
    .setStatisticsEnabled( true );
----

Reliable Topic configuration has the following elements:

* `statistics-enabled`: Specifies whether the statistics gathering is
enabled for your Reliable Topic. If set to `false`, you cannot collect statistics
in your implementation and also
link:https://docs.hazelcast.org/docs/management-center/latest/manual/html/index.html#monitoring-reliable-topics[Hazelcast Management Center^]
will not show them. Its default value is `true`.
* `message-listener`: Message listener class that listens to the
messages when they are added or removed.
* `read-batch-size`: Minimum number of messages that Reliable Topic
tries to read in batches. Its default value is 10.
* `topic-overload-policy`: Policy to handle an overloaded topic.
Available values are `DISCARD_OLDEST`, `DISCARD_NEWEST`, `BLOCK` and `ERROR`.
Its default value is `BLOCK`. See <<slow-consumers, Slow Consumers>> for definitions of these policies.

[[lock]]
=== FencedLock

NOTE: `FencedLock` is a member of CP Subsystem API.
For detailed information, see the <<cp-subsystem, CP Subsystem>> chapter.

`FencedLock` is a linearizable and distributed implementation of
`java.util.concurrent.locks.Lock`, meaning that if you lock using a `FencedLock`,
the critical section that it guards is guaranteed to be executed by only one thread
in the entire cluster. Even though locks are great for synchronization, they can lead
to problems if not used properly. Also note that Hazelcast Lock does not support fairness.

NOTE: For detailed information and configuration, see the <<fencedlock, FencedLock section>>
under the CP Subsystem chapter.

[[using-try-catch-blocks-with-locks]]
==== Using Try-Catch Blocks with Locks

Always use locks with *try*-*catch* blocks. This ensures that locks are
released if an exception is thrown from
the code in a critical section. Also note that the `lock` method is outside
the *try*-*catch* block because we do not want to unlock
if the lock operation itself fails.

[source,java]
----
include::{javasource}/dds/lock/ExampleLock.java[tag=lock]
----

[[releasing-locks-with-trylock-timeout]]
==== Releasing Locks with tryLock Timeout

If a lock is not released in the cluster, another thread that is trying to get the
lock can wait forever. To avoid this, use `tryLock` with a timeout value. You can
set a high value (normally it should not take that long) for `tryLock`.
You can check the return value of `tryLock` as follows:

[source,java]
----
if ( lock.tryLock ( 10, TimeUnit.SECONDS ) ) {
  try {
    // do some stuff here..
  } finally {
    lock.unlock();
  }
} else {
  // warning
}
----

[[understanding-lock-behavior]]
==== Understanding Lock Behavior

* Locks are fail-safe. If a member holds a lock and some other members go down,
the cluster will keep your locks safe and available.
Moreover, when a member leaves the cluster, all the locks acquired by that
dead member will be removed so that those
locks are immediately available for live members.
* Locks are not automatically removed. If a lock is not used anymore, Hazelcast
does not automatically perform garbage collection in the lock.
This can lead to an `OutOfMemoryError`. If you create locks on the fly,
make sure they are destroyed.
* Locks are re-entrant. The same thread can lock multiple times on the same lock.
Note that for other threads to be able to require this lock, the owner of the lock
must call `unlock` as many times as the owner called `lock`.


[[iatomiclong]]
=== IAtomicLong

NOTE: `IAtomicLong` is a member of CP Subsystem API.
For detailed information, see the <<cp-subsystem, CP Subsystem>> chapter.

Hazelcast `IAtomicLong` is the distributed implementation of
`java.util.concurrent.atomic.AtomicLong`. It offers most of AtomicLong's operations
such as `get`, `set`, `getAndSet`, `compareAndSet` and `incrementAndGet`.
Since IAtomicLong is a distributed implementation, these operations involve remote
calls and thus their performances differ from AtomicLong.

The following example code creates an instance, increments it
by a million and prints the count.

[source,java]
----
include::{javasource}/dds/iatomiclong/ExampleIAtomicLong.java[tag=ial]
----

When you start other instances with the code above,
you will see the count as *member count* times *a million*.

[[sending-functions-to-iatomiclong]]
==== Sending Functions to IAtomicLong

You can send functions to an IAtomicLong. `IFunction` is a Hazelcast owned,
single method interface. The following example `IFunction` implementation
adds two to the original value.

[source,java]
----
include::{javasource}/dds/iatomiclong/IAtomicLongExecuteFuncs.java[tag=add2func]
----

[[executing-functions-on-iatomiclong]]
==== Executing Functions on IAtomicLong

You can use the following methods to execute functions on IAtomicLong:

* `apply`: Applies the function to the value in IAtomicLong without
changing the actual value and returning the result.
* `alter`: Alters the value stored in the IAtomicLong by applying the function.
It does not send back a result.
* `alterAndGet`: Alters the value stored in the IAtomicLong by applying the function,
storing the result in the IAtomicLong and returning the result.
* `getAndAlter`: Alters the value stored in the IAtomicLong by applying the function
and returning the original value.

The following example includes these methods.

[source,java]
----
include::{javasource}/dds/iatomiclong/IAtomicLongExecuteFuncs.java[tag=ialef]
----

The output of the above class when run is as follows:

```
apply.result: 3
apply.value: 1
alter.value: 3
alterAndGet.result: 3
alterAndGet.value: 3
getAndAlter.result: 1
getAndAlter.value: 3
```

[[reasons-to-use-functions-with-iatomic]]
==== Reasons to Use Functions with IAtomicLong

The reason for using a function instead of a simple code line like
`atomicLong.set(atomicLong.get() + 2));` is that the IAtomicLong read and write
operations are not atomic. Since `IAtomicLong` is a distributed implementation,
those operations can be remote ones, which may lead to race problems. By using functions,
the data is not pulled into the code, but the code is sent to the data. This makes it more scalable.

[[isemaphore]]
=== ISemaphore

NOTE: `ISemaphore` is a member of CP Subsystem API.
For detailed information, see the <<cp-subsystem, CP Subsystem>> chapter.

Hazelcast ISemaphore is the distributed implementation of `java.util.concurrent.Semaphore`.

[[controlling-thread-counts-with-permits]]
==== Controlling Thread Counts with Permits

Semaphores offer **permit**s to control the thread counts when performing concurrent activities.
To execute a concurrent activity, a thread grants a permit or waits until a permit
becomes available. When the execution is completed, the permit is released.

TIP: `ISemaphore` with a single permit may be considered as a lock. Unlike the locks,
when semaphores are used, any thread can release the permit depending on the configuration,
and semaphores can have multiple permits. For more information, see the <<semaphore-configuration, Semaphore Configuration section>>.

WARNING: Hazelcast ISemaphore does not support fairness at all times. There are some edge cases
where the fairness is not honored, e.g., when the permit becomes available
at the time when an internal timeout occurs.

When a permit is acquired on ISemaphore:

* If there are permits, the number of permits in the semaphore is decreased by one
and the calling thread performs its activity. If there is contention,
the longest waiting thread acquires the permit before all other threads.
* If no permits are available, the calling thread blocks until a permit
becomes available. When a timeout happens during this block, the thread is interrupted.

[[example-semaphore-code]]
==== Example Semaphore Code

The following example code uses an `IAtomicLong` resource 1000 times,
increments the resource when a thread starts to use it and decrements
it when the thread completes.

[source,java]
----
include::{javasource}/dds/semaphore/SemaphoreMember.java[tag=sm]
----

If you execute the above code 5 times,
the following output appears:

`At iteration: 0, Active Threads: 1`

`At iteration: 1, Active Threads: 2`

`At iteration: 2, Active Threads: 3`

`At iteration: 3, Active Threads: 3`

`At iteration: 4, Active Threads: 3`

As you can see, the maximum count of concurrent threads is equal or
smaller than three. If you remove the semaphore acquire/release statements
in in the above example, you will see that there is no limitation on the number
of concurrent usages.

[[iatomicreference]]
=== IAtomicReference

NOTE: `IAtomicReference` is a member of CP Subsystem API.
For detailed information, see the <<cp-subsystem, CP Subsystem>> chapter.

The `IAtomicLong` is very useful if you need to deal with a long,
but in some cases you need to deal with a reference. That is why Hazelcast
also supports the `IAtomicReference` which is the distributed version of
the `java.util.concurrent.atomic.AtomicReference`.

Here is an IAtomicReference example.

[source,java]
----
include::{javasource}/dds/ExampleIAtomicReference.java[tag=iar]
----

When you execute the above example, the output is as follows:

`foo`

[[sending-functions-to-iatomicreference]]
==== Sending Functions to IAtomicReference

Just like `IAtomicLong`, `IAtomicReference` has methods that accept a
'function' as an argument, such as `alter`, `alterAndGet`, `getAndAlter` and `apply`.
There are two big advantages of using these methods:

* From a performance point of view, it is better to send the function to the data
than the data to the function. Often the function is a lot smaller than the data and
therefore cheaper to send over the line. Also the function only needs to be transferred
once to the target machine and the data needs to be transferred twice.
* You do not need to deal with concurrency control. If you would perform a load, transform,
store, you could run into a data race since another thread might have updated the value you
are about to overwrite.

[[using-iatomicreference]]
==== Using IAtomicReference

The following are some considerations you need to know when you use IAtomicReference:

* `IAtomicReference` works based on the byte-content and not on the object-reference.
If you use the `compareAndSet` method, do not change to the original value because its
serialized content will then be different. It is also important to know that if you rely
on Java serialization, sometimes (especially with hashmaps) the same object can result in
different binary content.
* All methods returning an object return a private copy. You can modify the private copy,
but the rest of the world is shielded from your changes. If you want these changes to be
visible to the rest of the world, you need to write the change back to the `IAtomicReference`;
but be careful about introducing a data-race.
* The 'in-memory format' of an `IAtomicReference` is `binary`. The receiving side does
not need to have the class definition available unless it needs to be deserialized on the
other side, e.g., because a method like 'alter' is executed. This deserialization is done
for every call that needs to have the object instead of the binary content, so be careful
with expensive object graphs that need to be deserialized.
* If you have an object with many fields or an object graph and you only need to calculate
some information or need a subset of fields, you can use the `apply` method. With the `apply`
method, the whole object does not need to be sent over the line; only the information that is
relevant is sent.

[[icountdownlatch]]
=== ICountDownLatch

NOTE: `ICountDownLatch` is a member of CP Subsystem API.
For detailed information, see the <<cp-subsystem, CP Subsystem>> chapter.

Hazelcast `ICountDownLatch` is the distributed implementation of
`java.util.concurrent.CountDownLatch`. But unlike Java's implementation,
Hazelcast's `ICountDownLatch` count can be reset after a countdown has finished,
but not during an active count.


[[gate-keeping-concurrent-activities]]
==== Gate-Keeping Concurrent Activities

`ICountDownLatch` is considered to be a gate keeper for concurrent activities.
It enables the threads to wait for other threads to complete their operations.
The following examples describe the mechanism of `ICountDownLatch`.

Assume that there is a leader process and there are follower processes that will
wait until the leader completes. Here is the leader:

[source,java]
----
include::{javasource}/dds/countdownlatch/Leader.java[tag=leader]
----

Since only a single step is needed to be completed as a sample, the above code
initializes the latch with 1. Then, the code sleeps for a while to simulate a
process and starts the countdown. Finally, it clears up the latch. Let's write a follower:

[source,java]
----
include::{javasource}/dds/countdownlatch/Follower.java[tag=follower]
----

The follower class above first retrieves `ICountDownLatch` and then calls the `await`
method to enable the thread to listen for the latch. The method `await` has a timeout
value as a parameter. This is useful when the `countDown` method fails. To see `ICountDownLatch`
in action, start the leader first and then start one or more followers. You will see that the
followers wait until the leader completes.

[[pn-counter]]
=== PN Counter

A Conflict-free Replicated Data Type (CRDT) is a distributed data structure
that achieves high availability by relaxing consistency constraints.
There may be several replicas for the same data and these replicas can be
modified concurrently without coordination. This means that you may achieve
high throughput and low latency when updating a CRDT data structure.
On the other hand, all of the updates are replicated asynchronously.
Each replica then receives updates made on other replicas eventually and
if no new updates are done, all replicas which can communicate to each other
return the same state (converge) after some time.

Hazelcast offers a lightweight CRDT PN counter (Positive-Negative Counter)
implementation where each Hazelcast instance can increment and decrement the
counter value and these updates are propagated to all replicas. Only a Hazelcast
member can store state for a counter which means that counter method invocations
performed on a Hazelcast member are usually local (depending on the configured
replica count). If there is no member failure, it is guaranteed that each replica
sees the final value of the counter eventually. Counter's state converges with each
update and all CRDT replicas that can communicate to each other will eventually have the same state.

Using the PN Counter, you can get a distributed counter, increment and decrement it,
and query its value with RYW (read-your-writes) and monotonic reads. The implementation
borrows most methods from the `AtomicLong` which should be familiar in most cases and
easily interchangeable in the existing code.

Some examples of PN counter are:

* counting the number of "likes" or "+1"
* counting the number of logged in users
* counting the number of page hits/views.

**How it works**

The counter supports adding and subtracting values as well as retrieving
the current counter value. Each replica of this counter can perform operations
locally without coordination with the other replicas, thus increasing availability.
The counter guarantees that whenever two members have received the same set of updates,
possibly in a different order, their state is identical, and any conflicting updates
are merged automatically. If no new updates are made to the shared state, all
members that can communicate will eventually have the same data.

The updates to the counter are applied locally when invoked on a CRDT replica.
A CRDT replica can be any Hazelcast instance **which is NOT a client or a lite member**.
You can configure the number of replicas in the cluster using the `replica-count`
configuration element.

When invoking updates from a non-replica instance, the invocation is remote.
This may lead to indeterminate state - the update may be applied but the response
has not been received. In this case, the caller is notified with a
`TargetDisconnectedException` when invoked from a client or a `MemberLeftException`
when invoked from a member.

The read and write methods provide monotonic read and RYW (read-your-write) guarantees.
These guarantees are session guarantees which mean that if no replica with the previously
observed state is reachable, the session guarantees are lost and the method invocation
throws a `ConsistencyLostException`. This does not mean that an update is lost.
All of the updates are part of some replica and eventually reflected in the state of all other replicas.
This exception just means that you cannot observe your own writes because all replicas
that contain your updates are currently unreachable. After you have received a
`ConsistencyLostException`, you can either wait for a sufficiently up-to-date replica
to become reachable in which case the session can be continued or you can reset the
session by calling the method `reset(). If you have called this method, a new session
is started with the next invocation to a CRDT replica.

NOTE: The CRDT state is kept entirely on non-lite (data) members. If there aren't any
and the methods here are invoked on a lite member, they fail with a `NoDataMemberInClusterException`.


The following is an example code.

[source,java]
----
include::{javasource}/dds/ExamplePNCounter.java[tag=pnc]
----

This code snippet creates an instance of a PN counter, increments it by 5 and retrieves the value.

[[configuring-pn-counter]]
==== Configuring PN Counter

Following is an example declarative configuration snippet:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <pn-counter name="default">
        <replica-count>10</replica-count>
        <statistics-enabled>true</statistics-enabled>
    </pn-counter>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  pn-counter:
    default:
      replica-count: 10
      statistics-enabled: true
----

PN Counter has the following configuration elements:

* `name`: Name of your PN Counter.
* `replica-count`: Number of replicas on which state for this PN counter is kept.
This number applies in quiescent state, if there are currently membership changes or
clusters are merging, the state may be temporarily kept on more replicas.
Its default value is Integer.MAX_VALUE. Generally, keeping the state on more replicas
means that more Hazelcast members are able to perform updates locally but it also means
that the PN counter state is kept on more replicas, increasing the network traffic,
decreasing the speed at which replica states converge and increasing the size of the
PN counter state kept on each replica.
* `statistics-enabled`: Specifies whether the statistics gathering is enabled for your
PN Counter. If set to `false`, you cannot collect statistics in your implementation
(using `getLocalPNCounterStats()`) and also
link:https://docs.hazelcast.org/docs/management-center/latest/manual/html/index.html#monitoring-pn-counters[Hazelcast Management Center^]
will not show them. Its default value is `true`.

Following is an equivalent snippet of Java configuration:

[source,java]
----
include::{javasource}/dds/ExamplePNCounter.java[tag=pncc]
----

[[configuring-the-crdt-replication-mechanism]]
==== Configuring the CRDT Replication Mechanism

NOTE: Configuring the replication mechanism is for advanced use
cases only - usually the default configuration works fine for most cases.

In some cases, you may want to configure the replication mechanism for all
CRDT implementations. The CRDT states are replicated in rounds (the period is
configurable) and in each round the state is replicated up to the configured number
of members. Generally speaking, you may increase the speed at which replicas converge
at the expense of more network traffic or decrease the network traffic at the expense
of slower convergence of replicas.
Hazelcast implements the state-based replication mechanism - the CRDT state for changed
CRDTs is replicated in its entirety to other replicas on each replication round.

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <crdt-replication>
        <max-concurrent-replication-targets>1</max-concurrent-replication-targets>
        <replication-period-millis>1000</replication-period-millis>
    </crdt-replication>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  crdt-replication:
    max-concurrent-replication-targets: 1
    replication-period-millis: 1000
----

CRDT replication has the following configuration elements:

* `max-concurrent-replication-targets`: The maximum number of target members
that we replicate the CRDT states to in one period. A higher count leads to
states being disseminated more rapidly at the expense of burst-like behavior -
one update to a CRDT leads to a sudden burst in the number of replication messages
in a short time interval. Its default value is 1 which means that each replica
replicates state to only one other replica in each replication round.
* `replication-period-millis`: The period between two replications of CRDT states
in milliseconds. A lower value increases the speed at which changes are disseminated
to other cluster members at the expense of burst-like behavior - less updates are
batched together in one replication message, and one update to a CRDT may cause a
sudden burst of replication messages in a short time interval. The value must be a
positive non-null integer. Its default value is 1000 milliseconds which means that
the changed CRDT state is replicated every 1 second.

Following is an equivalent snippet of Java configuration:

[source,java]
----
include::{javasource}/dds/CRDTReplication.java[tag=crdt]
----

[[flakeidgenerator]]
=== Flake ID Generator

Hazelcast Flake ID Generator is used to generate cluster-wide unique identifiers.
Generated identifiers are `long` primitive values and are k-ordered (roughly ordered).
IDs are in the range from 0 to Long.MAX_VALUE.

[[generating-cluster-wide-flake-ids]]
==== Generating Cluster-Wide IDs

The IDs contain timestamp component and a node ID component, which is assigned when the
member joins the cluster. This allows the IDs to be ordered and unique without any
coordination between the members, which makes the generator safe even in split-brain
scenarios (for limitations in this case, see the <<node-id-assignment, Node ID assignment section>> below).

Timestamp component is in milliseconds since 1.1.2018, 0:00 UTC and has 41 bits.
This caps the useful lifespan of the generator to little less than 70 years (until ~2088).
The sequence component is 6 bits. If more than 64 IDs are requested in single millisecond,
IDs gracefully overflow to the next millisecond and uniqueness is guaranteed in this case.
The implementation does not allow overflowing by more than 15 seconds, if IDs are requested
at higher rate, the call blocks. Note, however, that clients are able to generate even faster
because each call goes to a different (random) member and the 64 IDs/ms limit is for single member.

[[flakeidgenerator-performance]]
==== Performance

Operation on member is always local, if the member has valid node ID, otherwise it's remote.
On the client, the `newId()` method goes to a random member and gets a batch of IDs,
which is then returned locally for a limited time. The pre-fetch size and the validity
time can be configured for each client and member.

[[flakeidgenerator-example]]
==== Example

Let's write an example identifier generator.

[source,java]
----
include::{javasource}/dds/ExampleFlakeIdGenerator.java[tag=fid]
----

[[node-id-assignment]]
==== Node ID Assignment

Flake IDs require a unique node ID to be assigned to each member, from which point the
member can generate unique IDs without any coordination. Hazelcast uses the member list
version from the moment when the member joined the cluster as a unique node ID.

The join algorithm is specifically designed to ensure that member list join version is unique
for each member in the cluster. This ensures that IDs are unique even during network splits,
with one caveat: at most one member is allowed to join the cluster during a network split.
If two members join different subclusters, they are likely to get the same node ID. This is
resolved when the cluster heals, but until then, they can generate duplicate IDs.

[[node-id-overflow]]
===== Node ID Overflow

Node ID component of the ID has 16 bits. Members with the member list join version higher
than 2^16 won't be able to generate IDs, but functionality is preserved by forwarding to another member.
It is possible to generate IDs on any member or client as long as there is at least one
member with join version smaller than 2^16 in the cluster. The remedy is to restart the cluster:
the node ID component will be reset and assigned starting from zero again. Uniqueness after the
restart will be preserved thanks to the timestamp component.

==== Configuring Flake ID Generator

Following is an example declarative configuration snippet:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <flake-id-generator name="default">
        <prefetch-count>100</prefetch-count>
        <prefetch-validity-millis>600000</prefetch-validity-millis>
        <epoch-start>1514764800000</epoch-start>
        <node-id-offset>0</node-id-offset>
        <bits-sequence>6</bits-sequence>
        <bits-node-id>16</bits-node-id>
        <allowed-future-millis>15000</allowed-future-millis>
        <statistics-enabled>true</statistics-enabled>
    </flake-id-generator>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  flake-id-generator:
    default:
      prefetch-count: 100
      prefetch-validity-millis: 600000
      epoch-start: 1514764800000
      node-id-offset: 0
      bits-sequence: 6
      bits-node-id: 16
      allowed-future-millis: 15000
      statistics-enabled: true
----

The following are the descriptions of configuration elements and attributes:

* `name`: Name of your Flake ID Generator. It is a required attribute.
* `prefetch-count`: Count of IDs which are pre-fetched on the background
when one call to FlakeIdGenerator.newId() is made. Its value must be in the
range 1 -100,000. Its default value is 100. This setting pertains only to
`newId()` calls made on the member that configured it.
* `prefetch-validity-millis`: Specifies for how long the pre-fetched IDs can
be used. After this time elapses, a new batch of IDs are fetched. Time unit is
milliseconds. Its default value is 600,000 milliseconds (10 minutes). The IDs
contain a timestamp component, which ensures a rough global ordering of them.
If an ID is assigned to an object that was created later, it will be out of order.
If ordering is not important, set this value to 0. This setting pertains only to
`newId()` calls made on the member that configured it.
* `epoch-start`: Offset of the timestamp component. Time unit is
milliseconds, default is 1514764800000 (1.1.2018 0:00 UTC)
* `node-id-offset`: Specifies the offset that is added to the node ID assigned
to cluster member for this generator. Might be useful in A/B deployment scenarios
where you have cluster A which you want to upgrade. You create cluster B and
for some time both will generate IDs and you want to have them unique.
In this case, configure node ID offset for generators on cluster B.
* `bits-sequence`: Bit-length of the sequence component. Default value is 6 bits.
* `bits-node-id`: Bit-length of node id component. Default value is 16 bits.
* `allowed-future-millis`: Sets how far to the future is the generator allowed to
generate IDs without blocking. Default is 15 seconds.
* `statistics-enabled`: Specifies whether the statistics gathering is enabled
for your Flake ID Generator. If set to `false`, you cannot collect statistics
in your implementation (using `getLocalFlakeIdGeneratorStats()`) and also
link:https://docs.hazelcast.org/docs/management-center/latest/manual/html/index.html#monitoring-flake-id-generators[Hazelcast Management Center^]
will not show them. Its default value is `true`.

[[replicated-map]]
=== Replicated Map

A Replicated Map is a distributed key-value data structure where
the data is replicated to all members in the cluster.
It provides full replication of entries to all members for high speed access.

The following are the features of Replicated Map:

* When you have a Replicated Map in the  cluster, your clients
can communicate with any cluster member.
* All cluster members are able to perform write operations.
* It supports all methods of the interface `java.util.Map`.
* It supports automatic initial fill up when a new member is started.
* It provides statistics for entry access, write and update so that
you can monitor it using Hazelcast Management Center.
* New members joining to the cluster pull all the data from the existing members.
* You can listen to entry events using listeners. See the
<<using-entrylistener-on-replicated-map, Using EntryListener on Replicated Map section>>.

[[replicating-instead-of-partitioning]]
==== Replicating Instead of Partitioning

A Replicated Map does not partition data
(it does not spread data to different cluster members); instead,
it replicates the data to all members.

Replication leads to higher memory consumption.
However, a Replicated Map has faster read and write access since
the data is available on all members.

Writes could take place on local/remote members in order to provide write-order,
eventually being replicated to all other members.

Replicated Map is suitable for objects, catalog data, or idempotent calculable data
(such as HTML pages). It fully implements the `java.util.Map` interface,
but it lacks the methods from `java.util.concurrent.ConcurrentMap` since
there are no atomic guarantees to writes or reads.

NOTE: If Replicated Map is used from a unisocket client and this
unisocket client is connected to a lite member, the entry listeners cannot be registered/de-registered.

NOTE: You cannot use Replicated Map from a lite member.
A `com.hazelcast.replicatedmap.ReplicatedMapCantBeCreatedOnLiteMemberException`
is thrown if `com.hazelcast.core.HazelcastInstance.getReplicatedMap(name)`
is invoked on a lite member.

[[example-replicated-map-code]]
==== Example Replicated Map Code

Here is an example of Replicated Map code. The HazelcastInstance's
`getReplicatedMap` method gets the Replicated Map, and the Replicated Map's
`put` method creates map entries.

[source,java]
----
include::{javasource}/dds/replicatedmap/FillMapMember.java[tag=fmm]
----

`HazelcastInstance.getReplicatedMap()` returns
`com.hazelcast.core.ReplicatedMap` which, as stated above, extends the
`java.util.Map` interface.

The `com.hazelcast.core.ReplicatedMap` interface has some
additional methods for registering entry listeners or retrieving
values in an expected order.

[[considerations-for-replicated-map]]
==== Considerations for Replicated Map

If you have a large cluster or very high occurrences of updates,
the Replicated Map may not scale linearly as expected since
it has to replicate update operations to all members in the cluster.

Since the replication of updates is performed in an asynchronous manner,
we recommend you enable back pressure in case your system has high occurrences
of updates. See the <<back-pressure, Back Pressure section>> to learn how to enable it.

Replicated Map has an anti-entropy system that converges values to a
common one if some of the members are missing replication updates.

Replicated Map does not guarantee eventual consistency because there are
some edge cases that fail to provide consistency.

Replicated Map uses the internal partition system of Hazelcast in order to
serialize updates happening on the same key at the same time. This happens
by sending updates of the same key to the same Hazelcast member in the cluster.

Due to the asynchronous nature of replication, a Hazelcast member could
die before successfully replicating a "write" operation to other members
after sending the "write completed" response to its caller during the write process.
In this scenario, Hazelcast's internal partition system promotes one of the replicas
of the partition as the primary one. The new primary partition does not have
the latest "write" since the dead member could not successfully replicate the update.
(This leaves the system in a state that the caller is the only one that has the
update and the rest of the cluster have not.) In this case even the anti-entropy
system simply could not converge the value since the source of true information
is lost for the update. This leads to a break in the eventual consistency
because different values can be read from the system for the same key.

Other than the aforementioned scenario, the Replicated Map behaves
like an eventually consistent system with read-your-writes and monotonic-reads consistency.

[[configuration-design-for-replicated-map]]
==== Configuration Design for Replicated Map

There are several technical design decisions you should consider when you configure a Replicated Map.

**Initial Provisioning**

If a new member joins the cluster, there are two ways you can handle
the initial provisioning that is executed to replicate all existing
values to the new member. Each involves how you configure the async fill up.

First, you can configure async fill up to true, which does not block
reads while the fill up operation is underway. That way,
you have immediate access on the new member, but it will take time until
all the values are eventually accessible. Not yet
replicated values are returned as non-existing (null).

Second, you can configure for a synchronous initial fill up (by configuring
the async fill up to false), which blocks every read or write access to the map until the
fill up operation is finished. Use this with caution since it might block your
application from operating.

[[configuring-replicated-map]]
==== Configuring Replicated Map

Replicated Map can be configured programmatically or declaratively.

**Declarative Configuration:**

You can declare your Replicated Map configuration in the Hazelcast configuration
file `hazelcast.xml`. See the following example:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <replicatedmap name="default">
        <in-memory-format>BINARY</in-memory-format>
        <async-fillup>true</async-fillup>
        <statistics-enabled>true</statistics-enabled>
        <entry-listeners>
            <entry-listener include-value="true">
                com.hazelcast.examples.EntryListener
            </entry-listener>
       </entry-listeners>
       <split-brain-protection-ref>splitbrainprotection-name</split-brain-protection-ref>
    </replicatedmap>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  replicatedmap:
    default:
      in-memory-format: BINARY
      async-fillup: true
      statistics-enabled: true
      entry-listeners:
        - class-name: com.hazelcast.examples.EntryListener
      split-brain-protection-ref: splitbrainprotection-name
----

Replicated Map has the following configuration elements:

* `in-memory-format`: Internal storage format.  See the
<<in-memory-format-on-replicated-map, In-Memory Format section>>. Its default value is `OBJECT`.
* `async-fillup`: Specifies whether the Replicated Map is available
for reads before the initial replication is completed. Its default value is `true`.
If set to `false`, i.e., synchronous initial fill up, no exception is thrown when
the Replicated Map is not yet ready, but `null` values can be seen until the initial
replication is completed.
* `statistics-enabled`: Specifies whether the statistics gathering is enabled
for your Replicated Map. If set to `false`, you cannot collect statistics in your
implementation (using `getLocalReplicatedMapStats()`) and also
link:https://docs.hazelcast.org/docs/management-center/latest/manual/html/index.html#monitoring-replicated-maps[Hazelcast Management Center^]
will not show them. Its default value is `true`.
* `entry-listener`: Full canonical classname of the `EntryListener` implementation.
** `entry-listener#include-value`: Specifies whether the event includes the value or not.
Sometimes the key is enough to react on an event. In those situations, setting this value
to `false` saves a deserialization cycle. Its default value is `true`.
** `entry-listener#local`: Not used for Replicated Map since listeners are always local.
* `split-brain-protection-ref`: Name of quorum configuration that you want this Replicated Map to use.
See the <<split-brain-protection-for-replicated-map, Split-Brain Protection for Replicated Map section>>.

**Programmatic Configuration:**

You can configure a Replicated Map programmatically, as you can do for all other
data structures in Hazelcast. You must create the configuration upfront, when you
instantiate the `HazelcastInstance`.
A basic example of how to configure the Replicated Map using the programmatic approach
is shown in the following snippet.

[source,java]
----
include::{javasource}/dds/replicatedmap/ReplicatedMapConfiguration.java[tag=rmc]
----

All properties that can be configured using the declarative configuration
are also available using programmatic configuration
by transforming the tag names into getter or setter names.

[[in-memory-format-on-replicated-map]]
===== In-Memory Format on Replicated Map

Currently, you can use the following `in-memory-format` options with the Replicated Map:

* `OBJECT` (default): The data is stored in deserialized form.
This configuration is the default choice since
the data replication is mostly used for high speed access. Please
be aware that changing the values without a `Map.put()` is
not reflected on the other members but is visible on the changing members
for later value accesses.
* `BINARY`: The data is stored in serialized binary format and has to be
deserialized on every request. This
option offers higher encapsulation since changes to values are always
discarded as long as the newly changed object is
not explicitly `Map.put()` into the map again.

[[using-entrylistener-on-replicated-map]]
==== Using EntryListener on Replicated Map

A `com.hazelcast.core.EntryListener` used on a Replicated Map serves
the same purpose as it would on other
data structures in Hazelcast. You can use it to react on add, update
and remove operations. Replicated Maps do not yet support eviction.

[[difference-in-entrylistener-on-replicated-map]]
===== Difference in EntryListener on Replicated Map

The fundamental difference in Replicated Map behavior, compared to the
other data structures, is that an EntryListener only reflects
changes on local data. Since replication is asynchronous, all listener
events are fired only when an operation is finished
on a local member. Events can fire at different times on different members.

[[example-of-replicated-map-entrylistener]]
===== Example of Replicated Map EntryListener

Here is a code example for using EntryListener on a Replicated Map.

The `HazelcastInstance` s `getReplicatedMap` method gets a
Replicated Map (customers), and the `ReplicatedMap` s `addEntryListener`
method adds an entry listener to the Replicated Map. Then, the `ReplicatedMap` s `put`
method adds a Replicated Map
entry and updates it. The method `remove` removes the entry.

[source,java]
----
include::{javasource}/dds/replicatedmap/ListeningMember.java[tag=lm]
----

[[split-brain-protection-for-replicated-map]]
==== Split-Brain Protection for Replicated Map

Replicated Map can be configured to check for a minimum number of available
members before applying its operations (see the <<split-brain-protection, Split-Brain Protection section>>).
This is a check to avoid performing successful queue operations on all parts of a
cluster during a network partition.

The following is a list of methods, grouped by the protection types, that support split-brain
protection checks:

* WRITE, READ_WRITE:
** `clear`
** `put`
** `putAll`
** `remove`
* READ, READ_WRITE:
** `containsKey`
** `containsValue`
** `entrySet`
** `get`
** `isEmpty`
** `keySet`
** `size`
** `values`

**Configuring Split-Brain Protection**

Split-brain protection for Replicated Map can be configured programmatically
using the method link:{docBaseUrl}/javadoc/com/hazelcast/config/ReplicatedMapConfig.html[setSplitBrainProtectionName()^],
or declaratively using the element `split-brain-protection-ref`. Following is an example declarative configuration:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <replicatedmap name="default">
        <split-brain-protection-ref>splitbrainprotection-name</split-brain-protection-ref>
    </replicatedmap>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  replicatedmap:
    default:
      split-brain-protection-ref: splitbrainprotection-name
----

The value of `split-brain-protection-ref` should be the split-brain protection configuration name which you
configured under the `split-brain-protection` element as explained in the <<split-brain-protection, Split-Brain Protection section>>.

[[cardinality-estimator]]
=== Cardinality Estimator Service

Hazelcast's cardinality estimator service is a data structure which implements
Flajolet's HyperLogLog algorithm for estimating cardinalities of unique objects in theoretically huge data sets.
The implementation offered by Hazelcast includes improvements from Google's
version of the algorithm, i.e., HyperLogLog++.

The cardinality estimator service does not provide any ways to configure
its properties, but rather uses some well tested defaults:

* `P`: Stands for precision with a default value of 14
(using the 14 LSB of the hash for the index)
* `M`: 2 ^ P = 16384 (16K) registers
* `P'`: Stands for sparse precision with a default value of 25
* `Durability`: Count of backups for each estimator with a default value of 2

NOTE: It is important to understand that this data structure is not
100% accurate, it is used to provide estimates. The error rate is typically
a result of `1.04/sqrt(M)` which in our implementation is around 0.81% for high percentiles.

The memory consumption of this data structure is close to 16K despite the
size of elements in the source data set or stream.

There are two phases in using the cardinality estimator.

. Add objects to the instance of the estimator, e.g., for IPs `estimator.add("0.0.0.0.")`.
The provided object is first serialized and then the byte array is used to
generate a hash for that object.
+
NOTE: Objects must be serializable in a form that Hazelcast understands.
+
. Compute the estimate of the set so far `estimator.estimate()`.

See the link:{docBaseUrl}/javadoc/com/hazelcast/cardinality/CardinalityEstimator.html[cardinality estimator Javadoc^]
for more information on its API.

The following is an example code.

[source,java]
----
include::{javasource}/dds/ExampleCardinalityEstimator.java[tag=ces]
----

[[split-brain-protection-for-cardinality-estimator]]
==== Split-Brain Protection for Cardinality Estimator

Cardinality Estimator can be configured to check for a minimum number of
available members before applying its operations (see the <<split-brain-protection,
Split-Brain Protection section>>). This is a check to avoid performing successful queue
operations on all parts of a cluster during a network partition.

The following is a list of methods, grouped by the protection types, that support
split-brain protection checks:

* WRITE, READ_WRITE:
** `add`
** `addAsync`
* READ, READ_WRITE:
** `estimate`
** `estimateAsync`

**Configuring Split-Brain Protection**

Split-brain protection for Cardinality Estimator can be configured
programmatically using the method link:{docBaseUrl}/javadoc/com/hazelcast/config/CardinalityEstimatorConfig.html[setSplitBrainProtectionName()^],
or declaratively using the element `split-brain-protection-ref`.
Following is an example declarative configuration:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <cardinality-estimator name="default">
        <split-brain-protection-ref>splitbrainprotection-name</split-brain-protection-ref>
    </cardinality-estimator>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  cardinality-estimator:
    default:
      split-brain-protection-ref: splitbrainprotection-name
----

The value of `split-brain-protection-ref` should be the split-brain protection configuration name
which you configured under the `split-brain-protection` element as explained in the
<<split-brain-protection, Split-Brain Protection section>>.

**Configuring Merge Policy**

While recovering from a split-brain syndrome, Cardinality Estimator
in the small cluster merges into the bigger cluster based on a configured
merge policy. When an estimator merges into the cluster, an estimator with
the same name might already exist in the cluster.
So the merge policy resolves these kinds of conflicts with different out-of-the-box strategies.
It can be configured programmatically using the method
link:{docBaseUrl}/javadoc/com/hazelcast/config/CardinalityEstimatorConfig.html[setMergePolicyConfig()^],
or declaratively using the element `merge-policy`.
Following is an example declarative configuration:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <cardinality-estimator name="default">
        <merge-policy batch-size="102">HyperLogLogMergePolicy</merge-policy>
    </cardinality-estimator>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  cardinality-estimator:
      merge-policy:
        batch-size: 102
        class-name: HyperLogLogMergePolicy
----

The following out-of-the-box merge policies are available:

* `DiscardMergePolicy`: Estimator from the smaller cluster is discarded.
* `HyperLogLogMergePolicy`: Estimator merges with the existing one,
using the algorithmic merge for HyperLogLog. This is the default policy.
* `PassThroughMergePolicy`: Estimator from the smaller cluster wins.
* `PutIfAbsentMergePolicy`: Estimator from the smaller cluster wins if it doesn't exist in the cluster.

[[event-journal]]
=== Event Journal

The event journal is a distributed data structure that stores the history
of mutation actions on map or cache. Each action on the map or cache which
modifies its contents (such as `put`, `remove` or scheduled tasks which are
not triggered by using the public API) creates an event which is stored in
the event journal. The event stores the event type as well as the key, old
value and updated value for the entry (when applicable). As a user, you can
only append to the journal indirectly by using the map and cache methods or
configuring the expiration and eviction.  By reading from the event journal
you can recreate the state of the map or cache at any point in time.

NOTE: Currently the event journal does not expose a public API for reading the
event journal in Hazelcast IMDG. The event journal can be used to stream event
data to Hazelcast Jet, so it should be used in conjunction with
link:http://jet.hazelcast.org/[Hazelcast Jet^]. Because of this we describe
how to configure it but not how to use it from IMDG in this section. If you
enable and configure the event journal, you may only reach it through private
API and you most probably do not get any benefits but the journal retains events
nevertheless and consumes heap space.

The event journal has a fixed capacity and an expiration time. Internally it is
structured as a ringbuffer (partitioned by ringbuffer item) and shares many
similarities with it.

[[interaction-with-evictions-and-expiration-for-imap]]
==== Interaction with Evictions and Expiration for IMap

Configuring IMap with eviction and expiration can cause the event journal to
contain different events on the different replicas of the same partition. You
can run into issues if you are reading from the event journal and the partition
owner is terminated. A backup replica is then promoted into the partition owner
but the event journal will contain different events. The event count should stay
the same but the entries which you previously thought were evicted and expired
could now be "alive" and vice versa.

This is because eviction and expiration randomly choose entries to be
evicted/expired. The entry is not coordinated between partition replicas.
In these cases, the event journal diverges and will not converge at any
future point, but will remain inconsistent just as well as the contents
of the internal record stores are inconsistent between replicas. You may
say that the event journal on a specific replica is in-sync with the record
store on that replica but the event journals and record stores between
replicas are out-of-sync.

[[configuring-event-journal-capacity]]
==== Configuring Event Journal Capacity

By default, an event journal is configured with a `capacity` of 10000 items.
This creates a single array per partition, roughly the size of the capacity
divided by the number of partitions. Thus, if the configured capacity is
10000 and number of partitions is 271, we create 271 arrays of size 36
(10000/271). If
a `time-to-live` is configured, then an array of longs is also created
that stores the expiration time for every item.
A single array of the event journal keeps events that are only related
to the map entries in that partition. In a lot of cases you may want to
change this `capacity` number to something that better fits your needs.
As the capacity is shared between partitions, keep in mind not to set it
to a value which is too low for you. Setting the capacity to a number
lower than the partition count results in an error when initializing the
event journal.

Below is a declarative configuration example of an event journal with a
`capacity` of 5000 items for a map and 10000 items for a cache:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <map name="default">
        <event-journal enabled="true">
            <capacity>5000</capacity>
            <time-to-live-seconds>20</time-to-live-seconds>
        </event-journal>
    </map>
    ...
    <cache name="default">
        <event-journal enabled="true">
            <capacity>10000</capacity>
            <time-to-live-seconds>0</time-to-live-seconds>
        </event-journal>
    </cache>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  map:
    default:
      event-journal:
        enabled: true
        capacity: 5000
        time-to-live-seconds: 20
  cache:
    default:
      event-journal:
        enabled: true
        capacity: 10000
        time-to-live-seconds: 0
----


You can also configure an event journal programmatically. The following
is a programmatic version of the above declarative configuration:

[source,java]
----
include::{javasource}/dds/EventJournalConfiguration.java[tag=ejc]
----

The `mapName` and `cacheName` attributes define the map or cache to which
this event journal configuration applies. You can use pattern-matching and
the `default` keyword when doing so. For instance, by using a `mapName` of
`journaled*`, the journal configuration applies to all maps whose names start
with "journaled" and don't have other journal configurations that match (e.g.,
if you would have a more specific journal configuration with an exact name match).
If you specify the `mapName` or `cacheName` as `default`, the journal configuration
applies to all maps and caches that don't have any other journal configuration.
This means that potentially all maps and/or caches have one single
event journal configuration.

[[event-journal-partitioning]]
==== Event Journal Partitioning

The event journal is a partitioned data structure. The partitioning
is done by the event key. Because of this, the map and cache entry
with a specific key is co-located with the events for that key and
will be migrated accordingly.
Also, the backup count for the event journal is equal to the backup
count of the map or cache for which it contains events. The events on
the backup replicas will be created with the map or cache backup operations
and no additional network traffic is introduced when appending events
to the event journal.

[[configuring-event-journal-ttl]]
==== Configuring Event Journal time-to-live

You can configure Hazelcast event journal with a `time-to-live` in seconds.
Using this setting, you can control how long the items remain in
the event journal before they are expired. By default, the `time-to-live`
is set to 0, meaning that unless the item is overwritten,
it remains in the journal indefinitely. The expiration time of the existing
journal events is checked whenever a new event is appended to the event
journal or when the event journal is being read. If the journal is not
being read from or written to, the journal may keep expired items indefinitely.

In the example below, an event journal is configured with a `time-to-live` of 180 seconds:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <cache name="myCache">
        <event-journal enabled="true">
            <capacity>10000</capacity>
            <time-to-live-seconds>180</time-to-live-seconds>
        </event-journal>
    </cache>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  cache:
    myCache:
      event-journal:
        enabled: true
        capacity: 10000
        time-to-live-seconds: 180
----
