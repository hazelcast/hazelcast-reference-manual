

== Distributed Query

Distributed queries access data from multiple data sources stored on
either the same or different members.

Hazelcast partitions your data and spreads it across cluster of members.
You can iterate over the map entries and look for certain entries
(specified by predicates) you are interested in. However, this is not
very efficient because you have to bring the entire entry set and iterate
locally. Instead, Hazelcast allows you to run distributed queries on your
distributed map.


=== How Distributed Query Works

. The requested predicate is sent to each member in the cluster.
. Each member looks at its own local entries and filters them according
to the predicate. At this stage, key/value pairs of the entries are
deserialized and then passed to the predicate.
. The predicate requester merges all the results coming from each
member into a single set.

Distributed query is highly scalable. If you add new members to the
cluster, the partition count for each member is reduced and thus the
time spent by each member on iterating its entries is reduced. In addition,
the pool of partition threads evaluates the entries concurrently in each
member and the network traffic is also reduced since only filtered data
is sent to the requester.

Hazelcast offers the following APIs for distributed query purposes:

* Criteria API
* Distributed SQL Query


==== Employee Map Query Example

Assume that you have an "employee" map containing values of
`Employee` objects, as coded below.

[source,java]
----
public class Employee implements Serializable {
    private String name;
    private int age;
    private boolean active;
    private double salary;

    public Employee(String name, int age, boolean active, double salary) {
        this.name = name;
        this.age = age;
        this.active = active;
        this.salary = salary;
    }

    public Employee() {
    }

    public String getName() {
        return name;
    }

    public int getAge() {
        return age;
    }

    public double getSalary() {
        return salary;
    }

    public boolean isActive() {
        return active;
    }
}
----

Now let's look for the employees who are active and have an age less
than 30 using the aforementioned APIs (Criteria API and Distributed
SQL Query). The following subsections describe each query mechanism
for this example.

NOTE: When using Portable objects, if one field of an object exists
on one member but does not exist on another one, Hazelcast does not
throw an unknown field exception.
Instead, Hazelcast treats that predicate, which tries to perform a
query on an unknown field, as an always false predicate.


==== Querying with Criteria API

Criteria API is a programming interface offered by Hazelcast that
is similar to the Java Persistence Query Language (JPQL). Below
is the code for the <<employee-map-query-example, above example query>>.

[source,java]
----
IMap<String, Employee> map = hazelcastInstance.getMap( "employee" );

EntryObject e = Predicates.newPredicateBuilder().getEntryObject();
Predicate predicate = e.is( "active" ).and( e.get( "age" ).lessThan( 30 ) );

Collection<Employee> employees = map.values( predicate );
----

In the above example code, `predicate` verifies whether the entry is
active and its `age` value is less than 30. This `predicate` is
applied to the `employee` map using the `map.values(predicate)` method.
This method sends the predicate to all cluster members
and merges the results coming from them. Since the predicate is
communicated between the members, it needs to
be serializable.

NOTE: Predicates can also be applied to `keySet`, `entrySet` and
`localKeySet` of the Hazelcast distributed map.

===== Predicates Class Operators

The `Predicates` class includes many operators for your query requirements.
The following are descriptions for some of them:

* `equal`: Checks if the result of an expression is equal to a given value.
* `notEqual`: Checks if the result of an expression is not equal to a given value.
* `instanceOf`: Checks if the result of an expression has a certain type.
* `like`: Checks if the result of an expression matches some string pattern. %
(percentage sign) is the placeholder for many characters,  (underscore) is
placeholder for only one character.
* `ilike`: A case-insensitive variant of `like`.
* `greaterThan`: Checks if the result of an expression is greater than a
certain value.
* `greaterEqual`: Checks if the result of an expression is greater than or
equal to a certain value.
* `lessThan`: Checks if the result of an expression is less than a certain
value.
* `lessEqual`: Checks if the result of an expression is less than or equal
to a certain value.
* `between`: Checks if the result of an expression is between two values
(this is inclusive).
* `in`: Checks if the result of an expression is an element of a certain
collection.
* `isNot`: Checks if the result of an expression is false.
* `regex`: Checks if the result of an expression matches some regular
expression.
* `alwaysTrue`: The result of an expression always matches.
* `alwaysFalse`: The result of an expression ever matches.


NOTE: See the
link:{docBaseUrl}/javadoc/com/hazelcast/query/Predicates.html[Predicates Javadoc^]
for all predicates provided.


===== Combining Predicates with AND, OR, NOT

You can combine predicates using the `and`, `or` and `not` operators,
as shown in the below examples.

[source,java]
----
public Collection<Employee> getWithNameAndAge( String name, int age ) {
    Predicate namePredicate = Predicates.equal( "name", name );
    Predicate agePredicate = Predicates.equal( "age", age );
    Predicate predicate = Predicates.and( namePredicate, agePredicate );
    return employeeMap.values( predicate );
}
----

[source,java]
----
public Collection<Employee> getWithNameOrAge( String name, int age ) {
    Predicate namePredicate = Predicates.equal( "name", name );
    Predicate agePredicate = Predicates.equal( "age", age );
    Predicate predicate = Predicates.or( namePredicate, agePredicate );
    return employeeMap.values( predicate );
}
----

[source,java]
----
public Collection<Employee> getNotWithName( String name ) {
    Predicate namePredicate = Predicates.equal( "name", name );
    Predicate predicate = Predicates.not( namePredicate );
    return employeeMap.values( predicate );
}
----

===== Simplifying with PredicateBuilder

You can simplify predicate usage with the `PredicateBuilder` interface,
which offers simpler predicate building. See the
below example code which selects all people with a certain name and age.

[source,java]
----
public Collection<Employee> getWithNameAndAgeSimplified( String name, int age ) {
    EntryObject e = Predicates.newPredicateBuilder().getEntryObject();
    Predicate agePredicate = e.get( "age" ).equal( age );
    Predicate predicate = e.get( "name" ).equal( name ).and( agePredicate );
    return employeeMap.values( predicate );
}
----

==== Querying with SQL

`Predicates.sql()` takes the regular SQL `where` clause.
Here is an example:

[source,java]
----
IMap<String, Employee> map = hazelcastInstance.getMap( "employee" );
Set<Employee> employees = map.values( Predicates.sql( "active AND age < 30" ) );
----

NOTE: Hazelcast offers an SQL service that allows to execute SQL queries,
as opposed to SQL-like predicates in case of `Predicates.sql()`. See the
<<sql, SQL chapter>> for more information.

===== Supported SQL Syntax

**AND/OR:** `<expression> AND <expression> AND <expression>... `

* `active AND age>30`
* `active=false OR age = 45 OR name = 'Joe'`
* `active AND ( age > 20 OR salary < 60000 )`

**Equality:** `=, !=, <, <=, >, >=`

* `<expression> = value`
* `age <= 30`
* `name = 'Joe'`
* `salary != 50000`

**BETWEEN: ** `<attribute> [NOT] BETWEEN <value1> AND <value2>`

* `age BETWEEN 20 AND 33 ( same as age >= 20  AND age <= 33 )`
* `age NOT BETWEEN 30 AND 40 ( same as age < 30 OR age > 40 )`


**IN:** `<attribute> [NOT] IN (val1, val2,...)`

* `age IN ( 20, 30, 40 )`
* `age NOT IN ( 60, 70 )`
* `active AND ( salary >= 50000 OR ( age NOT BETWEEN 20 AND 30 ) )`
* `age IN ( 20, 30, 40 ) AND salary BETWEEN ( 50000, 80000 )`

**LIKE:** `<attribute> [NOT] LIKE "expression"`

The `%` (percentage sign) is placeholder for multiple characters,
an `_` (underscore) is placeholder for only one character.

* `name LIKE 'Jo%'` (true for 'Joe', 'Josh', 'Joseph' etc.)
* `name LIKE 'Jo_'` (true for 'Joe'; false for 'Josh')
* `name NOT LIKE 'Jo_'` (true for 'Josh'; false for 'Joe')
* `name LIKE 'J_s%'` (true for 'Josh', 'Joseph'; false 'John', 'Joe')


**ILIKE:** `<attribute> [NOT] ILIKE 'expression'`

Similar to LIKE predicate but in a case-insensitive manner.

* `name ILIKE 'Jo%'` (true for 'Joe', 'joe', 'jOe','Josh','joSH', etc.)
* `name ILIKE 'Jo_'` (true for 'Joe' or 'jOE'; false for 'Josh')

**REGEX**: `<attribute> [NOT] REGEX 'expression'`

* `name REGEX 'abc-.*'` (true for 'abc-123'; false for 'abx-123')

NOTE: You can escape the `%` and `_` placeholder characters in your
SQL queries with predicates using the
backslash (`\`) character. The apostrophe (`'`) can be escaped with another
apostrophe, i.e., `''`. If you use REGEX, you need to escape characters
according to the normal Java escape syntax; see link:https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html[here^]
for the details.

===== Querying Entry Keys with Predicates

You can use `__key` attribute to perform a predicated search for entry
keys. See the following example:

[source,java]
----
IMap<String, Person> personMap = hazelcastInstance.getMap(persons);
personMap.put("Alice", new Person("Alice", 35, Gender.FEMALE));
personMap.put("Andy",  new Person("Andy",  37, Gender.MALE));
personMap.put("Bob",   new Person("Bob",   22, Gender.MALE));
[...]
Predicate predicate = Predicates.sql("__key like A%");
Collection<Person> startingWithA = personMap.values(predicate);
----

In this example, the code creates a collection with the entries whose
keys start with the letter "A”.

==== Querying JSON Strings

You can query JSON strings stored inside your Hazelcast clusters. To
query a JSON string,
you first need to create a `HazelcastJsonValue` from the JSON string.
You can use ``HazelcastJsonValue``s both as keys and values in the
distributed data structures. Then, it is
possible to query these objects using the Hazelcast query methods
explained in this section.

[source,java]
----
String person1 = "{ \"name\": \"John\", \"age\": 35 }";
String person2 = "{ \"name\": \"Jane\", \"age\": 24 }";
String person3 = "{ \"name\": \"Trey\", \"age\": 17 }";

IMap<Integer, HazelcastJsonValue> idPersonMap = instance.getMap("jsonValues");

idPersonMap.put(1, new HazelcastJsonValue(person1));
idPersonMap.put(2, new HazelcastJsonValue(person2));
idPersonMap.put(3, new HazelcastJsonValue(person3));

Collection<HazelcastJsonValue> peopleUnder21 = idPersonMap.values(Predicates.lessThan("age", 21));
----

When running the queries, Hazelcast treats values extracted from
the JSON documents as Java types so they
can be compared with the query attribute. JSON specification
defines five primitive types to be used in the JSON
documents: `number`,`string`, `true`, `false` and `null`. The `string`,
`true/false` and `null` types are treated
as `String`, `boolean` and `null`, respectively. We treat the extracted
`number` values as ``long``s if they
can be represented by a `long`. Otherwise, ``number``s are treated
as ``double``s.

It is possible to query nested attributes and arrays in JSON documents.
The query syntax is the same
as querying other Hazelcast objects as explained in the
<<querying-in-collections-and-arrays, Querying in Collections and Arrays section>>.

[source,java]
----
/**
 * Sample JSON object
 *
 * {
 *     "departmentId": 1,
 *     "room": "alpha",
 *     "people": [
 *         {
 *             "name": "Peter",
 *             "age": 26,
 *             "salary": 50000
 *         },
 *         {
 *             "name": "Jonah",
 *             "age": 50,
 *             "salary": 140000
 *         }
 *     ]
 * }
 *
 *
 * The following query finds all the departments that have a person named "Peter" working in them.
 */
Collection<HazelcastJsonValue> departmentWithPeter = departments.values(Predicates.equal("people[any].name", "Peter"));
----

`HazelcastJsonValue` is a lightweight wrapper around your JSON strings.
It is used merely as a way to indicate
that the contained string should be treated as a valid JSON value.
Hazelcast does not check the validity of JSON
strings put into to maps. Putting an invalid JSON string in a map is
permissible. However, in that case
whether such an entry is going to be returned or not from a query is not defined.

===== Metadata Creation for JSON Querying

Hazelcast stores a metadata object per `HazelcastJsonValue` stored.
This metadata object is created every time
a `HazelcastJsonValue` is put into an IMap. Metadata is later used
to speed up the query operations. Metadata creation
is on by default. Depending on your application's needs, you may want
to turn off the metadata creation
to decrease the put latency and increase the throughput. You can configure
this using <<metadata-policy, Metadata Policy>>.

NOTE: JSON metadata is stored on-heap even when you use the `NATIVE`
in-memory format. If you are storing
``HazelcastJsonValue``s in your `NATIVE` maps, there is a certain
amount of on-heap cost per object. Metadata is not created unless you
put ``HazelcastJsonValue``s in your `NATIVE` maps even when metadata
creation is on.

==== Filtering with Paging Predicates

Hazelcast provides paging for defined predicates. With its `PagingPredicate`
interface, you can
get a collection of keys, values, or entries page by page by filtering
them with predicates and giving the size of the pages. Also, you
can sort the entries by specifying comparators. In this case, the comparator
should be `Serializable` and the serialization factory implementations you use,
e.g., `PortableFactory` and `DataSerializableFactory`, should be registered.
See the <<serialization, Serialization chapter>> on how to register these
factories.

Paging predicates require the objects to be deserialized both on the calling
side (either a member or client) and the member side from which the collection
is retrieved. Therefore, you need to register the serialization factories
you use on all the members and clients on which the paging predicates are used.
See the <<serialization, Serialization chapter>> on how to register these
factories.

In the example code below:

* The `greaterEqual` predicate gets values from the "students" map. This
predicate has a filter
to retrieve the objects with an "age" greater than or equal to 18.
* Then a `PagingPredicate` is constructed in which the page size is 5,
so that there are five objects in each page.
The first time the values are called creates the first page.
* It gets subsequent pages with the `nextPage()`
method of `PagingPredicate` and querying the map again with the
updated `PagingPredicate`.

[source,java]
----
IMap<Integer, Student> map = hazelcastInstance.getMap( "students" );
Predicate greaterEqual = Predicates.greaterEqual( "age", 18 );
PagingPredicate pagingPredicate = Predicates.pagingPredicate( greaterEqual, 5 );
// Retrieve the first page
Collection<Student> values = map.values( pagingPredicate );
...
// Set up next page
pagingPredicate.nextPage();
// Retrieve next page
values = map.values( pagingPredicate );
...
----

If a comparator is not specified for `PagingPredicate`, but you want
to get a collection of keys or values page by page, this collection must
be an instance of `Comparable` (i.e., it must implement `java.lang.Comparable`).
Otherwise, the `java.lang.IllegalArgument` exception is thrown.

You can also access a specific page more
easily with the help of the `setPage()` method. This way, if you make
a query for the hundredth page, for example, it gets all 100 pages at
once instead of reaching the hundredth page one by one using the `nextPage()` method.
Note that this feature tires the memory and see the
link:{docBaseUrl}/javadoc/com/hazelcast/query/PagingPredicate.html[PagingPredicate Javadoc^].

Paging Predicate, also known as Order & Limit, is not supported in
Transactional Context.

==== Filtering with Partition Predicate

You can run queries on a single partition in your cluster using
the partition predicate (`PartitionPredicate`).

The `Predicates.partitionPredicate()` method takes a predicate and partition key
as parameters, gets the partition ID using the key and  runs that predicate only
on the partition where that key belongs.

See the following code snippet:

[source,java]
----
...
Predicate predicate = Predicates.partitionPredicate(partitionKey, Predicates.alwaysTrue());

Collection<Integer> values = map.values(predicate);
Collection<String> keys = map.keySet(predicate);
...
----

By default there are 271 partitions, and using a regular predicate,
each partition needs to be accessed. However, if the
partition predicate only accesses a single partition, this can lead
to a big performance gain.

For the partition predicate to work correctly, you need to know which
partition your data belongs to so that you can send the
request to the correct partition. One of the ways of doing it is to
make use of the `PartitionAware` interface when data is
inserted, thereby controlling the owning partition. See the
<<partitionaware, PartitionAware section>> for more information and examples.

A concrete example may be a web shop that sells phones and accessories.
To find all the accessories of a phone,
a query could be executed that selects all accessories for that phone.
This query is executed on all members in the cluster and
therefore could generate quite a lot of load. However, if we would store
the accessories in the same partition as the phone, the
partition predicate could use the `partitionKey` of the phone to select
the right partition and then it queries for
the accessories for that phone; and this reduces the load on the system
and get faster query results.

==== Indexing Queries

Hazelcast distributed queries run on each member in parallel and return
only the results to the caller.
Then, on the caller side, the results are merged.

When a query runs on a
member, Hazelcast iterates through all the owned entries and finds the
matching ones. This can be made faster by indexing
the most-queried fields, just like you would do for your database.
Indexing adds overhead for each write
operation but reading will be a lot faster. If you query your map a
lot, make sure to add indexes for the most frequently
queried fields. For example, if you do `active AND age < 30` query,
make sure you add an index for the `active` and
`age` fields. The following example code does that by getting the map
from the Hazelcast instance and adding indexes to the map with the
IMap `addIndex` method.

[source,java]
----
IMap map = hazelcastInstance.getMap( "employees" );
// ordered, since we have ranged queries for this field
map.addIndex(new IndexConfig(IndexType.SORTED, "age"));
// not ordered, because boolean field cannot have range
map.addIndex(new IndexConfig(IndexType.HASH, "active"));
----

Note that creating indexes once is sufficient. Subsequent write
operations on the map are reflected in the index automatically. So,
although it is safe to call the `addIndex()` method repeatedly, there
will be a performance penalty due to the redundant index creation.

When you call, for example, `map.addIndex("fieldName", true)`, each
partition iterates over its records and adds each entry to the index.
The previously created index entry will be recreated and replaced with the new entry.
The performance penalty will be proportional to the number of entries. If you
have maps with a large number of entries, then synchronizing index addition process
is recommended.

Other than using the `addIndex()` method, you can define your index
declaratively or programmatically as described in the <<configuring-imap-indexes, Configuring IMap Indexes section>>.

===== Indexing Ranged Queries

`IMap.addIndex(IndexConfig)` is used for adding index. For
each indexed field, if you have ranged queries such as `age>30`,
`age BETWEEN 40 AND 60`, then use `IndexType.SORTED` index
Otherwise, use `IndexType.HASH`.

===== Configuring IMap Indexes

Also, you can define `IMap` indexes in configuration. An example is
shown below.

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <map name="default">
        <indexes>
            <index type="HASH">
                <attributes>
                    <attribute>name</attribute>
                </attributes>
            </index>
            <index>
                <attributes>
                    <attribute>age</attribute>
                </attributes>
            </index>
        </indexes>
    </map>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  map:
    default:
      indexes:
        - type: HASH
            attributes:
              - "name"
        - attributes:
            - "age"
----

[source,xml,indent=0,subs="verbatim,attributes",role="secondary"]
.Spring
----
<hz:map name="default">
    <hz:indexes>
        <hz:index type="HASH">
            <hz:attributes>
                <hz:attribute>name</hz:attribute>
            </hz:attributes>
        </hz:index>
        <hz:index>
            <hz:attributes>
                <hz:attribute>age</hz:attribute>
            </hz:attributes>
        </hz:index>
    </hz:indexes>
</hz:map>
----


You can also define `IMap` indexes using programmatic configuration,
as in the example below.

[source,java]
----
mapConfig.addIndexConfig(new IndexConfig(IndexType.HASH, "name"));
mapConfig.addIndexConfig(new IndexConfig(IndexType.SORTED, "age"));
----

The following is the Spring declarative configuration for the same
example.

NOTE: Non-primitive types to be indexed should implement *`Comparable`*.

NOTE: If you configure the data structure to use
<<configuring-high-density-memory-store, High-Density Memory Store>> **and**
indexes, the indexes are automatically stored in the High-Density Memory Store
as well. This prevents from running into full garbage collections when doing
a lot of updates to index.

===== Composite Indexes

Composite indexes, also known as compound indexes, are special kind of
indexes that are built on top of the multiple map entry attributes and
therefore may be used to significantly speed up the queries involving
those attributes simultaneously.

There are two distinct composite index types used for two different
purposes: unordered composite indexes and ordered ones.

====== Unordered Composite Indexes

The unordered indexes are used to perform equality queries, also known
as the point queries, e.g., `name = 'Alice'`. These are specifically
optimized for equality queries and don't support other comparison operators
like `>` or `+<=+`.

Additionally, the _composite_ unordered indexes allow speeding up the equality
queries involving multiple attributes simultaneously, e.g., `name = 'Alice'
and age = 33`. This example query results in a single composite index lookup
operation which can be performed very efficiently.

The unordered composite index on the `name` and `age` attributes may be
configured for a map as follows:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <map name="persons">
        <indexes>
            <index type="HASH">
                <attributes>
                    <attribute>name</attribute>
                    <attribute>age</attribute>
                </attributes>
            </index>
        </indexes>
    </map>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  map:
    default:
      - type: HASH
          attributes:
            - "name"
            - "age"
----

The attributes indexed by the _unordered_ composite indexes can't be
matched partially: the `name = 'Alice'` query can't utilize the composite
index configured above.

====== Ordered Composite Indexes

The ordered indexes are specifically designed to perform efficient order
comparison queries, also known as the range queries, e.g., `age > 33`. The
equality queries, like `age = 33`, are still supported by the ordered indexes,
but they are handled in a slightly less efficient manner comparing to the
unordered indexes.

The _composite_ ordered indexes extend the concept by allowing multiple
equality predicates and a single order comparison predicate to be combined
into a single index query operation. For instance, the `name = 'Alice' and
age > 33` and `name = 'Bob' and age = 33 and balance > 0.0` queries are good
candidates to be covered by an ordered composite index configured as follows:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <map name="persons">
        <indexes>
            <index>
                <attributes>
                    <attribute>name</attribute>
                    <attribute>age</attribute>
                    <attribute>balance</attribute>
                </attributes>
            </index>
        </indexes>
    </map>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  map:
    persons:
      indexes:
        - attributes:
          - "name"
          - "age"
          - "balance"
----

Unlike the _unordered_ composite indexes, partial attribute prefixes may be
matched for the _ordered_ composite indexes. In general, a valid non-empty
attribute prefix is formed as a sequence of zero or more equality predicates
followed by a zero or exactly one order comparison predicate. Given the index
definition above, the following queries may be served by the index: `name = 'Alice'`,
`name > 'Alice'`, `name = 'Alice' and age > 33`, `name = 'Alice' and age = 33 and
balance = 5.0`. The following queries can't be served the index: `age = 33`,
`age > 33 and balance = 0.0`, `balance > 0.0`.

While matching the ordered composite indexes, multiple order comparison
predicates acting on the same attribute are treated as a single range
predicate acting on that attribute. Given the index definition above, the
following queries may be served by the index: `name > 'Alice' and name < 'Bob'`,
`name = 'Alice' and age > 33 and age < 55`, `name = 'Alice' and age = 33 and
balance > 0.0 and balance < 100.0`.

====== Composite Index Matching and Selection

The order of attributes involved in a query plays no role in the selection
of the matching composite index: `name = 'Alice' and age = 33` and
`age = 33 and name = 'Alice'` queries are equivalent from the point of
view of the index matching procedure.

The attributes involved in a query can be matched partially by the composite
index matcher: `name = 'Alice' and age = 33 and balance > 0.0` can be
partially matched by the `name, age` composite index, the `name = 'Alice'
and age = 33` predicates are served by the matched index, while the
`balance > 0.0` predicate is processed by other means.

===== Bitmap Indexes

Bitmap indexes provide capabilities similar to unordered/hash indexes.
The same set of predicates is supported:

* `equal`
* `notEqual`
* `in`,
* `and`
* `or`
* `not`

But, unlike hash indexes, bitmap indexes are able
to achieve a much higher memory efficiency for low cardinality attributes
at the cost of reduced query performance. In practice, the query
performance is comparable to the performance of hash indexes, while
memory footprint reduction is high, usually around an order of magnitude.

Bitmap indexes are specifically designed for indexing of collection and
array attributes since a single `IMap` entry produces many index entries
in that case. A single hash index entry costs a few tens of bytes, while
a single bitmap index entry usually costs just a few bytes.

It's also possible to improve the memory footprint while indexing regular
single-value attributes, but the improvement is usually minor, depending
on the data layout and total number of indexes.

NOTE: Currently, bitmap indexes are not supported by off-heap High-Density
Memory Stores (HD).

====== Configuring Bitmap Indexes

In the simplest form, bitmap index for an `IMap` entry attribute can be
declaratively configured as follows:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <map name="persons">
        <indexes>
            <index type="BITMAP">
                <attributes>
                    <attribute>age</attribute>
                </attributes>
            </index>
        </indexes>
    </map>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  map:
    persons:
      indexes:
        - type: BITMAP
          attributes:
            - "age"
----

Internally, a unique non-negative `long` ID is assigned to every
indexed `IMap` entry based on the entry key. That unique ID is
required for bitmap indexes to distinguish one indexed `IMap` entry from
another.

The mapping between `IMap` entries and `long` IDs is not free and its
performance and memory footprint can be improved in certain cases. For
instance, if `IMap` entries already have a unique integer-valued
attribute, the attribute values can be used as unique `long` IDs
directly without any additional transformations. That can be configured
as follows:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<index type="BITMAP">
    <attributes>
        <attribute>age</attribute>
    </attributes>
    <bitmap-index-options>
        <unique-key>uniqueId</unique-key>
        <unique-key-transformation>RAW</unique-key-transformation>
    </bitmap-index-options>
</index>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
      indexes:
        - type: BITMAP
          attributes:
            - "age"
          bitmap-index-options:
            unique-key: uniqueId
            unique-key-transformation: RAW
----

The index definition above instructs Hazelcast to create a bitmap index
on the `age` attribute, extract the unique key values from `uniqueId` attribute
and use the raw (`RAW`) extracted values directly as `long` IDs. If the
extracted unique key value is not of `long` type, the widening
conversion is performed for the following types: `byte`, `short` and
`int`; boxed variants are also supported.

In certain cases, the extracted raw IDs might be randomly distributed.
This causes increased memory usage in bitmap indexes since the best case
scenario for them is sequential contiguous IDs. That can be countered by
applying the renumbering technique:


[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<index type="BITMAP">
    <attributes>
        <attribute>age</attribute>
    </attributes>
    <bitmap-index-options>
        <unique-key>uniqueId</unique-key>
        <unique-key-transformation>LONG</unique-key-transformation>
    </bitmap-index-options>
</index>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
      indexes:
        - type: BITMAP
          attributes:
            - "age"
          bitmap-index-options:
            unique-key: uniqueId
            unique-key-transformation: LONG
----

The index definition above instructs the bitmap index to extract the unique
keys from `uniqueId` attribute, convert every extracted non-negative
value to `long` (`LONG`) and assign an internal sequential unique `long`
ID based on that extracted and then converted unique value. The widening
conversion is applied to the extracted values, if necessary.

This long-to-long mapping is performed more efficiently than the general
object-to-long mapping done for the simple index definitions. Basically,
the following simple bitmap index definition:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<index type="BITMAP">
    <attributes>
        <attribute>age</attribute>
    </attributes>
</index>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
      indexes:
        - type: BITMAP
          attributes:
            - "age"
----


is equivalent to the following full-form definition:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<index type="BITMAP">
    <attributes>
        <attribute>age</attribute>
    </attributes>
    <bitmap-index-options>
        <unique-key>__key</unique-key>
        <unique-key-transformation>OBJECT</unique-key-transformation>
    </bitmap-index-options>
</index>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
      indexes:
        - type: BITMAP
          attributes:
            - "age"
          bitmap-index-options:
            unique-key: __key
            unique-key-transformation: OBJECT
----

Which indexes `age` attribute, uses `IMap` entry keys (`__key`) interpreted
as Java objects (`OBJECT`) to assign internal unique `long` IDs.

The full-form definition syntax is defined as follows:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<index type="BITMAP">
    <attributes>
        <attribute><attr></attribute>
    </attributes>
    <bitmap-index-options>
        <unique-key><key></unique-key>
        <unique-key-transformation><transformation></unique-key-transformation>
    </bitmap-index-options>
</index>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
      indexes:
        - type: BITMAP
          attributes:
            - <attribute>
          bitmap-index-options:
            unique-key: <key>
            unique-key-transformation: <transformation>
----


The following are the parameter descriptions:

* `<attr>`: Specifies the attribute index.
* `<key>`: Specifies the attribute to use as a unique key source
for internal unique `long` ID assignment.
* `<transformation>`: Specifies the transformation to be applied
to unique keys to generate unique `long` IDs from them. The following
transformations are supported:
** `OBJECT`: Object-to-long transformation. Each extracted unique
key value is interpreted as a Java object instance. Internally, an
object-to-long hash table is used to establish the mapping from unique
keys to unique IDs. Good as a general-purpose transformation.
** `LONG`: Long-to-long transformation. Each extracted unique key value
is interpreted as a non-negative `long` value, the widening conversion
from `byte`, `short` and `int` is performed, if necessary.
Internally, a long-to-long hash table is used to establish the mapping
from unique keys to unique IDs, which is more efficient than the
object-to-long hash table. It is good for sparse/random unique integer-valued keys
renumbering to raise the IDs density and to make the bitmap index more
memory-efficient as a result.
** `RAW`: Raw transformation. Each extracted unique key value is interpreted as
a non-negative `long` value, the widening conversion from `byte`, `short` and
`int` is performed, if necessary. Internally, no hash table of any kind is
used to establish the mapping from unique keys to unique IDs, the raw extracted
keys are used directly as IDs. It is good for dense unique integer-valued keys,
and it has the best performance in terms of time and memory.

The regular dotted attribute path syntax is supported for `<attr>` and
`<key>`:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<index type="BITMAP">
    <attributes>
        <attribute>name.first</attribute>
    </attributes>
</index>
<index type="BITMAP">
    <attributes>
        <attribute>name.first</attribute>
    </attributes>
    <bitmap-index-options>
        <unique-key>__key.id</unique-key>
    </bitmap-index-options>
</index>
<index type="BITMAP">
    <attributes>
        <attribute>name.first</attribute>
    </attributes>
    <bitmap-index-options>
        <unique-key>id.external</unique-key>
    </bitmap-index-options>
</index>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
      indexes:
        - type: BITMAP
          attributes:
            - name.first
        - type: BITMAP
          attributes:
            - name.first
          bitmap-index-options:
            unique-key: __key.id
        - type: BITMAP
          attributes:
            - name.first
          bitmap-index-options:
            unique-key: id.external
----


Collection and array indexing is also possible using the regular syntax:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<index type="BITMAP">
    <attributes>
        <attribute>habits[any]</attribute>
    </attributes>
</index>
<index type="BITMAP">
    <attributes>
        <attribute>habits[0]</attribute>
    </attributes>
</index>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
      indexes:
        - type: BITMAP
          attributes:
            - habits[any]
        - type: BITMAP
          attributes:
            - habits[0]
----


See <<indexing-in-collections-and-arrays, Indexing in Collections and Arrays section>>
for more details.

====== Bitmap Index Querying

Bitmap index matching and selection for queries are performed
automatically. No special treatment is required. The querying can be
performed using the regular `IMap` querying methods:
`IMap.values(Predicate)`, `IMap.entrySet(Predicate)`, etc.

===== Copying Indexes

The underlying data structures used by the indexes need to copy the
query results to make sure that the results are correct. This copying
process is performed either when reading the index from the data
structure (on-read) or writing to it (on-write).

On-read copying means that, for each index-read operation, the result
of the query is copied before it is sent to the caller. Depending on
the query result's size, this type of index copying may be slower since
the result is stored in a map, i.e., all entries need to have the hash
calculated before being stored. Unlike the index-read operations, each
index-write operation is fast, since there is no copying. So, this
option can be preferred in index-write intensive cases.

On-write copying means that each index-write operation completely copies
the underlying map to provide the copy-on-write semantics and this may
be a slow operation depending on the index size. Unlike index-write operations,
each index-read operation is fast since the operation only includes accessing
the map that stores the results and returning them to the caller.

Another option is never copying the results of a query to a separate map.
This means the results backed by the underlying index-map can change after
the query has been executed (such as an entry might have been added or removed
from an index, or it might have been remapped). This option can be preferred
if you expect "mostly correct" results, i.e., if it is not a problem when
some entries returned in the query result set do not match the initial query
criteria. This is the fastest option since there is no copying.

You can set one these options using the system property
`hazelcast.index.copy.behavior`. The following values, which are explained
in the above paragraphs, can be set:

* `COPY_ON_READ` (the default value)
* `COPY_ON_WRITE`
* `NEVER`

NOTE: Usage of this system property is supported for BINARY and OBJECT
in-memory formats. Only in Hazelcast 3.8.7, it is also supported for
NATIVE in-memory format.

===== Indexing Attributes with ValueExtractor

You can also define custom attributes that may be referenced in predicates,
queries and indexes. Custom attributes can be defined by implementing a
`ValueExtractor`. See the <<custom-attributes, Custom Attributes section>>
for details.

===== Using "this" as an Attribute

You can use the keyword `this` as an attribute name while adding an
index or creating a predicate. A basic usage is shown below.

[source,java]
----
map.addIndex(new IndexConfig(IndexType.SORTED, "this"));
Predicate<Integer, Integer> lessEqual = Predicates.between("this", 12, 20);
----

Another basic example using `SQL` predicate is shown below.

[source,java]
----
Predicates.sql("this = 'jones'")
Predicates.sql("this.age > 33")
----

The special attribute `this` acts on the value of a map entry. Typically,
you do not need to specify it while accessing a property of an entry's
value, since its presence is implicitly assumed if the special attribute
<<querying-entry-keys-with-predicates, __key>> is not specified.

==== Configuring Query Thread Pool

You can change the size of thread pool dedicated to query operations
using the `pool-size` property. Each query consumes a single thread
from a Generic Operations ThreadPool on each Hazelcast member - let's
call it the query-orchestrating thread.  That thread is blocked throughout
the whole execution-span of a query on the member.

The query-orchestrating thread uses the threads from the query-thread
pool in the following cases:

* if you run a `PagingPredicate` (since each page runs as a separate task)
* if you set the system property `hazelcast.query.predicate.parallel.evaluation`
to true (since the predicates are evaluated in parallel)

See the <<filtering-with-paging-predicates, Filtering with Paging Predicates section>>
and <<parallel-predicates, System Properties appendix>> for information on paging
predicates and for description of the above system property.

Below is an example of that declarative configuration.

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <executor-service name="hz:query">
        <pool-size>100</pool-size>
    </executor-service>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  ...
  executor-service:
    "hz:query":
      pool-size: 100
----

Below is the equivalent programmatic configuration.

[source,java]
----
Config cfg = new Config();
cfg.getExecutorConfig("hz:query").setPoolSize(100);
----

===== Query Requests from Clients

When dealing with the query requests coming from the clients to your
members, Hazelcast offers the following system properties to tune your
thread pools:

* `hazelcast.clientengine.thread.count` which is the number of threads
to process non-partition-aware client requests, like `map.size()` and
executor tasks. Its default value is the number of cores multiplied by 20.
* `hazelcast.clientengine.query.thread.count` which is the number of
threads to process query requests coming from the clients. Its default
value is the number of cores.

If there are a lot of query request from the clients, you may want to
increase the value of `hazelcast.clientengine.query.thread.count`. In
addition to this tuning, you may also consider increasing the value of
`hazelcast.clientengine.thread.count` if the CPU load in your system is
not high and there is plenty of free memory.

=== Querying in Collections and Arrays

Hazelcast allows querying in collections and arrays.
Querying in collections and arrays is compatible with all Hazelcast
serialization methods, including the Portable serialization.


Let's have a look at the following data structure expressed in pseudo-code:

[source,java]
----
class Motorbike {
    Wheel[] wheels;
}

class Wheel {
   String name;

}
----

In order to query a single element of a collection/array, you can execute the following query:

[source,java]
----
// it matches all motorbikes where the zero wheel's name is 'front-wheel'
Predicate p = Predicates.equal("wheels[0].name", "front-wheel");
Collection<Motorbike> result = map.values(p);
----

It is also possible to query a collection/array using the `any` semantic as shown below:

[source,java]
----
// it matches all motorbikes where any wheel's name is 'front-wheel'
Predicate p = Predicates.equal("wheels[any].name", "front-wheel");
Collection<Motorbike> result = map.values(p);
----

The exact same query may be executed using the `SQL` predicate as shown below:

[source,java]
----
Predicate p = Predicates.sql("wheels[any].name = 'front-wheel'");
Collection<Motorbike> result = map.values(p);
----

`[]` notation applies to both collections and arrays.

[NOTE]
====
Hazelcast requires all elements of a collection to have the same type. Considering
and expanding the above example:

* If you have a `wheels` collection attribute, all of its elements must be of
the `Wheel` type, subclasses of `Wheel` are not allowed.
* Let’s say you have added a `seats` collection attribute, which is a `Seat`
object.  Then all of its elements must of this concrete `Seat` type.

So, you may have collections of different types in your map. However, each
collection’s elements must be of the same concrete type within that collection
attribute.

Consider custom attribute extractors if it is impossible or undesirable to reduce
the variety of types to a single type. See the <<custom-attributes, Custom Attributes section>>
for information on them.
====

==== Indexing in Collections and Arrays

You can also create an index using a query in collections and arrays.

Please note that in order to leverage the index, the attribute name used
in the query has to be the same as the one used
in the index definition.

Let's assume you have the following index definition:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <indexes>
        <index type="HASH">
            <attributes>
                <attribute>wheels[any].name</attribute>
            </attributes>
        </index>
    </indexes>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  ...
  indexes:
    - type: HASH
      attributes:
        - wheels.[any].name
----

The following query uses the index:

[source,java]
----
Predicate p = Predicates.equal("wheels[any].name", "front-wheel");
----

The following query, however, does NOT leverage the index, since it does
not use exactly the same attribute name that
was used in the index:

[source,java]
----
Predicates.equal("wheels[0].name", "front-wheel")
----

In order to use the index in the case mentioned above, you have to create
another index, as shown below:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <indexes>
        <index type="HASH">
            <attributes>
                <attribute>wheels[0].name</attribute>
            </attributes>
        </index>
    </indexes>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  ...
  indexes:
    - type: HASH
      attributes:
        - wheels.[0].name
----

==== Corner cases

Handling of corner cases may be a bit different than in a programming
language like `Java`.

Let's have a look at the following examples in order to understand the differences.
To make the analysis simpler, let's assume that there is only one `Motorbike`
object stored in a Hazelcast Map.

[cols="1,5,3,1,1"]
|===
|Id|Query|Data State|Extraction Result|Match

| 1
| `Predicates.equal("wheels[7].name", "front-wheel")`
| `wheels.size() == 1`
| `null`
| No

| 2
| `Predicates.equal("wheels[7].name", null)`
| `wheels.size() == 1`
| `null`
| Yes

| 3
| `Predicates.equal("wheels[0].name", "front-wheel")`
| `wheels[0].name == null`
| `null`
| No

| 4
| `Predicates.equal("wheels[0].name", null)`
| `wheels[0].name == null`
| `null`
| Yes

| 5
| `Predicates.equal("wheels[0].name", "front-wheel")`
| `wheels[0] == null`
| `null`
| No
| 6
| `Predicates.equal("wheels[0].name", null)`
| `wheels[0] == null`
| `null`
| Yes

| 7
| `Predicates.equal("wheels[0].name", "front-wheel")`
| `wheels == null`
| `null`
| No

| 8
| `Predicates.equal("wheels[0].name", null)`
| `wheels == null`
| `null`
| Yes
|===

As you can see, **no** ``NullPointerException``s or ``IndexOutOfBoundException``s
are thrown in the extraction process, even
though parts of the expression are `null`.

Looking at examples 4, 6 and 8, we can also easily notice that it is impossible to
distinguish which part of the
expression was null.
If we execute the following query `wheels[1].name = null`, it may be evaluated to
true because:

* `wheels` collection/array is null
* `index == 1` is out of bound
* `name` attribute of the wheels[1] object is `null`.

In order to make the query unambiguous, extra conditions would have to be added, e.g.,
`wheels != null AND wheels[1].name = null`.

=== Custom Attributes

It is possible to define a custom attribute that may be referenced in predicates,
queries and indexes.

A custom attribute is a "synthetic" attribute that does not exist as a `field` or
a `getter` in the object that it is extracted from.
Thus, it is necessary to define the policy on how the attribute is supposed to be
extracted.
Currently the only way to extract a custom attribute is to implement a
`com.hazelcast.query.extractor.ValueExtractor`
that encompasses the extraction logic.

Custom Attributes are compatible with all Hazelcast serialization methods,
including the Portable serialization.

==== Implementing a ValueExtractor

In order to implement a `ValueExtractor`, implement the
`com.hazelcast.query.extractor.ValueExtractor` interface
and the `extract()` method. This method does not return any values
since the extracted value is collected by the `ValueCollector`.
In order to return multiple results from a single extraction, invoke the
`ValueCollector.collect()` method
multiple times, so that the collector collects all results.

See the link:{docBaseUrl}/javadoc/com/hazelcast/query/extractor/ValueExtractor.html[ValueExtractor^] and
link:{docBaseUrl}/javadoc/com/hazelcast/query/extractor/ValueCollector.html[ValueCollector^] Javadocs.

===== ValueExtractor with Portable Serialization

Portable serialization is a special kind of serialization where there
is no need to have the class of the serialized object on the
classpath in order to read its attributes. That is the reason why the
target object passed to the `ValueExtractor.extract()`
method is not of the exact type that has been stored. Instead, an instance
of a `com.hazelcast.query.extractor.ValueReader` is passed.
`ValueReader` enables reading the attributes of a Portable object in a
generic and type-agnostic way.
It contains two methods:

* `read(String path, ValueCollector<T> collector)` - enables passing all
results directly to the `ValueCollector`.
* `read(String path, ValueCallback<T> callback)` - enables filtering, transforming
and grouping the result of the read operation and manually passing it to the
`ValueCollector`.

See the link:{docBaseUrl}/javadoc/com/hazelcast/query/extractor/ValueReader.html[ValueReader^] Javadoc.

===== Returning Multiple Values from a Single Extraction

It sounds counter-intuitive, but a single extraction may return multiple
values when arrays or collections are
involved.
Let's have a look at the following data structure in pseudo-code:

[source,java]
----
class Motorbike {
    Wheel[] wheel;
}

class Wheel {
    String name;
}
----

Let's assume that we want to extract the names of all wheels from a
single motorbike object. Each motorbike has two
wheels so there are two names for each bike. In order to return both
values from the extraction operation, collect them
separately using the `ValueCollector`. Collecting multiple values in
this way allows you to operate on these multiple
values as if they were single values during the evaluation of the predicates.

Let's assume that we registered a custom extractor with the name `wheelName`
and executed the following query:
`wheelName = front-wheel`.

The extraction may return up to two wheel names for each `Motorbike` since
each `Motorbike` has up to two wheels.
In such a case, it is enough if a single value evaluates the predicate's
condition to true to return a match, so
it returns a `Motorbike` if "any" of the wheels matches the expression.

==== Extraction Arguments

A `ValueExtractor` may use a custom argument if it is specified in the query.
The custom argument may be passed within the square brackets located after the
name of the custom attribute,
e.g., `customAttribute[argument]`.

Let's have a look at the following query: `currency[incoming] == EUR`
The `currency` is a custom attribute that uses a `com.test.CurrencyExtractor`
for extraction.

The string `incoming` is an argument that is passed to the `ArgumentParser`
during the extraction.
The parser parses the string according to its custom logic and it returns a
parsed object.
The parsed object may be a single object, array, collection, or any arbitrary
object.
It is up to the `ValueExtractor` implementation to understand the semantics of
the parsed argument object.

For now it is **not** possible to register a custom `ArgumentParser`, thus a
default parser is used.
It follows a `pass-through` semantic, which means that the string located in
the square brackets is passed "as is" to
the `ValueExtractor.extract()` method.

Please note that using square brackets within the argument string are not allowed.

==== Configuring a Custom Attribute Programmatically

The following snippet demonstrates how to define a custom attribute using a `ValueExtractor`.

[source,java]
----
AttributeConfig attributeConfig = new AttributeConfig();
attributeConfig.setName("currency");
attributeConfig.setExtractorClassName("com.bank.CurrencyExtractor");

MapConfig mapConfig = new MapConfig();
mapConfig.addAttributeConfig(attributeConfig);
----

`currency` is the name of the custom attribute that will be extracted using
the `CurrencyExtractor` class.

Keep in mind that an extractor may not be added after the map has been instantiated.
All extractors have to be defined upfront in the map's initial configuration.

==== Configuring a Custom Attribute Declaratively

The following snippet demonstrates how to define a custom attribute in the
Hazelcast XML Configuration.

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <map name="trades">
        <attributes>
            <attribute extractor-class-name="com.bank.CurrencyExtractor">currency</attribute>
        </attributes>
    </map>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  map:
    trades:
      attributes:
        currency:
          extractor-class-name: com.bank.CurrencyExtractor
----

Analogous to the example above, `currency` is the name of the custom attribute
that will be extracted using the
`CurrencyExtractor` class.

Please note that an attribute name may begin with an ASCII letter [A-Za-z] or
digit [0-9] and may contain
ASCII letters [A-Za-z], digits [0-9] or underscores later on.

==== Indexing Custom Attributes

You can create an index using a custom attribute.

The name of the attribute used in the index definition has to match the one
used in the attributes configuration.

Defining indexes with extraction arguments is allowed, as shown in the example
below:

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <indexes>
        <!-- custom attribute without an extraction argument -->
        <index>
            <attributes>
                <attribute>currency</attribute>
            </attributes>
        </index>
        <!-- custom attribute using an extraction argument -->
        <index>
            <attributes>
                <attribute>currency[incoming]</attribute>
            </attributes>
        </index>
    </indexes>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  ...
  indexes:
    attributes:
      - "currency"
      - "currency[incoming]"
----


[[aggregations]]
=== Aggregations

Aggregations allow to compute a value of some function (e.g sum or max) over the
stored map entries. The computation is performed in a fully distributed manner,
so no data other than the computed function value is transferred to a caller,
making the computation fast.

NOTE: If the <<setting-in-memory-format, in-memory format>> of your data is `NATIVE`,
aggregations always run on the partition threads. If the data is of type `BINARY`
or `OBJECT`, they also mostly run on the partition threads, however, they may run on
the separate query threads to avoid blocking partition threads (if there are no ongoing migrations).

==== Aggregator API

The aggregation is split into three phases represented by three methods:

. `accumulate()`
. `combine()`
. `aggregate()`

There are also the following callbacks:

* `onAccumulationFinished()` called when the accumulation phase finishes
* `onCombinationFinished()` called when the combination phase finishes

These callbacks enable releasing the state that might have been initialized and
stored in the Aggregator - to reduce the network traffic.

Each phase is described below. See also the
link:{docBaseUrl}/javadoc/com/hazelcast/aggregation/Aggregator.html[Aggregator Javadoc^]
for the API's details.

**Accumulation:**

During the accumulation phase each Aggregator accumulates all entries passed
to it by the query engine.
It accumulates only those pieces of information that are required to calculate
the aggregation result in the last phase - that's implementation specific.

In case of the `DoubleAverage` aggregation the Aggregator would accumulate:

* the sum of the elements it accumulated
* the count of the elements it accumulated

**Combination:**

Since aggregation is executed in parallel on each partition of the cluster,
the results need to be combined after the accumulation phase in order to be able
to calculate the final result.

In case of the `DoubleAverage` aggregation, the aggregator would sum up all
the sums of the elements and all the counts.


**Aggregation:**

Aggregation is the last phase that calculates the final result from the
results accumulated and combined in the preceding phases.

In case of the `DoubleAverage` aggregation, the Aggregator would just
divide the sum of the elements by their count (if non-zero).

==== Aggregations and Map Interfaces

Aggregations are available on `com.hazelcast.map.IMap` only.
IMap offers the method `aggregate` to apply the aggregation logic on
the map entries. This method can be called with or without a predicate. You can refer
to its link:{docBaseUrl}/javadoc/com/hazelcast/map/IMap.html#aggregate-com.hazelcast.aggregation.Aggregator-[Javadoc^]
to see the method details.


==== Example Implementation

Here's an example implementation of the Aggregator:

[source,java]
----
include::{javasource}/distributedquery/SimpleFastAggregationsDemo.java[tag=fademo]
----

As you can see:

* the `accumulate()` method calculates the sum and count of the elements
* the `combine()` method combines the results from all the accumulations
* the `aggregate()` method calculates the final result.

==== Built-In Aggregations

The `com.hazelcast.aggregation.Aggregators` class provides a wide variety of built-in Aggregators.
The full list is presented below:

* `count`
* `distinct`
* `bigDecimal` `sum`/`avg`/`min`/`max`
* `bigInteger` `sum`/`avg`/`min`/`max`
* `double` `sum`/`avg`/`min`/`max`
* `integer` `sum`/`avg`/`min`/`max`
* `long` `sum`/`avg`/`min`/`max`
* `number` `avg`
* `comparable` `min`/`max`
* `fixedPointSum`, `floatingPointSum`

To use the any of these Aggregators, instantiate them using the `Aggregators`
factory class.

Each built-in Aggregator can also navigate to an attribute of the object passed
to the `accumulate()` method (via reflection). For example, `Aggregators.distinct("address.city")`
extracts the `address.city` attribute from the object passed to the Aggregator and
accumulate the extracted value.

==== Configuration Options

On each partition, after the entries have been passed to the aggregator, the
accumulation runs in parallel.
It means that each aggregator is cloned and receives a sub-set of the entries
received from a partition.
Then, it runs the accumulation phase in all of the cloned aggregators - at the
end, the result is combined into a single accumulation result.
It speeds up the processing by at least the factor of two - even in case of simple
aggregations. If the accumulation logic is more "heavy", the speed-up may be more significant.

In order to switch the accumulation into a sequential mode just set the
`hazelcast.aggregation.accumulation.parallel.evaluation` property to `false`
(it's set to `true` by default).

=== Projections

There are cases where instead of sending all the data returned by a query from
a member, you want to transform (strip down) each result object in order to avoid
redundant network traffic.

For example, you select all employees based on some criteria, but you just want to
return their name instead of the whole Employee object. It is easily doable with the
Projection API.

==== Projection API

The Projection API provides the method `transform()` which is called on each result
object. Its result is then gathered as the final query result entity. You can refer
to the link:{docBaseUrl}/javadoc/com/hazelcast/projection/Projection.html[Projection Javadoc^]
for the API's details.

===== Projections and Map Interfaces

Projections are available on `com.hazelcast.map.IMap` only. IMap offers the method
`project` to apply the projection logic on the map entries. This method can be called
with or without a predicate. See its
link:{docBaseUrl}/javadoc/com/hazelcast/map/IMap.html#project-com.hazelcast.projection.Projection-[Javadoc^]
to see the method details.


==== Example implementation

Let's consider the following domain object stored in an IMap:

[source,java]
----
public class Employee implements Serializable {

    private String name;

    public Employee() {
    }

    public String getName() {
        return name;
    }

    public void setName(String firstName) {
        this.name = name;
    }
}
----

To return just the names of the Employees, you can run the query in the following way:

[source,java]
----
Collection<String> names = employees.project(new Projection<Map.Entry<String, Employee>, String>() {

    @Override
    public String transform(Map.Entry<String, Employee> entry) {
        return entry.getValue().getName();
    }
}, somePredicate);
----


==== Built-In Projections

The `com.hazelcast.projection.Projections` class provides two built-in
Projections:

* `singleAttribute`
* `multiAttribute`

The `singleAttribute` Projection enables extracting a single attribute
from an object (via reflection). For example, `Projection.singleAttribute("address.city")`
extracts the `address.city` attribute from the object passed to the Projection.

The `multiAttribute` Projection enables extracting multiples attributes from an
object (via reflection). For example, `Projection.multiAttribute("address.city", "postalAddress.city")`
extracts both attributes from the object passed to the Projection and return them in an `Object[]` array.

=== Continuous Query Cache

A continuous query cache is used to cache the result of a continuous query. After
the construction of a continuous query cache, all changes on IMap are asynchronously
reflected to this cache via events. This makes this cache as an asynchronously updated
view of IMap. You can create a continuous query cache either on the client or member.

==== Keeping Query Results Local and Ready

A continuous query cache is beneficial when you need to query the distributed
`IMap` data in a very frequent and fast way. By using a continuous query cache,
the result of the query will always be ready and local to the application.

==== Accessing Continuous Query Cache from Member

The following code snippet shows how you can access a continuous query cache
from a member.

[source,java]
----
include::{javasource}/distributedquery/CQC.java[tag=cqc]
----

==== Accessing Continuous Query Cache from Client Side

The following code snippet shows how you can access a continuous query cache
from the client side.
The difference in this code from the member side code above is that you configure
and instantiate
a client instance instead of a member instance.

[source,java]
----
include::{javasource}/distributedquery/CQCClient.java[tag=cqcclient]
----

==== Features of Continuous Query Cache

The following features of continuous query cache are valid for both
the member and client:

* The initial query that is run on the existing `IMap` data during the
continuous query cache construction can be enabled/disabled according to
the supplied predicate via `QueryCacheConfig.setPopulate()`.
* Continuous query cache allows you to run queries with indexes and perform
event batching and coalescing.
* A continuous query cache is evictable. Note that a continuous query cache
has a default maximum capacity of 10000. If you need a non-evictable cache, you
should configure the eviction via `QueryCacheConfig.setEvictionConfig()`.
* A listener can be added to a continuous query cache using `QueryCache.addEntryListener()`.
* `IMap` events are reflected in continuous query cache in the same order as
they were generated on map entries. Since events are created on entries stored
in partitions, ordering of events is maintained based on the ordering within
the partition. You can add listeners to capture lost events using `EventLostListener`
and you can recover lost events with the method `QueryCache.tryRecover()`.
Recovery of lost events largely depends on the size of the buffer on Hazelcast members.
Default buffer size is 16 per partition, i.e., 16 events per partition can be maintained
in the buffer. If the event generation is high, setting the buffer size to a higher
number provides better chances of recovering lost events. You can set buffer size
with `QueryCacheConfig.setBufferSize()`. You can use the following example code for a
recovery case.
+
[source,java]
----
QueryCache queryCache = map.getQueryCache("cache-name", Predicates.sql("this > 20"), true);
queryCache.addEntryListener(new EventLostListener() {
@Override
public void eventLost(EventLostEvent event) {
       queryCache.tryRecover();
      }
}, false);
----
+
* You can populate a continuous query cache with only the keys of its entries and
retrieve the subsequent values directly via `QueryCache.get()` from the underlying
`IMap`. This helps to decrease the initial population time when the values are very
large.

==== Configuring Continuous Query Cache

You can configure continuous query cache declaratively or programmatically; the
latter is mostly explained in the previous section. The parent configuration element
is `<query-caches>` which should be placed within your `<map>` configuration. You can
create your query caches using the  `<query-cache>` sub-element under `<query-caches>`.

The following is an example declarative configuration.

[source,xml,indent=0,subs="verbatim,attributes",role="primary"]
.XML
----
<hazelcast>
    ...
    <map>
        <query-caches>
            <query-cache name="myContQueryCache">
                <include-value>true</include-value>
                <predicate type="class-name">com.hazelcast.examples.ExamplePredicate</predicate>
                <entry-listeners>
                    <entry-listener>...</entry-listener>
                </entry-listeners>
                <in-memory-format>BINARY</in-memory-format>
                <populate>true</populate>
                <coalesce>false</coalesce>
                <batch-size>2</batch-size>
                <delay-seconds>3</delay-seconds>
                <buffer-size>32</buffer-size>
                <eviction size="1000" max-size-policy="ENTRY_COUNT" eviction-policy="LFU"/>
                <indexes>
                    <index>
                        <attributes>
                            <attribute>age</attribute>
                        </attributes>
                    </index>
                </indexes>
            </query-cache>
        </query-caches>
    </map>
    ...
</hazelcast>
----

[source,yml,indent=0,subs="verbatim,attributes",role="secondary"]
.YAML
----
hazelcast:
  map:
    query-caches:
        myContQueryCache:
          include-value: true
          predicate:
            class-name: com.hazelcast.examples.ExamplePredicate
          entry-listeners:
            - class-name: "..."
          in-memory-format: BINARY
          populate: true
          coalesce: false
          batch-size: 2
          delay-seconds: 3
          buffer-size: 32
          eviction:
            size: 1000
            max-size-policy: ENTRY_COUNT
            eviction-policy: LFU
          indexes:
            - attributes:
              - "age"
----


Continuous query caches have the following configuration elements:

* `name`: Name of your continuous query cache.
* `include-value`: Specifies whether the value will be cached too. Its default
value is true.
* `predicate`: Predicate to filter events which are applied to the query cache.
* `entry-listeners`: Adds listeners (listener classes) for your query cache entries.
See the <<registering-map-listeners, Registering Map Listeners section>>.
* `in-memory-format`: Type of the data to be stored in your query cache.
See the <<setting-in-memory-format, Setting In-Memory Format section>>.
Its default value is BINARY.
* `populate`: Specifies whether the initial population of your query cache is
enabled. Its default value is true.
* `coalesce`: Specifies whether the coalescing of your query cache is enabled.
Its default value is false.
* `delay-seconds`: Minimum time in seconds that an event waits in the member's
buffer. Its default value is 0.
* `batch-size`: Batch size used to determine the number of events sent in a
batch to your query cache. Its default value is 1.
* `buffer-size`: Maximum number of events which can be stored in a partition
buffer. Its default value is 16.
* `eviction`: Configuration for the eviction of your query cache. See the
<<configuring-map-eviction, Configuring Map Eviction section>>.
* `indexes`: Indexes for your query cache defined by using this element's `<index>`
sub-elements. See the <<configuring-imap-indexes, Configuring IMap Indexes section>>.

Please take the following configuration considerations and publishing logic into account:

If  `delay-seconds` is equal to or smaller than **0**, then `batch-size` loses its
function. Each time there is an event, all the entries in the buffer are pushed to the subscriber.

If `delay-seconds` is bigger than **0**, the following logic applies:

* If `coalesce` is set to **true**, the buffer is checked for an event with the
same key; if so, it is overridden by the current event. Then:
** The current size of the buffer is checked: if the current size of the buffer
is equal to or larger than `batch-size`, then the events counted as much as the
`batch-size` are pushed to the subscriber. Otherwise, no events are sent.
** After finishing with checking `batch-size`, the `delay-seconds` is checked.
The buffer is scanned from the oldest to youngest entries; all the entries that are
older than `delay-seconds` are pushed to the subscriber.

=== MapReduce Deprecation and Removal

This section informs Hazelcast users about the MapReduce deprecation and removal,
its motivation and replacements.

==== Motivation

We've decided to deprecate the MapReduce framework in Hazelcast IMDG 3.8.
MapReduce support was completely removed in Hazelcast IMDG 4.0. The
MapReduce framework provided the distributed computing model and it was used
to back the old Aggregations system. Unfortunately the implementation didn't
live up to the expectations and adoption wasn't high, so it never got out of
Beta status. Apart from that the current shift in development away from M/R-like
processing to a more near-realtime, streaming approach left us with the decision
to deprecate and finally remove the MapReduce framework from Hazelcast IMDG. With
that said, we want to introduce the successors and replacements; fast Aggregations
on top of Query infrastructure and the Hazelcast Jet distributed computing platform.

==== Built-In Aggregations

MapReduce is a very powerful tool, however it's demanding in terms of space, time
and bandwidth. We realized that we don't need so much power when we simply want
to find out a simple metric such as the number of entries matching a predicate.
Therefore, the built-in aggregations were rebuilt on top of the existing Query
infrastructure (count, sum, min, max, mean, variance) which automatically leverages
any matching query index. The aggregations are computed in tho phases:

* 1st phase: on each member (scatter)
* 2nd phase: one member aggregates responses from members (gather)

It is not as flexible as a full-blown M/R system due to the 2nd phase being
single-member and the input can be massive in some use cases. The member doing
the 2nd step needs enough capacity to hold all intermediate results from all
members from the 1st step, but in practice it is sufficient for many aggregation
tasks like "find average" or "find highest" and other common examples.

The benefits are:

* improved performance
* simplified API
* utilization of existing indexes.

See the <<aggregations, Aggregations section>> for examples. If you
need a more powerful tool like MapReduce, then there is Hazelcast Jet.
See its reference <<hazelcast-jet, here>> and link:https://jet.hazelcast.org/[website^]
for more information.

==== Jet Compared with New Aggregations

Hazelcast has native support for aggregation operations on the contents of its
distributed data structures. They operate on the assumption that the aggregating
function is commutative and associative, which allows the two-tiered approach where
first the local data is aggregated, then all the local subresults sent to one member,
where they are combined and returned to the user. This approach works quite well as
long as the result is of manageable size. Many interesting aggregations produce an O(1)
result and for those, the native aggregations are a good match.

The main area where native aggregations may not be sufficient are the operations that
group the data by key and produce results of size O (`keyCount`). The architecture of
Hazelcast aggregations is not well adapted to this use case, although it still works
even for moderately-sized results (up to 100 MB, as a ballpark figure). Beyond these
numbers, and whenever something more than a single aggregation step is needed, Jet
becomes the preferred choice. In the mentioned use case Jet helps because it doesn't
send the entire hashtables in serialized form and materialize all the results on the
user's machine, but rather streams the key-value pairs directly into a target IMap.
Since it is a distributed structure, it doesn't focus its load on a single member.

Jet's DAG paradigm offers much more than the basic map-reduce-combine cascade. Among
other setups, it can compose several such cascades and also perform co-grouping,
joining and many other operations in complex combinations.
