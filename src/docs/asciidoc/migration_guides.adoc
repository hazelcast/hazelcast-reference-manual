
[appendix]
== Migration Guides

This appendix provides guidelines when upgrading to a new Hazelcast IMDG version.
See also the link:https://docs.hazelcast.org/docs/rn/index.html[release notes^] document
for the changes for each Hazelcast IMDG release.

=== Upgrading to Hazelcast IMDG 4.0

This section provides the guidelines for you
when migrating to Hazelcast IMDG 4.0

==== Removal of Hazelcast Client Module

* The `hazelcast-client` module has been merged into the core module: All the classes
in the `hazelcast-client` module have been moved to `hazelcast`.
`hazelcast-client.jar` will not be created anymore.
* Also the `com.hazelcast.client` Java module is not used anymore. All classes
are now available within the `com.hazelcast.core` module.

==== Changes in Client Connection Retry Mechanism

* The `connection-attempt-period` and `connection-attempt-limit`
configuration have been removed. Instead, the elements of
`connection-retry` are now used. See the <<configuring-client-connection-retry>>
for the usage of those new elements.

==== Increasing the Member/Client Thread Counts

If there are 20 or more processors detected, the Hazelcast member
by default starts 4+4 (4 input and 4 output) I/O threads.
This is to increase out of the box performance on faster machines
because often (especially the cache with
caching situations) the performance is I/O bound and
having some extra cores available for I/O can make a significant
difference. If less than 20 cores are detected, 3+3 IO threads are used
and the behavior remains the same as Hazelcast IMDG 3.x series.

A smart client, by default, gets 3+3 (3 input and 3 output) I/O threads to
speed up the performance. Before Hazelcast IMDG 4.0, this was
1+1. However, the client I/O can become a bottleneck with too few threads.
If TLS/SSL is enabled, then by default a smart client
makes use of 3+3 I/O threads which was already the case with previous versions.

There is a new performance feature in Hazelcast IMDG 4.0 called
thread overcommit. By default, Hazelcast creates more
threads than it has cores, e.g., on a 20 cores machine it creates 28 threads;
20 threads for the partition operations
and 4+4 threads for I/O. In case of a typical caching usage (get/put/set, etc.)
having too many threads can cause a performance
degradation due to increased context switching. So there is
a new option called `hazelcast.operation.thread.overcommit`.
If this property is set to true, i.e., `-Dhazelcast.operation.thread.overcommit=true`,
which is the default, Hazelcast uses the old style thread
configuration where there are more threads than cores. If set to false,
the number of partition threads plus the I/O threads will be equal to the  core count.
It depends on the environment if this gives a performance boost or not.
In some environments it can give a significant boost
and in some it will give a significant loss; it is best to benchmark
for your specific situation. If you are doing lots of queries or other tasks
which are CPU-bound, e.g aggregations, you probably want to have as many cores available to partition
operations as possible.

==== Removing Deprecated Client Configurations

The following methods of `ClientConfig` have been refactored:

* `addNearCacheConfig(String, NearCacheConfig)` -> `addNearCacheConfig(NearCacheConfig)`
* `setSmartRouting(boolean)` -> `getNetworkConfig().setSmartRouting(boolean);`
* `getSocketInterceptorConfig()` -> `getNetworkConfig().getSocketInterceptorConfig();`
* `setSocketInterceptorConfig(SocketInterceptorConfig)` -> `getNetworkConfig().setSocketInterceptorConfig(SocketInterceptorConfig);`
* `getConnectionTimeout()` -> `getNetworkConfig().getConnectionTimeout();`
* `setConnectionTimeout(int)` -> `getNetworkConfig().setConnectionTimeout(int);`
* `addAddress(String)` -> `getNetworkConfig().addAddress(String);`
* `getAddresses()` -> `getNetworkConfig().getAddresses();`
* `setAddresses(List)` -> `getNetworkConfig().setAddresses(List);`
* `isRedoOperation()` -> `getNetworkConfig().isRedoOperation();`
* `setRedoOperation(boolean)` -> `getNetworkConfig().setRedoOperation(boolean);`
* `getSocketOptions()` -> `getNetworkConfig().getSocketOptions();`
* `setSocketOptions()` -> `getNetworkConfig().setSocketOptions(SocketOptions);`
* `setSocketOptions()` -> `getNetworkConfig().setSocketOptions(SocketOptions);`
* `getNetworkConfig().setAwsConfig(new ClientAwsConfig());` -> `getNetworkConfig().setAwsConfig(new AwsConfig());`

Also the `ClientAwsConfig` class has been renamed as `AwsConfig`.

See the following table for the before/after configuration samples.

[cols="1a,1a"]
|===

| *_Before IMDG 4.0_* | *_After IMDG 4.0_*

|

[source,java,options="nowrap"]
----
ClientConfig clientConfig = new ClientConfig();
clientConfig.addNearCacheConfig("myCache", new NearCacheConfig());
----

|

[source,java,options="nowrap"]
----
ClientConfig clientConfig = new ClientConfig();
NearCacheConfig nearCacheConfig = new NearCacheConfig("myCache");
clientConfig.addNearCacheConfig(nearCacheConfig);
----

|

[source,java,options="nowrap"]
----
ClientConfig clientConfig = new ClientConfig();
            clientConfig.setSmartRouting(true);
            clientConfig.isSmartRouting();
            clientConfig.getSocketInterceptorConfig();
            clientConfig.setSocketInterceptorConfig(new SocketInterceptorConfig());
            clientConfig.getConnectionTimeout();
            clientConfig.setConnectionTimeout(1000);
            clientConfig.addAddress("127.0.0.1:5701");
            clientConfig.getAddresses();
            clientConfig.setAddresses(Collections.singletonList("127.0.0.1:5701"));
            clientConfig.isRedoOperation();
            clientConfig.setRedoOperation(true);
            clientConfig.getSocketOptions();
            clientConfig.setSocketOptions(new SocketOptions());
            clientConfig.getNetworkConfig().setAwsConfig(new ClientAwsConfig());
            ClientAwsConfig awsConfig = clientConfig.getNetworkConfig().getAwsConfig();
        }
----

|

[source,java,options="nowrap"]
----
ClientConfig clientConfig = new ClientConfig();
            clientConfig.getNetworkConfig().setSmartRouting(true);
            clientConfig.getNetworkConfig().isSmartRouting();
            clientConfig.getNetworkConfig().getSocketInterceptorConfig();
            clientConfig.getNetworkConfig().setSocketInterceptorConfig(new SocketInterceptorConfig());
            clientConfig.getNetworkConfig().getConnectionTimeout();
            clientConfig.getNetworkConfig().setConnectionTimeout(1000);
            clientConfig.getNetworkConfig().addAddress("127.0.0.1:5701");
            clientConfig.getNetworkConfig().getAddresses();
            clientConfig.getNetworkConfig().setAddresses(Collections.singletonList("127.0.0.1:5701"));
            clientConfig.getNetworkConfig().isRedoOperation();
            clientConfig.getNetworkConfig().setRedoOperation(true);
            clientConfig.getNetworkConfig().getSocketOptions();
            clientConfig.getNetworkConfig().setSocketOptions(new SocketOptions());
            clientConfig.getNetworkConfig().setAwsConfig(new AwsConfig());
            AwsConfig awsConfig = clientConfig.getNetworkConfig().getAwsConfig();
        }
----

|===

==== Changes in Index Configuration

In order to support further extensibility of Hazelcast,
index configuration has been refactored.

Index type is now defined through the `IndexType` enumeration
instead of the boolean flag: ordered index is now referred to as
`IndexType.SORTED`, unordered as `IndexType.HASH`.

In composite indexes, index parts are now defined as
a list of strings instead of a single string with comma-separated values.

With these changes, the following configuration parameters
have been renamed:

**Programmatic configuration objects and methods:**

* `MapIndexConfig` -> `IndexConfig`
* `MapConfig.getMapIndexConfig` -> `MapConfig.getIndexConfig`
* `MapConfig.setMapIndexConfig` -> `MapConfig.setIndexConfig`
* `MapConfig.addMapIndexConfig` -> `MapConfig.addIndexConfig`
* `IMap.addIndex(String, boolean)` -> `IMap.addIndex(IndexConfig)`


See the following table for the before/after samples.

[cols="1a,1a"]
|===

| *_Before IMDG 4.0_* | *_After IMDG 4.0_*


2+|Programmatic Configuration

|

[source,java,options="nowrap"]
----
MapIndexConfig indexConfig = new MapIndexConfig();
indexConfig.setOrdered(false);
indexConfig.setAttribute("name, age");

MapConfig mapConfig = new MapConfig();
mapConfig.addMapIndexConfig(indexConfig);
----

|

[source,java,options="nowrap"]
----
IndexConfig indexConfig = new IndexConfig();
indexConfig.setType(IndexType.HASH);
indexConfig.addAttribute("name");
indexConfig.addAttribute("age");

MapConfig mapConfig = new MapConfig();
mapConfig.addIndexConfig(indexConfig);
----

2+|Declarative Configuration

|

[source,xml,options="nowrap"]
----
<hazelcast>
    ...
    <map name="person">
        <indexes>
            <index ordered="false">name, age</index>
        </indexes>
    </map>
    ...
</hazelcast>
----

|

[source,xml,options="nowrap"]
----
<hazelcast>
    ...
    <map name="person">
        <indexes>
            <index type="HASH">
                <attributes>
                    <attribute>name</attribute>
                    <attribute>age</attribute>
                </attributes>
            </index>
        </indexes>
    </map>
    ...
</hazelcast>

2+|Dynamic Index Create

|

[source,java,options="nowrap"]
----
IMap map;

map.addIndex("name, age", false);
----

|

[source,java,options="nowrap"]
----
IMap map;

map.addIndex(new IndexConfig(IndexType.HASH, "name", "age"));
----
|===

==== Changes in Custom Attributes

<<custom-attributes, Custom attributes>> are referenced in
predicates, queries and indexes. Some improvements have been
performed in Hazelcast's query engine and one of the results
is the change in custom attribute configurations.

With this change, the following configuration parameters
have been renamed:

**Declarative configuration elements:**

* `extractor` -> `extractor-class-name`

**Programmatic configuration objects and methods:**

* `MapAttributeConfig` -> `AttributeConfig`
* `setExtractor()` -> `setExtractorClassName()`
* `addMapAttributeConfig()` -> `addAttributeConfig()`


See the following table for the before/after samples.

[cols="1a,1a"]
|===

| *_Before IMDG 4.0_* | *_After IMDG 4.0_*

2+|Programmatic Configuration

|

[source,java,options="nowrap"]
----
MapAttributeConfig attributeConfig = new MapAttributeConfig();
attributeConfig.setName("currency");
attributeConfig.setExtractor("com.bank.CurrencyExtractor");

MapConfig mapConfig = new MapConfig();
mapConfig.addMapAttributeConfig(attributeConfig);
----

|

[source,java,options="nowrap"]
----
AttributeConfig attributeConfig = new AttributeConfig();
attributeConfig.setName("currency");
attributeConfig.setExtractorClassName("com.bank.CurrencyExtractor");

MapConfig mapConfig = new MapConfig();
mapConfig.addAttributeConfig(attributeConfig);
----

2+|Declarative Configuration

|

[source,xml,options="nowrap"]
----
<hazelcast>
    ...
    <map name="trades">
        <attributes>
            <attribute extractor="com.bank.CurrencyExtractor">currency</attribute>
        </attributes>
    </map>
    ...
</hazelcast>
----

|

[source,xml,options="nowrap"]
----
<hazelcast>
    ...
    <map name="trades">
        <attributes>
            <attribute extractor-class-name="com.bank.CurrencyExtractor">currency</attribute>
        </attributes>
    </map>
    ...
</hazelcast>
----
|===


Also, some custom query attribute classes were previously abstract classes
with one abstract method. They have been converted into functional interfaces:

* `ValueCallback`
* `ValueExtractor`

[cols="1a,1a"]
|===

| *_Before IMDG 4.0_* | *_After IMDG 4.0_*

|

[source,java,options="nowrap"]
----
public static class PortableNameExtractor extends ValueExtractor<ValueReader, Object> {
    @Override
    public void extract(ValueReader target, Object argument, ValueCollector collector) {
        target.read("name", new ValueCallback<Object>() {
            @Override
            public void onResult(Object value) {
                collector.addObject(value);
            }
        });
    }
}
----

|

[source,java,options="nowrap"]
----
public static class PortableNameExtractor implements ValueExtractor<ValueReader, Object> {
    @Override
    public void extract(ValueReader target, Object argument, ValueCollector collector) {
        target.read("name", (ValueCallback) value -> collector.addObject(value));
    }
}
----
|===

==== Removal of MapReduce

MapReduce API has been removed, which was deprecated
since Hazelcast IMDG 3.8. Instead, you can use the
<<fast-aggregations>> on top of Query infrastructure and the
link:https://docs.hazelcast.org/docs/jet/latest/manual/[Hazelcast Jet^]
distributed computing platform as its successors and replacements.

See the following table for the before(MapReduce)/after(Hazelcast Jet)
word count sample.

[cols="1a,1a"]
|===

| *_Before IMDG 4.0 (MapReduce)_* | *_After IMDG 4.0 (Hazelcast Jet)_*

|

[source,java,options="nowrap"]
----
JobTracker tracker = hazelcastInstance.getJobTracker("default");

IMap<String, String> map = hazelcastInstance.getMap(MAP_NAME);
KeyValueSource<String, String> source = KeyValueSource.fromMap(map);

Job<String, String> job = tracker.newJob(source);
ICompletableFuture<Map<String, Integer>> future = job
           .mapper(new TokenizerMapper())
           .combiner(new WordcountCombinerFactory())
           .reducer(new WordcountReducerFactory())
           .submit();

     System.out.println(ToStringPrettyfier.toString(future.get()));
----

|

[source,java,options="nowrap"]
----
JobTracker t = hz.getJobTracker("word-count");
IMap<Long, String> documents = hz.getMap("documents");
LongSumAggregation<String, String> aggr = new LongSumAggregation<>();
Map<String, Long> counts =
        t.newJob(KeyValueSource.fromMap(documents))
         .mapper((Long x, String document, Context<String, Long> ctx) ->
                 Stream.of(document.toLowerCase().split("\\W+"))
                       .filter(w -> !w.isEmpty())
                       .forEach(w -> ctx.emit(w, 1L)))
         .combiner(aggr.getCombinerFactory())
         .reducer(aggr.getReducerFactory())
         .submit()
         .get();
----
|===

See the link:https://github.com/hazelcast/hazelcast-jet-code-samples/tree/master/core-api/wordcount-core-api[Jet Code Samples^] for a full insight.

==== Refactoring of Migration Listener

The `MigrationListener` API has been refactored.
With this change, an event is published when a new
migration process starts and another event when migration
is completed. These events include statistics
about the migration process including the start time,
planned migration count, completed migration count, etc.

Additionally, a migration event is published on each replica
migration, both for primary and backup replica migrations.
This event includes the partition ID, replica index and
migration progress statistics.

Before IMDG 4.0, the following were the events listened by
`MigrationListener`:

* `migrationStarted`
* `migrationCompleted`
* `migrationFailed`

After IMDG 4.0, we have the following events instead:

* `migrationStarted`
* `migrationFinished`
* `replicaMigrationCompleted`
* `replicaMigrationFailed`

See the following table for the before/after samples.

[cols="1a,1a"]
|===

| *_Before IMDG 4.0_* | *_After IMDG 4.0_*

|

[source,java,options="nowrap"]
----
import com.hazelcast.core.MigrationEvent;
import com.hazelcast.core.MigrationListener;

public class ClusterMigrationListener implements MigrationListener {
    @Override
    public void migrationStarted(MigrationEvent migrationEvent) {
        System.err.println("Started: " + migrationEvent);
    }
    @Override
    public void migrationCompleted(MigrationEvent migrationEvent) {
        System.err.println("Completed: " + migrationEvent);
    }
    @Override
    public void migrationFailed(MigrationEvent migrationEvent) {
        System.err.println("Failed: " + migrationEvent);
    }
}
----

|

[source,java,options="nowrap"]
----
import com.hazelcast.partition.MigrationListener;
import com.hazelcast.partition.MigrationState;
import com.hazelcast.partition.ReplicaMigrationEvent;

public class ClusterMigrationListener implements MigrationListener {

    @Override
    public void migrationStarted(MigrationState state) {
        System.out.println("Migration Started: " + state);
    }

    @Override
    public void migrationFinished(MigrationState state) {
        System.out.println("Migration Finished: " + state);
    }

    @Override
    public void replicaMigrationCompleted(ReplicaMigrationEvent event) {
        System.out.println("Replica Migration Completed: " + event);
    }

    @Override
    public void replicaMigrationFailed(ReplicaMigrationEvent event) {
        System.out.println("Replica Migration Failed: " + event);
    }
}
----
|===

==== Defaulting to OpenSSL

Hazelcast IMDG defaults to use OpenSSL when:

* when you use <<tlsssl, TLS/SSL>> and Hazelcast IMDG detects some <<integrating-openssl-boringssl, OpenSSL>> capabilities
* the Java version is less than 11
* no explicit <<configuring-hazelcast-for-openssl, SSLEngineFactory>> is configured.

==== Changes in the Security Configurations

===== Replacing `group` by Simple Cluster Name Configuration

Both the client and member configurations have the `GroupConfig` (or `<group>` in XML) replaced
by a simple  cluster name configuration. The password part from the `GroupConfig` which
was already deprecated is removed now.

See the following table for the before/after sample configurations.

[cols="1a,1a"]
|===

| *_Before IMDG 4.0_* | *_After IMDG 4.0_*

|

[source,xml,options="nowrap"]
----
<hazelcast>
    <group>
        <name>dev</name>
        <password>dev-pass</password>
    </group>
</hazelcast>
----

|

[source,xml,options="nowrap"]
----
<hazelcast>
    <cluster-name>dev</cluster-name>
</hazelcast>
----
|===

===== Member Authentication and Identity Configuration

Hazelcast IMDG 4.0 replaces the `<member-credentials-factory>`, `<member-login-modules>` and
`<client-login-modules>` configuration by references to security realms.
The security realms is a new abstraction in the security configuration of Hazelcast members.
It defines the security configuration independently on the configuration
part where the security is used. The component requesting security just references
the security realm name.

See the following table for the before/after sample configurations.

[cols="1a,1a"]
|===

| *_Before IMDG 4.0_* | *_After IMDG 4.0_*

|

[source,xml,options="nowrap"]
----
<security enabled="true">
    <member-credentials-factory class-name="com.hazelcast.examples.MyCredentialsFactory">
        <properties>
            <property name="property">value</property>
        </properties>
    </member-credentials-factory>
    <member-login-modules>
        <login-module class-name="com.hazelcast.examples.MyRequiredLoginModule" usage="REQUIRED">
            <properties>
                <property name="property">value</property>
            </properties>
        </login-module>
    </member-login-modules>
    <client-login-modules>
        <login-module class-name="com.hazelcast.examples.MyRequiredLoginModule" usage="REQUIRED">
            <properties>
                <property name="property">value</property>
            </properties>
        </login-module>
    </client-login-modules>
</security>
----

|

[source,xml,options="nowrap"]
----
<security enabled="true">
    <realms>
        <realm name="realm1">
            <authentication>
                <jaas>
                    <login-module class-name="com.hazelcast.examples.MyRequiredLoginModule" usage="REQUIRED">
                        <properties>
                            <property name="property">value</property>
                        </properties>
                    </login-module>
                </jaas>
            </authentication>
            <identity>
                <credentials-factory class-name="com.hazelcast.examples.MyCredentialsFactory">
                    <properties>
                        <property name="property">value</property>
                    </properties>
                </credentials-factory>
            </identity>
        </realm>
    </realms>
    <member-authentication realm="realm1"/>
    <client-authentication realm="realm1"/>
</security>
----
|===

===== Client Identity Configuration

The `<credentials>` configuration is not supported
anymore in the client security configuration.
Existing `<credentials-factory>` configuration allows
to fully replace the credentials as it is more flexible.
There are also new `<username-password>` and `<token>`
configuration elements which simplify the migration.

See the following table for the before/after sample configurations.

[cols="1a,1a"]
|===

| *_Before IMDG 4.0_* | *_After IMDG 4.0_*

|

[source,xml,options="nowrap"]
----
<security>
    <credentials>com.acme.security.JohnDoeCredentials</credentials>
</security>
----

|

[source,xml,options="nowrap"]
----
<security>
    <username-password username="johndoe" password="s3crEt"/>
</security>
----
|===

==== JAAS Authentication Cleanups

===== Introducing New Principal Types

The `ClusterPrincipal` class representing an authenticated user within the JAAS Subject
has been replaced by three different principal types:

* `ClusterIdentityPrincipal`
* `ClusterRolePrincipal`
* `ClusterEndpointPrincipal`

All these new principal types share the `HazelcastPrincipal` interface so
it is simple to get or remove them all from the subject.

With this change, the `Credentials` object is not referenced from
the principals anymore.

Also, `DefaultPermissionPolicy` which was consuming `ClusterPrincipal`
and also reading the endpoint address from it works with the new
`ClusterRolePrincipals` and `ClusterEndpointPrincipals` principal types.

See the following table for the before/after sample `IPermissionPolicy` implementations.

[cols="1a,1a"]
|===

| *_Before IMDG 4.0_* | *_After IMDG 4.0_*

|

[source,java,options="nowrap"]
----
public PermissionCollection getPermissions(Subject subject, Class<? extends Permission> type) {
    PermissionCollection collection = ...;
    for (ClusterPrincipal principal : subject.getPrincipals(ClusterPrincipal.class)) {
      String endpoint = principal.getEndpoint();
      String principalName = principal.getPrincipal();
      addPermissionsToPrincipal(collection, principalName, endpoint);
    }
    return collection;
}
----

|

[source,java,options="nowrap"]
----
public PermissionCollection getPermissions(Subject subject, Class<? extends Permission> type) {
    PermissionCollection collection = ...;
    Set<ClusterEndpointPrincipal> endpointPrincipals = subject.getPrincipals(ClusterEndpointPrincipal.class);
    String endpoint = endpointIterator.hasNext() ? endpointIterator.next().getName() : null;
    for (ClusterRolePrincipal rolePrincipal : subject.getPrincipals(ClusterRolePrincipal.class)) {
        String role = rolePrincipal.getName();
        addPermissionsToPrincipal(collection, role, endpoint);
    }
    return collection;
}
----
|===

===== Changes in ClusterLoginModule

`ClusterLoginModule` in Hazelcast IMDG 3.x contained four
abstract methods to alter the behavior of `LoginModule`:

* `onLogin`
* `onCommit`
* `onAbort`
* `onLogout`

The login module was retrieving `Credentials` and
using it to create the `ClusterPrincipal` back then.

In Hazelcast IMDG 4.0, only `onLogin` is abstract.
Others now have empty implementations. The login module creates
`ClusterEndpointPrincipal` automatically and adds it to the `Subject`.

The `getName()` abstract method has been added. It is used for
constructing `ClusterIdentityPrincipal`. The `addRole(String)` method
can be called by the child implementations to add `ClusterRolePrincipals`
with the given name.

Also, `ClusterLoginModule` introduces three login module options (boolean),
which allows skipping principals of a given type to the JAAS `Subject`.
It allows, for instance, to have just one `ClusterIdentityPrincipal`
in the `Subject` even if there are more login modules in the chain. These
options are:

* `skipIdentity`
* `skipRole`
* `skipEndpoint`.

See the following table for the before/after sample implementations.

[cols="1a,1a"]
|===

| *_Before IMDG 4.0_* | *_After IMDG 4.0_*

|

[source,java,options="nowrap"]
----
// Adds a single "foo" ClusterPrincipal to the JAAS Subject if credentials match.
public class TestLoginModule extends ClusterLoginModule {

    @Override
    public boolean onLogin() throws LoginException {
        UsernamePasswordCredentials usernamePasswordCredentials = (UsernamePasswordCredentials) credentials;
        if ("foo".equals(usernamePasswordCredentials.getUsername())
                && "bar".equals(usernamePasswordCredentials.getPassword())) {
            // the "foo" principal is added
            return true;
        }
        throw new FailedLoginException("Username or password doesn't match expected value.");
    }

    @Override
    public boolean onCommit() {
        return loginSucceeded;
    }

    @Override
    protected boolean onAbort() {
        return true;
    }

    @Override
    protected boolean onLogout() {
        return true;
    }
}
----

|

[source,java,options="nowrap"]
----
// Adds 3 principals to the JAAS Subject if credentials match:
// ClusterIdentityPrincipal "foo", ClusterRolePrincipal "admin" and a ClusterEndpointPrincipal
public class TestLoginModule extends ClusterLoginModule {

    private String name;

    @Override
    public boolean onLogin() throws LoginException {
        NameCallback ncb = new NameCallback("");
        PasswordCallback pcb = new PasswordCallback("", false);
        try {
            callbackHandler.handle(new Callback[] { ncb, pcb });
        } catch (IOException \| UnsupportedCallbackException e) {
            throw new LoginException("Unable to handle credentials");
        }
        name = credentials.getName();
        if ("foo".equals(name)
                && Arrays.equals("bar".toCharArray(), pcb.getPassword())) {
            addRole("admin");
            return true;
        }
        throw new FailedLoginException("Username or password doesn't match expected value.");
    }

    @Override
    protected String getName() {
        return name;
    }
}
----
|===

===== Changes in Credentials for Client Protocol

In Hazelcast IMDG 3.x, the custom credentials coming through
the client protocol was always automatically deserialized. To
avoid this, the `Credentials` interface has been redesigned in
Hazelcast IMDG 4.0 to contain only the `getName()`
(renamed from `getPrincipal()`) method.
The endpoint handling has been moved out of the interface.

Now, `Credentials` has two new subinterfaces:

* `PasswordCredentials`: The existing `UsernamePasswordCredentials` class
is the default implementation.
* `TokenCredentials`: The new `SimpleTokenCredentials` class has been introduced
to implement it.

`TokenCredentials` is just a holder for byte array, and
the authentication implementations themselves, i.e., custom `LoginModules`,
are responsible for the data deserialization when needed.

The data from client authentication message is not deserialized by Hazelcast members
anymore. For standard authentication, `UsernamePasswordCredentials` is constructed.
For custom authentication, `SimpleTokenCredentials` is constructed.
If the original `Credentials` object is not a `PasswordCredentials`
or `TokenCredentials` instance, then it can be deserialized manually.
However, the deserialization during authentication remains a dangerous
operation and should be avoided.

See the following table for the before/after sample implementations.

[cols="1a,1a"]
|===

| *_Before IMDG 4.0_* | *_After IMDG 4.0_*

|

[source,java,options="nowrap"]
----
// login module already gets a deserialized credentials object
public boolean onLogin() throws LoginException {
    if (credentials == null \|\| !(credentials instanceof CustomCredentials)) {
        throw new FailedLoginException("No valid CustomCredentials found");
    }
    CustomCredentials custom = (CustomCredentials) credentials;
    if (!verify(custom.getJsonToken())) {
      throw new FailedLoginException("JSON token is not valid.");
    }
    return true;
}
----

|

[source,java,options="nowrap"]
----
// login module can ask for credentials, but it gets just a byte array "token"
// wrapped in the SimpleTokenCredentials instance
public boolean onLogin() throws LoginException {
    CredentialsCallback cc = new CredentialsCallback();
    try {
        callbackHandler.handle(new Callback[] { cc });
    } catch (IOException | UnsupportedCallbackException e) {
        throw new FailedLoginException("Unable to retrieve credentials. " + e.getMessage());
    }
    Credentials creds = cc.getCredentials();
    if (creds == null \|\| !(creds instanceof TokenCredentials)) {
        throw new FailedLoginException("No valid TokenCredentials found");
    }
    TokenCredentials tokenCreds = (TokenCredentials) creds;
    if (!verify(new String(tokenCreds.getToken()))) {
      throw new FailedLoginException("JSON token is not valid.");
    }
    return true;
}
----
|===


NOTE: `Credentials` serialization and deserialization in the member protocol
has not been changed.

===== Changes in JAAS Callbacks

In Hazelcast IMDG 3.x, the `CallbackHandler` implementation `ClusterCallbackHandler`
was only able to work with Hazelcast's `CredentialsCallback`.
In Hazelcast IMDG 4.0, it also works with the standard Java Callback implementations
`NameCallback` and `PasswordCallback`.

`DefaultLoginModule` was using the login module options to retrieve the
member's `Config` object. Now, custom `Callback` types have been
implemented which can be used to retrieve additional data required for
the authentication.

List of the supported ``Callback``s in Hazelcast IMDG 4.0:

* `javax.security.auth.callback.NameCallback`
* `javax.security.auth.callback.PasswordCallback`
* `com.hazelcast.security.CredentialsCallback` (provides access to the incoming `Credentials` instance)
* `com.hazelcast.security.EndpointCallback` (allows retrieving the remote host address, it's a replacement for `Credentials.getEndpoint()` in Hazelcast IMDG 3.x)
* `com.hazelcast.security.ConfigCallback` (allows retrieving member's `Config` object)
* `com.hazelcast.security.SerializationServiceCallback` (provides access to Hazelcast `SerializationService`)
* `com.hazelcast.security.ClusterNameCallback` (provides access to Hazelcast cluster name sent by the connecting party)

==== Renaming Quorum as Split Brain Protection

Both in the API/code samples and documentation, the term "quorum" has been
replaced by "split-brain protection".

With this change, the following configuration parameters
have been renamed:

**Declarative configuration elements:**

* `quorum` -> `split-brain-protection`
* `quorum-size` -> `minimum-cluster-size`
* `quorum-ref` ->  `split-brain-protection-ref`
* `quorum-type` -> `protect-on`
* `probabilistic-quorum` -> `probabilistic-split-brain-protection`
* `recently-active-quorum` -> `recently-active-split-brain-protection`
* `quorum-function-class-name` -> `split-brain-protection-function-class-name`
* `quorum-listeners` -> `split-brain-protection-listeners`

**Programmatic configuration objects and methods:**

* `QuorumConfig` -> `SplitBrainProtectionConfig`
* `QuorumConfig.setSize()` -> `SplitBrainProtectionConfig.setMinimumClusterSize()`
* `QuorumConfig.setType()` -> `SplitBrainProtectionConfig.setProtectOn()`
* `QuorumListenerConfig` -> `SplitBrainProtectionListenerConfig`
* `QuorumEvent` -> `SplitBrainProtectionEvent`
* `QuorumService` -> `SplitBrainProtectionService`
* `QuorumService.getQuorum()` -> `SplitBrainProtectionService.getSplitBrainProtection()`
* `isPresent()` -> `hasMinimumSize()`
* `setQuorumName()` -> `setSplitBrainProtectionName()`
* `addQuorumConfig()` -> `addSplitBrainProtectionConfig()`
* `newProbabilisticQuorumConfigBuilder()` -> `newProbabilisticSplitBrainProtectionConfigBuilder()`
* `newRecentlyActiveQuorumConfigBuilder()` -> `newRecentlyActiveSplitBrainProtectionConfigBuilder()`

See the following table for a before/after sample.

[cols="1a,1a"]
|===

| *_Before IMDG 4.0_* | *_After IMDG 4.0_*

|

[source,xml,options="nowrap"]
----
<hazelcast>
    ...
    <quorum name="quorumRuleWithFourMembers" enabled="true">
        <quorum-size>4</quorum-size>
    </quorum>
    <map name="default">
        <quorum-ref>quorumRuleWithFourMembers</quorum-ref>
    </map>
    ...
</hazelcast>
----

|

[source,xml,options="nowrap"]
----
<hazelcast>
    ...
    <split-brain-protection name="splitBrainProtectionRuleWithFourMembers" enabled="true">
        <minimum-cluster-size>4</minimum-cluster-size>
    </split-brain-protection>
    <map name="default">
        <split-brain-protection-ref>splitBrainProtectionRuleWithFourMembers</split-brain-protection-ref>
    </map>
    ...
</hazelcast>
----
|===

==== Renaming getID to getClassId in IdentifiedDataSerializable

The `getId()` method of the `IdentifiedDataSerializable` interface
is a method with a common name, meaning a naming conflict would happen frequently.
For example, database entities also have a `getId()` method.
Therefore, it has been renamed as `getClassId()`.

See the following table showing the interface code before and after IMDG 4.0.

[cols="1a,1a"]
|===

| *_Before IMDG 4.0_* | *_After IMDG 4.0_*

|

[source,java,options="nowrap"]
----
package com.hazelcast.nio.serialization;

public interface IdentifiedDataSerializable extends DataSerializable {

    int getFactoryId();

    int getId();
}
----

|

[source,java,options="nowrap"]
----
package com.hazelcast.nio.serialization;

public interface IdentifiedDataSerializable extends DataSerializable {

    int getFactoryId();

    int getClassId();
}

----
|===

==== Renaming `group-name` as `cluster-name`

The `group` configuration element has been renamed
as `cluster`.

See the following table showing before/after samples.

[cols="1a,1a"]
|===

| *_Before IMDG 4.0_* | *_After IMDG 4.0_*

|

[source,java,options="nowrap"]
----
Config config = new Config();
config.getGroupConfig().setName( "production" );
----

|

[source,java,options="nowrap"]
----
Config config = new Config();
config.getClusterName( "production" );
----
|===


==== Introducing Lambda Friendly Interfaces

===== Entry Processor

The `EntryBackupProcessor` interface has been removed in favor
of `EntryProcessor` which now defines how the entries will be processed
both on the primary and the backup replicas.

Because of this, the `AbstractEntryProcessor` interface has been removed.
This should make writing entry processors more lambda friendly.

[cols="1a,1a"]
|===

| *_Before IMDG 4.0_* | *_After IMDG 4.0_*

|

[source,java]
----
        map.executeOnKey(key, new AbstractEntryProcessor<Integer, Employee>() {

            @Override
            public Object process(Map.Entry<Integer, Employee> entry) {
                Employee employee = entry.getValue();
                if (employee == null) {
                    employee = new Employee();
                }
                employee.setSalary(value);
                entry.setValue(employee);
                return null;
            }
        });
----

|

[source,java]
----
map.executeOnKey(key,
        entry -> {
            Employee employee = entry.getValue();
            if (employee == null) {
                employee = new Employee();
            }
            employee.setSalary(value);
            entry.setValue(employee);
            return null;
        });
----
|===

This should cover most cases. If you need to define a custom
backup entry processor, you can override the `EntryProcessor#getBackupProcessor` method.

[source,java,options="nowrap"]
----
map.executeOnKey(key, new EntryProcessor<Object, Object, Object>() {
    @Override
    public Object process(Entry<Object, Object> entry) {
        // process primary entry
    }

    private Object processBackupEntry(Entry<Object, Object> backupEntry) {
        // process backup entry
    }

    @Nullable
    @Override
    public EntryProcessor<Object, Object, Object> getBackupProcessor() {
        return this::processBackupEntry;
    }
});
----

===== Functional and Serializable Interfaces

Introduces interfaces with single abstract method which declares a
checked exception. The interfaces are also `Serializable` and can be
readily used when providing a lambda which is then serialized.

The `Projection` class was an abstract interface for historical reasons.
It has been turned into a functional interface so it's more lambda-friendly.

See the following table for the before/after sample implementations.

[cols="1a,1a"]
|===

| *_Before IMDG 4.0_* | *_After IMDG 4.0_*

|

[source,java,options="nowrap"]
----
Collection<String> keys = map.project(new Projection<Entry<String, Double>, String>() {
    @Override
    public String transform(Entry<String, Double> input) {
        return input.getKey();
    }
});
----

|

[source,java,options="nowrap"]
----
Collection<String> keys = map.project(Entry::getKey);
----
|===

==== Expanding Nullable/Nonnull Annotations

The APIs of the distributed data structures have been made cleaner
by adding `Nullable` and `Nonnull` annotations, and
their API documentation have been improved:

* Now, it is obvious when looking at the API where `null` is allowed and
where it is not.
* Some methods were throwing `NullPointerException` while others were throwing
`IllegalArgumentException`. Now the behavior is aligned and an unexpected `null`
argument results in a `NullPointerException` being thrown.
* Some methods actually allowed `null` but there was no indication that they did.
* A method when used on the member would accept `null` and have some behavior
accordingly while, on the client, the method would throw a `NullPointerException`.
Now, the behavior of the member and client have been aligned.

The data structures and interfaces enhanced in this sense are listed below:

* `IQueue`, `ISet`, `IList`
* `IMap`, `MultiMap`, `ReplicatedMap`
* `Cluster`
* `ITopic`
* `Ringbuffer`
* `ScheduledExecutor`

==== Removal of ICompletableFuture

In Hazelcast IMDG 3.x series, `com.hazelcast.core.ICompletableFuture` was
introduced to enable reactive programming style. `ICompletableFuture` was
intended as a temporary, JDK 6 compatible replacement for `java.util.concurrent.CompletableFuture`
that was introduced in Java 8. Since Hazelcast 4.0 requires Java 8, the user-facing
asynchronous Hazelcast API methods now have their return type changed from
`ICompletableFuture` to Java 8's `java.util.concurrent.CompletionStage`.

Dependent computation stages registered using default async methods which do not
accept an explicit `Executor` argument (such as `thenAcceptAsync`, `whenCompleteAsync` etc)
are executed by the `java.util.concurrent.ForkJoinPool#commonPool()` (unless it does not
support a parallelism level of at least two, in which case, a new `Thread` is created to
run each task).

See the following table for the before/after samples.

[cols="1a,1a"]
|===

| *_Before IMDG 4.0_* | *_After IMDG 4.0_*

|

[source,java,options="nowrap"]
----
import com.hazelcast.core.ExecutionCallback;
import com.hazelcast.core.Hazelcast;
import com.hazelcast.core.HazelcastInstance;
import com.hazelcast.core.IMap;

public class Main {

    public static void main(String[] args) {
        HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
        IMap<Integer, String> map = hazelcastInstance.getMap("map");

        map.putAsync(1, "one").andThen(new ExecutionCallback<String>() {
            @Override
            public void onResponse(String response) {
                map.getAsync(1).andThen(new ExecutionCallback<String>() {
                    @Override
                    public void onResponse(String response) {
                        System.out.println("Value of 1 is " + response);
                    }

                    @Override
                    public void onFailure(Throwable t) {
                        t.printStackTrace();
                    }
                });
            }

            @Override
            public void onFailure(Throwable t) {
                t.printStackTrace();
            }
        });
    }
}
----

|

[source,java,options="nowrap"]
----
import com.hazelcast.core.Hazelcast;
import com.hazelcast.core.HazelcastInstance;
import com.hazelcast.map.IMap;

public class Main {

    public static void main(String[] args) {
        HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
        IMap<Integer, String> map = hazelcastInstance.getMap("map");

        map.putAsync(1, "one").whenCompleteAsync((response, throwable) -> {
            if (throwable == null) {
                map.getAsync(1).thenAcceptAsync(v -> {
                    System.out.println("Value of 1 is " + v);
                });
            } else {
                throwable.printStackTrace();
            }
        });
    }
}
----
|===



==== WAN Replication Configuration Changes

Previously, Configuring WAN replication was problematic:

* You needed to specify the fully qualified class name of the WAN implementation that should be used.
In most cases, this was the built-in Hazelcast IMDG Enterprise Edition (EE) implementation.
* There were various configuration options, some of which were present as Java class instance fields
or XML child nodes and attributes while others were present in a properties list. The issue with
the property list is that there was no checking for typos, no documentation and no IDE help.
* If you wanted to use a custom WAN publisher SPI implementation, some configuration options did not
make sense as they were tied to our implementation, e.g., WAN queue size.
* It was verbose.

The tag which was supposed to cover both cases, using the built-in Hazelcast EE implementation and a
custom WAN replication implementation (`wan-publisher` or `WanPublisherConfig`), has been separated into
two configuration elements/classes to be used for built-in and custom WAN publishers:

* `batch-publisher` (declarative configuration) or `WanBatchReplicationPublisherConfig` (programmatic configuration)
* `custom-publisher` (declarative configuration) or `CustomWanPublisherConfig` (programmatic configuration)

This means, if you're using the Hazelcast built-in WAN replication, the new configuration element
is `batch-publisher` or `WanBatchReplicationPublisherConfig`.
If you're using a custom WAN replication implementation, the new configuration element is
`custom-publisher` or `CustomWanPublisherConfig`.

Additionally, the group password has been removed from the configuration and now only the cluster name is checked
when connecting to the target cluster. This has been done to align the behavior with members forming a single
cluster, where members with different passwords but with the same cluster name (previously group name)
could form a cluster.

See the following tables for the before/after configuration examples.

[cols="1a,1a"]
|===

| *_Before IMDG 4.0_* | *_After IMDG 4.0_*

| The following was an example declarative configuration for a built-in `wan-publisher`:

[source,xml,options="nowrap"]
----
<wan-publisher group-name="builtInPublisher" publisher-id="builtInPublisherId">
    <class-name>com.hazelcast.enterprise.wan.impl.replication.WanBatchReplication</class-name>
    <queue-capacity>15000</queue-capacity>
    <queue-full-behavior>DISCARD_AFTER_MUTATION</queue-full-behavior>
    <initial-publisher-state>REPLICATING</initial-publisher-state>
    <wan-sync>
        <consistency-check-strategy>NONE</consistency-check-strategy>
    </wan-sync>
    <properties>
        <property name="endpoints">10.3.5.1:5701,10.3.5.2:5701</property>
        <property name="batch.size">1000</property>
        <property name="batch.max.delay.millis">2000</property>
        <property name="response.timeout.millis">60000</property>
        <property name="ack.type">ACK_ON_OPERATION_COMPLETE</property>
        <property name="snapshot.enabled">false</property>
        <property name="group.password">nyc-pass</property>
    </properties>
</wan-publisher>
----

Or, in programmatic config:

[source,java,options="nowrap"]
----
WanPublisherConfig publisherConfig = new WanPublisherConfig()
        .setGroupName("builtInPublisher")
        .setPublisherId("builtInPublisherId")
        .setClassName("com.hazelcast.enterprise.wan.impl.replication.WanBatchReplication")
        .setQueueCapacity(15000)
        .setQueueFullBehavior(WANQueueFullBehavior.DISCARD_AFTER_MUTATION)
        .setInitialPublisherState(WanPublisherState.REPLICATING);
publisherConfig.getWanSyncConfig().setConsistencyCheckStrategy(ConsistencyCheckStrategy.NONE);
Map<String, Comparable> properties = publisherConfig.getProperties();
properties.put("endpoints", "10.3.5.1:5701,10.3.5.2:5701");
properties.put("batch.size", 1000);
properties.put("batch.max.delay.millis", 2000);
properties.put("response.timeout.millis", 60000);
properties.put("ack.type", WanAcknowledgeType.ACK_ON_OPERATION_COMPLETE.toString());
properties.put("snapshot.enabled", false);
properties.put("group.password", "nyc-pass");
----

| And the following is the equivalent of the above configuration after IMDG 4.0:

[source,xml,options="nowrap"]
----
<batch-publisher>
    <cluster-name>builtInPublisher</cluster-name>
    <publisher-id>builtInPublisherId</publisher-id>
    <batch-size>1000</batch-size>
    <batch-max-delay-millis>2000</batch-max-delay-millis>
    <response-timeout-millis>60000</response-timeout-millis>
    <acknowledge-type>ACK_ON_OPERATION_COMPLETE</acknowledge-type>
    <initial-publisher-state>REPLICATING</initial-publisher-state>
    <snapshot-enabled>false</snapshot-enabled>
    <queue-full-behavior>DISCARD_AFTER_MUTATION</queue-full-behavior>
    <queue-capacity>10000</queue-capacity>
    <target-endpoints>10.3.5.1:5701,10.3.5.2:5701</target-endpoints>
    <wan-sync>
        <consistency-check-strategy>NONE</consistency-check-strategy>
    </wan-sync>
</batch-publisher>
----

Or, in programmatic config:

[source,java,options="nowrap"]
----
WanBatchReplicationPublisherConfig publisherConfig = new WanBatchReplicationPublisherConfig()
        .setClusterName("builtInPublisher")
        .setPublisherId("builtInPublisherId")
        .setClassName("com.hazelcast.enterprise.wan.impl.replication.WanBatchReplication")
        .setQueueCapacity(15000)
        .setQueueFullBehavior(WanQueueFullBehavior.DISCARD_AFTER_MUTATION)
        .setInitialPublisherState(WanPublisherState.REPLICATING)
        .setTargetEndpoints("10.3.5.1:5701,10.3.5.2:5701")
        .setBatchSize(1000)
        .setBatchMaxDelayMillis(2000)
        .setResponseTimeoutMillis(60000)
        .setAcknowledgeType(WanAcknowledgeType.ACK_ON_OPERATION_COMPLETE)
        .setSnapshotEnabled(false);
publisherConfig.getWanSyncConfig().setConsistencyCheckStrategy(ConsistencyCheckStrategy.NONE);
----
|===

[cols="1a,1a"]
|===

| *_Before IMDG 4.0_* | *_After IMDG 4.0_*

| The following was an example declarative configuration for a custom `wan-publisher`:

[source,xml,options="nowrap"]
----
<wan-publisher group-name="customWanPublisherId">
    <class-name>com.myCompany.MyImplementation</class-name>
    <properties>
        <property name="some.property">some-value</property>
        <property name="some.other.property">some-other-value</property>
    </properties>
</wan-publisher>
----

Or, in programmatic config:

[source,java,options="nowrap"]
----
WanPublisherConfig publisherConfig = new WanPublisherConfig()
        .setGroupName("customWanPublisherId")
        .setClassName("com.myCompany.MyImplementation");
Map<String, Comparable> properties = publisherConfig.getProperties();
properties.put("some.property", "some-value");
properties.put("some.other.property", "some-other-value");
----

| And the following is the equivalent of the above configuration after IMDG 4.0:

[source,xml,options="nowrap"]
----
<custom-publisher>
    <publisher-id>customPublisherId</publisher-id>
    <class-name>com.myCompany.MyImplementation</class-name>
    <properties>
        <property name="some.property">some-value</property>
        <property name="some.other.property">some-other-value</property>
    </properties>
</custom-publisher>
----

Or, in programmatic config:

[source,java,options="nowrap"]
----
CustomWanPublisherConfig publisherConfig = new CustomWanPublisherConfig()
        .setPublisherId("customWanPublisherId")
        .setClassName("com.myCompany.MyImplementation");
Map<String, Comparable> properties = publisherConfig.getProperties();
properties.put("some.property", "some-value");
properties.put("some.other.property", "some-other-value");
----
|===



==== WAN Replication SPI Changes

In IMDG 3.x series, the WAN publisher SPI allowed you to plug into the lifecycle of a map/cache entry
and replicate the updates to another system. For example, you might implement replication to
Kafka or some JMS queue or even write out map and cache event changes to a log on disk.
The SPI was not very intuitive though:

* It was not clear which interface needed to be implemented (`WanReplicationPublisher` vs. `WanReplicationEndpoint`).
* You had to implement different interfaces, depending on whether you were using Hazelcast IMDG
Open Source or Enterprise edition.
* There were cases of leaking internals which don't make sense for some custom implementations.
* There were unused methods in the public SPI.

In Hazelcast IMDG 4.0, we have provided a new and cleaner WAN publisher SPI. You only need to
implement a single interface: `com.hazelcast.wan.WanReplicationPublisher`. This implementation can
then be set in the WAN replication configuration and be used with both Hazelcast Open Source and
Enterprise editions.

==== Predicate API Cleanups

The following refactors and cleanups have been performed
on the public Predicate related API:

* Moved the following classes from the `com.hazelcast.query` package
to `com.hazelcast.query.impl.predicates`:
** `IndexAwarePredicate`
** `VisitablePredicate`
** `SqlPredicate/Parser`
** `TruePredicate`
* Moved the `FalsePredicate` and `SkipIndexPredicate` classes to
the `com.hazelcast.query.impl.predicates` package.
* Converted `PagingPredicate` and `PartitionPredicate` to interfaces
and added `PagingPredicateImpl` and `PartitionPredicateImpl` to
the `com.hazelcast.query.impl.predicate` package.
* Converted `PredicateBuilder` and `EntryObject` to interfaces (and made
`EntryObject` a nested interface in `PredicateBuilder`) and added
`PredicateBuilderImpl` to the `com.hazelcast.query.impl.predicates` package.
* The public API classes/interfaces no longer extend `IndexAwarePredicate`/
`VisitablePredicate`; this dependency has been moved to the `impl` classes.
* Introduced the new factory methods in `Predicates`:
** `newPredicateBuilder()`
** `sql()`
** `pagingPredicate()`
** `partitionPredicate()`

Consequently, the public Predicate API now provides only interfaces (`Predicate`,
`PagingPredicate` and `PartitionPredicate`) with no dependencies on any internal APIs.

==== Changing the UUID String Type to UUID

Some public APIs that return UUID strings have been changed to return UUID.
These changes include `getUuid()` method of the `Endpoint` interface,
`getTxnId()` method of the `TransactionContext` interface,
return values of the listener registrations and `registrationId` parameters for the methods
that de-register the listeners.

See the following table for the before/after sample implementations.

[cols="1a,1a"]
|===

| *_Before IMDG 4.0_* | *_After IMDG 4.0_*

|

[source,java,options="nowrap"]
----
        HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
        String registrationId = hazelcastInstance.getClientService().addClientListener(new ClientListener() {
            @Override
            public void clientConnected(Client client) {
                String clientUuid = client.getUuid();
                System.out.println("Client connected >>> " + clientUuid);
            }

            @Override
            public void clientDisconnected(Client client) {
                String clientUuid = client.getUuid();
                System.out.println("Client disconnected >>> " + clientUuid);
            }
        });
        hazelcastInstance.getClientService().removeClientListener(registrationId);
----

|

[source,java,options="nowrap"]
----
        HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
        UUID registrationId = hazelcastInstance.getClientService().addClientListener(new ClientListener() {
            @Override
            public void clientConnected(Client client) {
                UUID clientUuid = client.getUuid();
                System.out.println("Client connected >>> " + clientUuid);
            }

            @Override
            public void clientDisconnected(Client client) {
                UUID clientUuid = client.getUuid();
                System.out.println("Client disconnected >>> " + clientUuid);
            }
        });
        hazelcastInstance.getClientService().removeClientListener(registrationId);
----
|===

==== Removal of Deprecated Concurrency API Implementations

After introduction of _CP Subsystem_ in Hazelcast IMDG 3.12,
legacy implementations of the distributed concurrency APIs, e.g., `ILock` and `IAtomicLong`,
had been deprecated.
In IMDG 4.0, these deprecated implementations and additionally
`ILock` and `ICondition` interfaces are completely removed.

Differently from Hazelcast IMDG 3.12, _CP Subsystem_ received
an _unsafe_ operation mode in IMDG 4.0 which provides weaker
consistency guarantees similar to former implementations in Hazelcast IMDG 3.x series.

For more information see the <<cp-subsystem, CP Subsystem section>>.

See the following table for the before/after samples.

[cols="1a,1a"]
|===

| *_Before IMDG 4.0_* | *_After IMDG 4.0_*

|

[source,java,options="nowrap"]
----
import com.hazelcast.core.Hazelcast;
import com.hazelcast.core.HazelcastInstance;
import com.hazelcast.core.IAtomicLong;
import com.hazelcast.core.IAtomicReference;
import com.hazelcast.core.ICountDownLatch;
import com.hazelcast.core.ILock;
import com.hazelcast.core.ISemaphore;

public class Main {

    public static void main(String[] args) {
        HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();

        IAtomicLong atomiclong = hazelcastInstance.getAtomicLong("atomiclong");
        atomiclong.incrementAndGet();

        IAtomicReference<String> atomicref = hazelcastInstance.getAtomicReference("atomicref");
        atomicref.set("value");

        ILock lock = hazelcastInstance.getLock("lock");
        lock.tryLock();

        ISemaphore semaphore = hazelcastInstance.getSemaphore("semaphore");
        semaphore.tryAcquire();

        ICountDownLatch latch = hazelcastInstance.getCountDownLatch("latch");
        latch.countDown();
    }
}
----

|

[source,java,options="nowrap"]
----
import com.hazelcast.core.Hazelcast;
import com.hazelcast.core.HazelcastInstance;
import com.hazelcast.cp.CPSubsystem;
import com.hazelcast.cp.IAtomicLong;
import com.hazelcast.cp.IAtomicReference;
import com.hazelcast.cp.ICountDownLatch;
import com.hazelcast.cp.ISemaphore;
import com.hazelcast.cp.lock.FencedLock;

public class Main {

    public static void main(String[] args) {
        HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
        CPSubsystem cpSubsystem = hazelcastInstance.getCPSubsystem();

        IAtomicLong atomiclong = cpSubsystem.getAtomicLong("atomiclong");
        atomiclong.incrementAndGet();

        IAtomicReference<String> atomicref = cpSubsystem.getAtomicReference("atomicref");
        atomicref.set("value");

        FencedLock lock = cpSubsystem.getLock("lock");
        lock.tryLock();

        ISemaphore semaphore = cpSubsystem.getSemaphore("semaphore");
        semaphore.tryAcquire();

        ICountDownLatch latch = cpSubsystem.getCountDownLatch("latch");
        latch.countDown();
    }
}
----
|===

==== Removal of Legacy Merge Policies

All legacy merge policies have been removed. Replacements of
legacies are under the `com.hazelcast.spi.merge` package.

These are the replacements for IMap and ICache:

_Removed IMap Merge Policies and Their Replacements_

* `com.hazelcast.map.merge.HigherHitsMapMergePolicy` -> `com.hazelcast.spi.merge.HigherHitsMergePolicy`
* `com.hazelcast.map.merge.LatestUpdateMapMergePolicy` -> `com.hazelcast.spi.merge.LatestUpdateMergePolicy`
* `com.hazelcast.map.merge.PassThroughMergePolicy` -> `com.hazelcast.spi.merge.PassThroughMergePolicy`
* `com.hazelcast.map.merge.PutIfAbsentMapMergePolicy` -> `com.hazelcast.spi.merge.PutIfAbsentMergePolicy`

_Removed ICache Merge Policies and Their Replacements_

* `com.hazelcast.cache.merge.HigherHitsCacheMergePolicy` -> `com.hazelcast.spi.merge.HigherHitsMergePolicy`
* `com.hazelcast.cache.merge.LatestAccessCacheMergePolicy` -> `com.hazelcast.spi.merge.LatestAccessMergePolicy`
* `com.hazelcast.cache.merge.PassThroughCacheMergePolicy` -> `com.hazelcast.spi.merge.PassThroughMergePolicy`
* `com.hazelcast.cache.merge.PutIfAbsentCacheMergePolicy` -> `com.hazelcast.spi.merge.PutIfAbsentMergePolicy`

Moreover, the `setMergePolicy/getMergePolicy` methods have been
removed from `MapConfig`, `ReplicatedMapConfig` and `CacheConfig`.
They have been replaced by the `setMergePolicyConfig/getMergePolicyConfig` methods.

The `merge-policy` declarative configuration element that
has been used in the older IMDG versions still can be used:

```
<merge-policy batch-size="100">LatestAccessMergePolicy</merge-policy>
```

==== AWS Configuration Changes

AWS programmatic configuration is merged with a more universal configuration
infrastructure common to all cloud providers. The declarative configuration
remains unchanged.

[cols="1a,1a"]
|===

| *_Before IMDG 4.0_* | *_After IMDG 4.0_*

|

[source,java,options="nowrap"]
----
AwsConfig config = new AwsConfig();
config.setSecretKey("my-secret-key") ;
config.setRegion("my-region");
config.setSecurityGroupName("my-security-group");
config.setTagKey("my-tag-key");
config.setTagValue("my-tag-value");
...
config.setEnabled(true);
----

|

[source,java,options="nowrap"]
----
AwsConfig config = new AwsConfig();
config.setProperty("secret-key", "my-secret-key") ;
config.setProperty("region", "my-region");
config.setProperty("security-group-name", "my-security-group-name");
config.setProperty("tag-key", "my-tag-key");
config.setProperty("tag-value", "my-tag-value");
...
config.setEnabled(true);
----
|===

==== Undeprecation of Deprecated System Properties

The following properties have proven to be useful in containerized environments
and no longer deprecated:

* `hazelcast.rest.enabled`
* `hazelcast.memcache.enabled`
* `hazelcast.http.healthcheck.enabled`

==== Removal of Deprecations in `LoginModuleConfig`

The following deprecated methods were removed:

* `getImplementation()`, replaced by `getClassName()`.
* `setImplementation(Object)`, replaced by `setClassName(String)`.

In declarative configuration `class-name` property should be used instead.

==== Removal of Deprecations in `MultiMapConfig`

The following deprecated methods were removed:

* `getSyncBackupCount()`, replaced by `getBackupCount()`.
* `setSyncBackupCount(int)`, replaced by `setBackupCount(int)`.

In declarative configuration `backup-count` property should be used instead.

==== Removal of Deprecations in `PartitioningStrategyConfig`

Misspelled `setPartitionStrategy(PartitioningStrategy)` is removed,
`setPartitioningStrategy(PartitioningStrategy)` should be used instead.

==== Removal of Deprecations in `ServiceConfig`

The following deprecated methods were removed:

* `getServiceImpl()`, replaced by `getImplementation()`.
* `setServiceImpl(Object)`, replaced by `setImplementation(Object)`.

==== Removal of Deprecations in `TransactionContext`

Deprecated `getXaResource()` is removed, `HazelcastInstance.getXAResource()`
should be used instead.

=== Upgrading to Hazelcast IMDG 3.12.x

* **REST endpoint authentication**: The authentication to REST endpoints has been changed
in Hazelcast IMDG 3.12. Hazelcast IMDG 3.11.x checks group name and password, while 3.12 checks
just the group name when security is disabled, and it uses the client login modules when the security is enabled.
* **Upgrading Cluster Version From IMDG 3.11 to 3.12**:
For the IMDG versions before 3.12, REST API could be enabled by using the
`hazelcast.rest.enabled` system property, which is deprecated now.
IMDG 3.12 and newer versions introduce the `rest-api` configuration element
along with REST endpoint groups.
Therefore, a configuration change is needed specifically when performing
a rolling member upgrade from IMDG 3.11 to 3.12.
+
So, the steps listed in the above <<rolling-upgrade-procedure>> section
should be as follows:
+
. Shutdown the 3.11 member
. Wait until all partition migrations are completed
. Update the member with 3.12 binaries
. Update the configuration (see below)
. Start the member
+
For the 4th step ("Update the configuration"), the configuration
should be updated as follows:
+
[source,xml]
----
<hazelcast>
    ...
    <rest-api enabled="true">
        <endpoint-group name="CLUSTER_WRITE" enabled="true"/>
    </rest-api>
    ...
</hazelcast>
----
+
See the <<using-the-rest-endpoint-groups>> section for more
information.

=== Upgrading from Hazelcast IMDG 3.10.x

This section provides information to be considered when upgrading from Hazelcast IMDG 3.9.x to 3.10.x and newer.

* Starting with Hazelcast 3.10, split-brain recovery is supported for
the data structures whose in-memory format is `NATIVE`.


=== Upgrading from Hazelcast IMDG 3.9.x

This section provides information to be considered when upgrading from Hazelcast IMDG 3.9.x to 3.10.x and newer.

* The https://docs.hazelcast.org/docs/3.10/manual/html-single/#requirements-and-linuxunix-configuration[system property based configuration]
for Ping Failure Detector is deprecated. Instead, use the elements to configure it, an example of which is shown below:
+
[source,xml]
----
<hazelcast>
    <network>
    ...
        <failure-detector>
            <icmp enabled="true">
                <timeout-milliseconds>1000</timeout-milliseconds>
                <fail-fast-on-startup>true</fail-fast-on-startup>
                <interval-milliseconds>1000</interval-milliseconds>
                <max-attempts>2</max-attempts>
                <parallel-mode>true</parallel-mode>
                <ttl>255</ttl>
            </icmp>
        </failure-detector>
    </network>
    ...
</hazelcast>
----

Until Hazelcast IMDG 3.10, the configuration has been like the following:

[source,xml]
----
<hazelcast>
    ...
    <properties>
        <property name="hazelcast.icmp.enabled">true</property>
        <property name="hazelcast.icmp.parallel.mode">true</property>
        <property name="hazelcast.icmp.timeout">1000</property>
        <property name="hazelcast.icmp.max.attempts">3</property>
        <property name="hazelcast.icmp.interval">1000</property>
        <property name="hazelcast.icmp.ttl">0</property>
    </properties>
    ...
</hazelcast>
----

=== Upgrading to Hazelcast IMDG 3.8.x

This section provides information to be considered when upgrading from Hazelcast IMDG 3.7.x to 3.8.x and newer.

* **Introducing <wan-publisher> element**: The configuration element `<target-cluster>` has been replaced with
the element `<wan-publisher>` in WAN replication configuration.
* **WaitNotifyService** interface has been renamed as **OperationParker**.
* **Synchronizing WAN Target Cluster**: The URL for the related REST call has been changed from
`+http://member_ip:port/hazelcast/rest/wan/sync/map+` to `+http://member_ip:port/hazelcast/rest/mancenter/wan/sync/map+`.
* **`JCache usage`:** Due to a compatibility problem, `CacheConfig` serialization may not
work if your member is 3.8.x where x < 5. You need to use the 3.8.5 or higher versions where the problem is fixed.


=== Upgrading to Hazelcast IMDG 3.7.x

This section provides information to be considered when upgrading from Hazelcast IMDG 3.6.x to 3.7.x and newer.

* **Important note about Hazelcast System Properties:** Even Hazelcast has not been
recommending the usage of `GroupProperties.java` class while benefiting from system properties,
there has been a change to inform to the users who have been using this class:
the class `GroupProperties.java` has been replaced by `GroupProperty.java`.
In this new class, system properties are instances of the newly introduced `HazelcastProperty` object.
You can access the names of these properties by calling the `getName()` method of `HazelcastProperty`.
* **Removal of WanNoDelayReplication**: `WanNoDelayReplication` implementation of Hazelcast's WAN Replication has been removed.
You can still achieve this behavior by setting the batch size to `1` while configuring the WanBatchReplication.
See the <<defining-wan-replication, Defining WAN Replication section>> for more information.
* **`JCache` usage:** Changes in `JCache` implementation which broke compatibility of 3.6.x clients to 3.7, 3.7.1, 3.7.2 cluster members and
vice versa. 3.7, 3.7.1, 3.7.2 clients are also incompatible with 3.6.x cluster members.
This issue only affects Java clients which use `JCache` functionality.
+
You can use a compatibility option which can be used to ensure backwards compatibility with 3.6.x clients.
+
In order to upgrade a 3.6.x cluster and clients to 3.7.3 (or later), you need to use this
compatibility option on either the member or the client side, depending on which one is upgraded first:
+
** first upgrade your cluster members to 3.7.3, adding property `hazelcast.compatibility.3.6.client=true` to your configuration;
when started with this property, cluster members are compatible with 3.6.x and 3.7.3+ clients but not with 3.7, 3.7.1, 3.7.2 clients.
Once your cluster is upgraded, you may upgrade your applications to use client version 3.7.3+.
** upgrade your clients from 3.6.x to 3.7.3, adding property `hazelcast.compatibility.3.6.server=true` to your Hazelcast client configuration.
A 3.7.3 client started with this compatibility option is compatible with 3.6.x and 3.7.3+ cluster members but incompatible with 3.7, 3.7.1, 3.7.2 cluster members.
Once your clients are upgraded, you may then proceed to upgrade your cluster members to version 3.7.3 or later.
+
You may use any of the supported ways as described in the <<system-properties, System Properties section>> to configure
the compatibility option. When done upgrading your cluster and clients, you may remove the compatibility property from
your Hazelcast member configuration.
* The `eviction-percentage` and `min-eviction-check-millis` elements are deprecated.
They are ignored if configured, since the map eviction is based on the sampling of entries.
See the <<eviction-algorithm, Eviction Algorithm section>> for details.

=== Upgrading to Hazelcast IMDG 3.6.x

This section provides information to be considered when upgrading from Hazelcast IMDG 3.5.x to 3.6.x and newer.

* **Introducing new configuration options for WAN replication:** WAN replication related system properties, which are
configured on a per member basis, can now be configured per target cluster.
The following system properties are no longer valid.
** `hazelcast.enterprise.wanrep.batch.size`, see the <<batch-size, Batch Size section>>.
** `hazelcast.enterprise.wanrep.batchfrequency.seconds`, see the <<batch-maximum-delay, Batch Maximum Delay section>>.
** `hazelcast.enterprise.wanrep.optimeout.millis`, see the <<response-timeout, Response Timeout section>>.
** `hazelcast.enterprise.wanrep.queue.capacity`, see the <<queue-capacity, Queue Capacity section>>.
* **Removal of deprecated `getId()` method**: The method `getId()` in the interface `DistributedObject` has been removed.
Please use the `getName()` method instead.
* **Change in the Custom Serialization in the C++ Client Distribution**:
Before, the method `getTypeId()` was used to retrieve the ID of the object to be serialized.
With this release, the method `getHazelcastTypeId()` is used and you give your object as a parameter to this new method.
Also, `getTypeId()` was used in your custom serializer class; it has been renamed to `getHazelcastTypeId()`, too.
* The `LOCAL` transaction type has been deprecated. Use `ONE_PHASE` for the Hazelcast IMDG releases 3.6 and higher.

=== Upgrading to Hazelcast IMDG 3.5.x

This section provides information to be considered when upgrading from Hazelcast IMDG 3.4.x to 3.5.x and newer.

* **Introducing the `spring-aware` element:** Hazelcast used `SpringManagedContext` to scan `SpringAware` annotations by default.
This was causing some performance overhead for the users who do not use `SpringAware`.
With this release, `SpringAware` annotations are disabled by default.
By introducing the `spring-aware` element, it is possible to enable it by adding the `<hz:spring-aware />` tag to the configuration.
See the <<integration-with-spring, Spring Integration section>>.


=== Upgrading to Hazelcast IMDG 3.x

This section provides information to be considered when upgrading from Hazelcast IMDG 2.x to 3.x.

* **Removal of deprecated static methods:** The static methods of Hazelcast class reaching Hazelcast data components have been removed.
The functionality of these methods can be reached from the `HazelcastInstance` interface.
You should replace the following:
+
```
Map<Integer, String> customers = Hazelcast.getMap( "customers" );
```
+
with
+
[source,java]
----
HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
// or if you already started an instance named "instance1"
// HazelcastInstance hazelcastInstance = Hazelcast.getHazelcastInstanceByName( "instance1" );
Map<Integer, String> customers = hazelcastInstance.getMap( "customers" );
----
+
* **Renaming "instance" to "distributed object":** There were confusions about the term "instance";
it was used for both the cluster members and distributed objects (map, queue, topic, etc. instances).
Starting with this release, the term "instance" is used for Hazelcast instances.
The term "distributed object" is used for map, queue, etc. instances.
You should replace the related methods with the new renamed ones.
3.0.x clients are smart clients in that they know in which cluster member the data is located,
so you can replace your lite members with native clients.
+
[source,java]
----
public static void main( String[] args ) throws InterruptedException {
  HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
  IMap map = hazelcastInstance.getMap( "test" );
  Collection<Instance> instances = hazelcastInstance.getInstances();
  for ( Instance instance : instances ) {
    if ( instance.getInstanceType() == Instance.InstanceType.MAP ) {
      System.out.println( "There is a map with name: " + instance.getId() );
    }
  }
}
----
+
with
+
[source,java]
----
public static void main( String[] args ) throws InterruptedException {
  HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance();
  IMap map = hz.getMap( "test" );
  Collection<DistributedObject> objects = hazelcastInstance.getDistributedObjects();
  for ( DistributedObject distributedObject : objects ) {
    if ( distributedObject instanceof IMap ) {
      System.out.println( "There is a map with name: " + distributedObject.getName() );
    }
  }
}
----
+
* **Package structure change:** `PartitionService` has been moved to the `com.hazelcast.core` package from `com.hazelcast.partition`.
* **Listener API change:** The `removeListener` methods were taking the listener object as a parameter.
But this caused confusion since the same listener object may be used as a parameter for different listener registrations.
So we have changed the listener API. The `addListener` methods returns a unique ID and you can remove a listener by using this ID.
So you should do the following replacement if needed:
+
[source,java]
----
IMap map = hazelcastInstance.getMap( "map" );
map.addEntryListener( listener, true );
map.removeEntryListener( listener );
----
+
with
+
[source,java]
----
IMap map = hazelcastInstance.getMap( "map" );
String listenerId = map.addEntryListener( listener, true );
map.removeEntryListener( listenerId );
----
+
* **IMap changes:**
** `tryRemove(K key, long timeout, TimeUnit timeunit)` returns boolean indicating whether operation is successful.
** `tryLockAndGet(K key, long time, TimeUnit timeunit)` is removed.
** `putAndUnlock(K key, V value)` is removed.
** `lockMap(long time, TimeUnit timeunit)` and `unlockMap()` are removed.
** `getMapEntry(K key)` is renamed as `getEntryView(K key)`. The returned object's type (`MapEntry` class) is renamed as `EntryView`.
** There is no predefined names for merge policies. You just give the full class name of the merge policy implementation:
+
```
<merge-policy>com.hazelcast.map.merge.PassThroughMergePolicy</merge-policy>
```
+
Also the `MergePolicy` interface has been renamed as `MapMergePolicy` and
returning null from the implemented `merge()` method causes the existing entry to be removed.
+
* **IQueue changes:** There is no change on IQueue API but there are changes on how `IQueue` is configured:
there is no backing map configuration for queue. Settings like backup count are directly configured on the queue configuration.
See the <<queue, Queue section>>.
* **Transaction API change:** Transaction API has been changed. See the <<transactions, Transactions chapter>>.
* **ExecutorService API change:** The `MultiTask` and `DistributedTask` classes have been removed.
All the functionality is supported by the newly presented interface IExecutorService.
See the <<executor-service, Executor Service section>>.
* **LifeCycleService API:** The lifecycle has been simplified. The `pause()`, `resume()`, `restart()` methods have been removed.
* **AtomicNumber:** `AtomicNumber` class has been renamed as `IAtomicLong`.
* **ICountDownLatch:** The `await()` operation has been removed. We expect users to use `await()` method with timeout parameters.
* **ISemaphore API:** The `ISemaphore` has been substantially changed. The `attach()`, `detach()` methods have been removed.
*  Before, the default value for `max-size` eviction policy was **cluster_wide_map_size**.
Starting with this release, the default is **PER_NODE**.
After upgrading, the `max-size` should be set according to this new default, if it is not changed.
Otherwise, it is likely that `OutOfMemoryException` may be thrown.
