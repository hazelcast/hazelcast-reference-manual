
== CP Subsystem

The CP subsystem is a component of a Hazelcast cluster that builds an in-memory strongly consistent layer. It is accessed via `HazelcastInstance.getCPSubsystem()`. Its data structures are _CP_ with respect to the _CAP_ principle, i.e., they always maintain linearizability and prefer consistency over availability during the network partitions.

Currently, the CP subsystem contains only the implementations of Hazelcast's concurrency APIs. These APIs do not maintain large states. For this reason, all the members of a Hazelcast cluster do not take part in the CP subsystem. The number of members that take part in the CP subsystem is specified with `CPSubsystemConfig.setCPMemberCount(int)`. Let's suppose the number of CP members is configured as **C**. Then, when Hazelcast cluster starts, the first **C** members form the CP subsystem. These members are called the CP members and they can also contain data for the other regular Hazelcast data structures, such as `IMap`, `ISet`, etc.

Data structures in the CP subsystem run in ``CPGroup``s. A _CP group_ consists of an odd number of ``CPMember``s between 3 and 7. Each CP group independently runs the Raft consensus algorithm. The operations are committed and executed only after they are successfully replicated to the majority of the CP members in a CP group. For instance, in a CP group of 5 CP members, the operations are committed when they are replicated to at least 3 CP members. Size of the CP groups is specified via `CPSubsystemConfig.setGroupSize(int)` and each CP group contains the same number of CP members. See the <<cpsubsystem-configuration, CP Subsystem Configuration section>> for configuration details.

Please note that the size of CP groups does not have to be same with the CP member count. Namely, the number of CP members in the cluster can be larger than the configured CP group size. In this case, CP groups are formed by selecting the CP members randomly. Also note that the current CP subsystem implementation works only in the memory, without persisting any state to the disk. It means that a crashed CP member is not able to recover by reloading its previous state. Therefore, the crashed CP members create a danger for gradually losing the majority of CP groups and eventually cause the total loss of availability of the CP subsystem. To prevent such situations, the failed CP members can be removed from the CP subsystem and replaced in the CP groups with the other available CP members. This flexibility provides a good degree of fault tolerance at runtime. See the <<cp-subsystem-management, CP Subsystem Management section>> for more details.

The CP subsystem runs 2 CP groups by default. The first one is the _Metadata_ group. It is an internal CP group which is responsible for maintaining the CP members and CP groups. It is initialized during the cluster startup process if the CP subsystem is enabled via `CPSubsystemConfig.setCPMemberCount(int)` configuration. The second group is the _DEFAULT_ CP group, whose name is given in `CPGroup.DEFAULT_GROUP_NAME`. If a group name is not specified while creating a proxy for a CP data structure, that data structure is mapped to the _DEFAULT_ CP group. For instance, when a CP `IAtomicLong` instance is created by calling `CPSubsystem.getAtomicLong("myAtomicLong")`, it will be initialized on the `DEFAULT` CP group. Besides these **2** predefined CP groups, custom CP groups can be created at runtime. If a CP `IAtomicLong` is created by calling `CPSubsystem.getAtomicLong("myAtomicLong@myGroup")`, first a new CP group is created with the name `myGroup` and then `myAtomicLong` is initialized on this custom CP group.

The current set of CP data structures have quite low memory overheads. Moreover, related to the Raft consensus algorithm, each CP group makes use of internal heartbeat RPCs to maintain the authority of the leader member and help lagging the CP members to make progress. Last but not least, the new CP Lock and Semaphore implementations rely on a brand new session mechanism. In a nutshell, a Hazelcast member or client starts a new session on the corresponding CP group when it makes its very first Lock or Semaphore acquire request, and then periodically commits session heartbeats to this CP group to indicate its liveliness. It means that if the CP Locks and Semaphores are distributed into multiple CP groups, there will be a session management overhead. See the <<cp-sessions, CP Sessions section>> for more details. For the aforementioned reasons, we recommend you to use a minimal number of CP groups. For most use cases, the `DEFAULT` CP group should be sufficient to maintain all the CP data structure instances. The custom CP groups could be created when the throughput of CP subsystem is needed to be improved.

**API Code Sample:**

[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=apisample]
----

WARNING: The CP data structure proxies differ from the other data structure proxies in one aspect: if you call the `DistributedObject.destroy()` method on a CP data structure proxy, that data structure is terminated on the underlying CP and cannot be reinitialized on the same CP group. For this reason, please make sure that you are completely done with a CP data structure before destroying its proxy.

=== CP Sessions

For CP data structures which are performing ownership management of the resources, such as Lock or Semaphore, a session is required to keep track the liveliness of the caller. In this context, the caller means an entity that uses the CP subsystem APIs. It can be either a Hazelcast member or a client. A caller initially creates a session before sending its very first session based request to the CP group, such as the Lock/Semaphore acquire. After creating a session on the CP group, the caller stores its session ID locally and sends it alongside its session based operations. A single session is used by multiple Lock and Semaphore instances. When a CP group receives a session based operation, it checks the validity of the session using the session ID information available in the operation. An operation with a valid session ID is accepted as a session heartbeat. While a caller is idle, in other words, it does not send any session based operation to the CP group for a while, it commits periodic heartbeats to the CP group in the background in order to keep its session alive. This interval is specified in `CPSubsystemConfig.getSessionHeartbeatIntervalSeconds()`.

A session is closed when the caller does not touch the session during a predefined duration. In this case, the caller is assumed to be crashed and all its resources are released automatically. This duration is specified in `CPSubsystemConfig.getSessionTimeToLiveSeconds()`. See the <<cpsubsystem-configuration, CP Subsystem Configuration section>> to learn the recommendations to choose a reasonable session time-to-live duration.

Sessions offer a trade-off between liveliness and safety. If you set a very small value using `CPSubsystemConfig.setSessionTimeToLiveSeconds(int)`, then a session owner could be considered crashed very quickly and its resources can be released prematurely. On the other hand, if you set a large value, a session could be kept alive for an unnecessarily long duration even if its owner actually crashes.

See the <<cpsubsystem-configuration, CP Subsystem Configuration section>> for more details.

=== FencedLock

`FencedLock` is a linearizable & distributed & reentrant implementation of `j.u.c.locks.Lock`. `FencedLock` is accessed via `CPSubsystem.getLock(String)`. It is CP with respect to the CAP principle. It works on top of the Raft consensus algorithm. It offers linearizability during crash-stop failures and network partitions. If a network partition occurs, it remains available on at most one side of the partition.
`FencedLock` works on top of CP sessions. Please see <<cp-sessions, CP Sessions>> section for more information about CP sessions.

By default, `FencedLock` is reentrant. Once a caller acquires the lock, it can acquire the lock reentrantly as many times as it wants in a linearizable manner. Reentrancy behaviour can be configured via `FencedLockConfig`. For instance, reentrancy can be disabled and `FencedLock` can work as a non-reentrant mutex. One can also set a custom reentrancy limit. When reentrancy limit is exceeded, `FencedLock` does not block a lock call. Instead, it fails with `LockAcquireLimitExceededException` or a specified return value. Please check the locking methods to see details about the behavior and <<fencedlock-configuration, FencedLock Configuration>> section for the configuration.

Distributed locks are unfortunately *not equivalent* to single-node mutexes because of the complexities in distributed systems, such as uncertain communication patterns and independent process failures. In an asynchronous network, no lock service can guarantee mutual exclusion, because there is no way to distinguish between a slow and a crashed process. Consider the following scenario, where a Hazelcast client acquires a `FencedLock`, then hits a long GC pause. Since it will not be able to commit session heartbeats while paused, its CP session will be eventually closed. After this moment, another Hazelcast client can acquire this lock. If the first client wakes up again, it may not immediately notice that it has lost ownership of the lock. In this case, multiple clients think they
hold the lock. If they attempt to perform an operation on a shared resource, they can break the system. To prevent such situations, one may choose to use an infinite session timeout, but this time probably she is going to deal with liveliness issues. Even if the first client crashes, requests sent by 2 clients can be re-ordered in the network and hit the external resource in reverse order.

There is a simple solution for this problem. Lock holders are ordered by a monotonic fencing token, which increments each time the lock is assigned to a new owner. This fencing token can be passed to external services or resources to ensure sequential execution of side effects performed by lock holders.

The following figure illustrates the idea. In this figure, Client-1 acquires the lock first and receives 1 as its fencing token. Then, it passes this token to the external service, which is our shared resource in this scenario. Just after that, Client-1 hits a long GC pause and eventually loses ownership of the lock because it misses to commit CP session heartbeats. Then, Client-2 chimes in and acquires the lock. Similar to Client-1, Client-2 passes its fencing token to the external service. After that, once Client-1 comes back alive, its write request will be rejected by the external service, and only Client-2 will be able to safely talk it.

image::FencedLock.png[Fenced Lock]

You can read more about the fencing token idea in Martin Kleppmann's https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html[How to do distributed locking] blog post and Google's https://ai.google/research/pubs/pub27897[Chubby paper]. `FencedLock` integrates this idea with the good old `j.u.c.locks.Lock` abstraction, exlucuding `j.u.c.locks.Condition`. `newCondition()` is not implemented and will throw `UnsupportedOperationException`.

All of the API methods in the new `FencedLock` abstraction offer the exactly-once execution semantics. For instance, even if a `lock()` call is internally retried because of a crashed CP member, the lock is acquired only once. The same rule also applies to the other methods in the API.


=== Configuration

==== CPSubsystem Configuration

- `cp-member-count`: Number of `CPMember` s to initialize the `CPSubsystem`. It is `0` by default and the CP subsystem is disabled. The CP subsystem is enabled when a positive value is set. After the CP subsystem is initialized successfully, more CP members can be added at run-time and number of active CP members can go beyond the configured CP member count. Number of CP members can be smaller than total size of the Hazelcast cluster. For instance, one can run 5 CP members in a 20-member Hazelcast cluster.
+
If set, must be greater than or equal to `group-size`.

- `group-size`: Number of CP members for running CP groups. If set, it must be an odd number between `3` and `7`. Otherwise, `cp-member-count` is respected.
+
If set, must be smaller than or equal to `cp-member-count`.

- `session-time-to-live-seconds`: Duration for a CP session to be kept alive after the last heartbeat it has received. The session will be closed if there is no new heartbeat during this duration. Session TTL must be decided wisely. If a very low value is set, CP session of a Hazelcast instance can be closed prematurely if the instance temporarily loses connectivity to the CP subsystem because of a network partition or a GC pause. In such an occasion, all CP resources of this Hazelcast instance, such as `FencedLock` or `ISemaphore`, are released. On the other hand, if a very large value is set, CP resources can remain assigned to an actually crashed Hazelcast instance for too long and liveliness problems can occur. The CP subsystem offers an API `CPSessionManagementService`, to deal with liveliness issues related to CP sessions. In order to prevent premature session expires, session TTL configuration can be set a relatively large value and `CPSessionManagementService.forceCloseSession(String, long)` can be manually called to close CP session of a crashed Hazelcast instance.
+
Must be greater than `session-heartbeat-interval-seconds`, and smaller than or equal to `missing-cp-member-auto-removal-seconds`. Default value is `300` seconds.

- `session-heartbeat-interval-seconds`: Interval for the periodically-committed CP session heartbeats. A CP session is started on a CP group with the first session-based request of a Hazelcast instance. After that moment, heartbeats are periodically committed to the CP group.
+
Must be smaller than `sessionTimeToLiveSeconds`. Default value is `5` seconds.

- `missing-cp-member-auto-removal-seconds`: Duration to wait before automatically removing a missing CP member from the CP subsystem. When a CP member leaves the cluster, it is not automatically removed from the CP subsystem, since it could be still alive and left the cluster because of a network partition. On the other hand, if a missing CP member is actually crashed, it creates a danger for its CP groups, because it will be still part of majority calculations. This situation could lead to losing majority of CP groups if multiple CP members leave the cluster over time.
+
With the default configuration, missing CP members will be automatically removed from the CP subsystem after `4` hours. This feature is very useful in terms of fault tolerance when CP member count is also configured to be larger than group size. In this case, a missing CP member will be safely replaced in its CP groups with other available CP membersin the CP subsystem. This configuration also implies that no network partition is expected to be longer than the configured duration.
+
Must be greater than or equal to `session-time-to-live-seconds`. Default value is `4` hours.

- `fail-on-indeterminate-operation-state`: Offers a choice between at-least-once and at-most-once execution of the operations on top of the Raft consensus algorithm. It is disabled by default and offers at-least-once execution guarantee. If enabled, it switches to at-most-once execution guarantee. When you invoke an API method on a CP data structure proxy, it replicates an internal operation to the corresponding CP group. After this operation is committed to majority of this CP group by the Raft leader node, it sends a response for the public API call. If a failure causes loss of the response, then the calling side cannot determine if the operation is committed on the CP group or not. In this case, if this configuration is disabled, the operation is replicated again to the CP group, and hence could be committed multiple times. If it is enabled, the public API call fails with `IndeterminateOperationStateException`.
+
Default value is `false`.

**Declarative Configuration**:

[source,xml]
----
<hazelcast>
    ...
    <cp-subsystem>
        <cp-member-count>7</cp-member-count>
        <group-size>3</group-size>
        <session-time-to-live-seconds>300</session-time-to-live-seconds>
        <session-heartbeat-interval-seconds>5</session-heartbeat-interval-seconds>
        <missing-cp-member-auto-removal-seconds>14400</missing-cp-member-auto-removal-seconds>
        <fail-on-indeterminate-operation-state>false</fail-on-indeterminate-operation-state>
    </cp-subsystem>
    ...
</hazelcast>
----

**Programmatic Configuration**:

[source,java]
----
include::{javasource}/cp/CpSubsystemConfiguration.java[tag=cpconf]
----

==== FencedLock Configuration

- `name`: Name of the FencedLock
- `lock-acquire-limit`: Maximum number of reentrant lock acquires. Once a caller acquires the lock this many times, it will not be able to acquire the lock again, until it makes at least one `unlock()` call.
+
By default, no upper bound is set for the number of reentrant lock acquires, which means that once a caller acquires a `FencedLock`, all of its further `lock()` calls will succeed. However, for instance, if you set `lock-acquire-limit` to `2`, once a caller acquires the lock, it will be able to acquire it once more, but its third `lock()` call will not succeed.
+
If `lock-acquire-limit` is set to 1, then the lock becomes non-reentrant.
+
`0` means there is no upper bound for number of reentrant lock acquires. Default value is `0`.

**Declarative Configuration**:

[source,xml]
----
<hazelcast>
    ...
    <cp-subsystem>
        ...
        <locks>
            <fenced-lock>
                <name>reentrant-lock</name>
                <lock-acquire-limit>0</lock-acquire-limit>
            </fenced-lock>
            <fenced-lock>
                <name>limited-reentrant-lock</name>
                <lock-acquire-limit>10</lock-acquire-limit>
            </fenced-lock>
            <fenced-lock>
                <name>non-reentrant-lock</name>
                <lock-acquire-limit>1</lock-acquire-limit>
            </fenced-lock>
        </locks>
    </cp-subsystem>
    ...
</hazelcast>
----

**Programmatic Configuration**:

[source,java]
----
include::{javasource}/cp/CpSubsystemConfiguration.java[tag=cplockconf]
----

==== Semaphore Configuration

- `name`: Name of the CP semaphore
- `jdk-compatible`: Enables / disables JDK compatibility of the CP `ISemaphore`. When it is JDK compatible, just as in the `j.u.c.Semaphore.release()` method, a permit can be released without acquiring it first, because acquired permits are not bound to threads. However, there is no auto-cleanup of acquired permits upon Hazelcast server/client failures. If a permit holder fails, its permits must be released manually. When JDK compatibility is disabled, a `HazelcastInstance` must acquire permits before releasing them and it cannot release a permit that it has not acquired. It means, you can acquire a permit from one thread and release it from another thread using the same `HazelcastInstance`, but not different instances of `HazelcastInstance`. In this mode, acquired permits are automatically released upon failure of the holder `HazelcastInstance`. So there is a minor behavioral difference to the `j.u.c.Semaphore.release()` method.
+
JDK compatibility is disabled by default.

**Declarative Configuration**:

[source,xml]
----
<hazelcast>
    ...
    <cp-subsystem>
        ...
        <semaphores>
            <cp-semaphore>
                <name>jdk-compatible-semaphore</name>
                <jdk-compatible>true</jdk-compatible>
            </cp-semaphore>
            <cp-semaphore>
                <name>another-semaphore</name>
                <jdk-compatible>false</jdk-compatible>
            </cp-semaphore>
        </semaphores>
    </cp-subsystem>
    ...
</hazelcast>
----

**Programmatic Configuration**:

[source,java]
----
include::{javasource}/cp/CpSubsystemConfiguration.java[tag=cpsemaconf]
----

==== Raft Algorithm Configuration

WARNING: These parameters are to tune specific parameters of Hazelcast's Raft consensus algorithm implementation and only for power users.

- `leader-election-timeout-in-millis`: Leader election timeout in milliseconds. If a candidate cannot win majority of the votes in time, a new election round is initiated. Default value is `2000`.
- `leader-heartbeat-period-in-millis`: Period for leader to send heartbeat messages to its followers. Default value is `5000`.
- `max-missed-leader-heartbeat-count`: Max number of missed leader heartbeats to trigger a new leader election. Default value is `5`.
- `append-request-max-entry-count`: Max entry count that can be sent in a single batch of append entries request. Default value is `50`.
- `commit-index-advance-count-to-snapshot`: Number of commits to initiate a new snapshot after the last snapshot's index. Default value is `1000`.
- `uncommitted-entry-count-to-reject-new-appends`: Max number of uncommitted entries in the leader's Raft log before temporarily rejecting new requests of callers. Default value is `100`.
- `append-request-backoff-timeout-in-millis`: Timeout for append request backoff in millis. After the leader sends an append request to a follower, it will not send a subsequent append request until the responds to the former request or this timeout occurs. Default value is `100`.

**Declarative Configuration**:

[source,xml]
----
<hazelcast>
    ...
    <cp-subsystem>
        ...
        <raft-algorithm>
            <leader-election-timeout-in-millis>2000</leader-election-timeout-in-millis>
            <leader-heartbeat-period-in-millis>5000</leader-heartbeat-period-in-millis>
            <max-missed-leader-heartbeat-count>5</max-missed-leader-heartbeat-count>
            <append-request-max-entry-count>50</append-request-max-entry-count>
            <commit-index-advance-count-to-snapshot>1000</commit-index-advance-count-to-snapshot>
            <uncommitted-entry-count-to-reject-new-appends>200</uncommitted-entry-count-to-reject-new-appends>
            <append-request-backoff-timeout-in-millis>250</append-request-backoff-timeout-in-millis>
        </raft-algorithm>
        ...
    </cp-subsystem>
    ...
</hazelcast>
----

**Programmatic Configuration**:

[source,java]
----
include::{javasource}/cp/CpSubsystemConfiguration.java[tag=cpraftconf]
----


=== CP Subsystem Management

Unlike dynamic nature of Hazelcast clusters, the CP subsystem requires manual intervention while expanding / shrinking its size, or when a CP member crashes or becomes unreachable. When a CP member becomes unreachable, it cannot be automatically removed from the CP subsystem because it could be still alive and partitioned away.

Moreover, the current CP subsystem implementation works only in memory without persisting any state to disk. It means that a crashed CP member will not be able to recover by reloading its previous state. Therefore, crashed CP members create a danger for gradually losing majority of CP groups and eventually total loss of availability of the CP subsystem. To prevent such kind of situations, `CPSubsystemManagementService` offers APIs for dynamic management of CP members.

The CP subsystem relies on Hazelcast's failure detectors to test reachability of CP members. Before removing a CP member from the CP subsystem, please make sure that it is declared as unreachable by Hazelcast's failure detector and removed from Hazelcast's member list.

CP member additions and removals are internally handled by performing a single membership change at a time. When multiple CP members are shutting down concurrently, their shutdown process is executed serially. First, the Metadata CP group creates a membership change plan for CP groups. Then, scheduled changes are applied to CP groups one by one. After all removals are done, the shutting down CP member is removed the active CP members list and its shutdown process is completed.

When a CP member is being shut down, it is replaced with another available CP member in all of its CP groups, including the Metadata group, in order to not to decrease or more importantly not to lose majority of CP groups. If there is no available CP member to replace a shutting down CP member in a CP group, that group's size will be reduced by 1 and its majority value will be recalculated.

A new CP member can be added to the CP subsystem to either increase number of available CP members for new CP groups or to fulfill missing slots in existing CP groups. After the initial Hazelcast cluster startup is done, an existing Hazelcast member can be be promoted to the CP member role. This new CP member will automatically join to CP groups that has missing members, and majority value of these CP groups will be recalculated.

A CP member may crash due to hardware problems or a defect in user code, or it may become unreachable because of connection problems, such as network partitions, network hardware failures, etc. If a CP member is known to be alive but only has temporary communication issues, it will catch up the other CP members and continue to operate normally after its communication issues are resolved. If it is known to be crashed or communication issues cannot be resolved in a short time, it can be preferable to remove this CP member from the CP subsystem, hence from all its CP groups. In this case, the unreachable CP member should be terminated to prevent any accidental communication with the rest of the CP subsystem.

When majority of a CP group is lost for any reason, that CP group cannot make progress anymore. Even a new CP member cannot join to this CP group, because all membership changes go through the Raft consensus algorithm. For this reason, the only option is to force-destroy the CP group via the `CPSubsystemManagementService.forceDestroyCPGroup()` API. When this API is used, the CP group is terminated non-gracefully, without the Raft algorithm mechanics. Then, all CP data structure proxies that talk to this CP group fail with `CPGroupDestroyedException`. However, if a new proxy is created afterwards, then this CP group will be re-created from scratch with a new set of CP members. Losing majority of a CP group can be likened to partition-loss scenario of AP Hazelcast.

Please note that CP groups that have lost their majority must be force-destroyed immediately, because they can block the Metadata CP group to perform membership changes.

Loss of majority of the Metadata CP group is the doomsday scenario for the CP subsystem. It is a fatal failure and the only solution is to reset the whole CP subsystem state via the `CPSubsystemManagementService.restart()` API. To be able to reset the CP subsystem, the initial size of the CP subsystem must be satisfied, which is defined by `CPSubsystemConfig.getCPMemberCount()`. For instance, `CPSubsystemConfig.getCPMemberCount()` is 5 and only 1 CP member is currently alive, when `CPSubsystemManagementService.restart()` is called, additional 4 regular Hazelcast members should exist in the cluster. New Hazelcast members can be started to satisfy `CPSubsystemConfig.getCPMemberCount()`.

==== CP Subsystem Management API

CP subsystem manamagent API can be accessed using Java API or REST interface. To communicate with REST interface there are two options; one is to access REST endpoint URL directly or using `cp-subsystem.sh` shell script, which comes with the Hazelcast package.

NOTE: The script `cp-cluster.sh` uses `curl` command and `curl` must be installed to be able to use the script.

* **Get Local CP Member:**
+
Returns the local CP member if this Hazelcast member is part of CP subsystem.

.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=localmember]
----

.REST API
[source,sh]
----
> curl http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/members/local
OR
> sh cp-subsystem.sh -o get-local-member --address 127.0.0.1 --port 5701

Sample Response:
{
    "uuid": "6428d7fd-6079-48b2-902c-bdf6a376051e",
    "address": "[127.0.0.1]:5701"
}
----

* **Get CP Groups:**
+
Returns the list of active CP groups.

.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=cpgroups]
----

.REST API
[source,sh]
----
> curl http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/groups
OR
> sh cp-subsystem.sh -o get-groups --address 127.0.0.1 --port 5701

Sample Response:
[{
    "name": "METADATA",
    "id": 0
}, {
    "name": "atomics",
    "id": 8
}, {
    "name": "locks",
    "id": 14
}]
----

* **Get a single CP Group:**
+
Returns the active CP group with the given name. There can be at most one active CP group with a given name.

.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=cpgroup]
----

.REST API
[source,sh]
----
> curl http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/groups/${CPGROUP_NAME}
OR
> sh cp-subsystem.sh -o get-group --group ${CPGROUP_NAME} --address 127.0.0.1 --port 5701

Sample Response:
{
    "id": {
        "name": "locks",
        "id": 14
    },
    "status": "ACTIVE",
    "members": [{
        "uuid": "33f84b0f-46ba-4a41-9e0a-29ee284c1c2a",
        "address": "[127.0.0.1]:5703"
    }, {
        "uuid": "59ca804c-312c-4cd6-95ff-906b2db13acb",
        "address": "[127.0.0.1]:5704"
    }, {
        "uuid": "777ff6ea-b8a3-478d-9642-47d1db019b37",
        "address": "[127.0.0.1]:5705"
    }, {
        "uuid": "c7856e0f-25d2-4717-9919-88fb3ecb3384",
        "address": "[127.0.0.1]:5702"
    }, {
        "uuid": "c6229b44-8976-4602-bb57-d13cf743ccef",
        "address": "[127.0.0.1]:5701"
    }]
}
----

* **Get CP Members:**
+
Returns the list of active CP members in the cluster.

.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=cpmembers]
----

.REST API
[source,sh]
----
> curl http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/members
OR
> sh cp-subsystem.sh -o get-members --address 127.0.0.1 --port 5701

Sample Response:
[{
    "uuid": "33f84b0f-46ba-4a41-9e0a-29ee284c1c2a",
    "address": "[127.0.0.1]:5703"
}, {
    "uuid": "59ca804c-312c-4cd6-95ff-906b2db13acb",
    "address": "[127.0.0.1]:5704"
}, {
    "uuid": "777ff6ea-b8a3-478d-9642-47d1db019b37",
    "address": "[127.0.0.1]:5705"
}, {
    "uuid": "c6229b44-8976-4602-bb57-d13cf743ccef",
    "address": "[127.0.0.1]:5701"
}, {
    "uuid": "c7856e0f-25d2-4717-9919-88fb3ecb3384",
    "address": "[127.0.0.1]:5702"
}]
----

* **Force Destroy a CP Group:**
+
Unconditionally destroys the given active CP group without using the Raft algorithm mechanics. This method must be used only when a CP group loses its majority and cannot make progress anymore. Normally, membership changes in CP groups, such as CP member promotion or removal, are done via the Raft consensus algorithm mechanics. However, when a CP group loses its majority, it will not be able to commit any new operation. Therefore, this method ungracefully terminates remaining members of the given CP group. It also performs a Raft commit to the Metadata CP group to update status of the destroyed group. Once a CP group id is destroyed, all CP data structure proxies created before the destroy will fail with `CPGroupDestroyedException`.
+
Once a CP group is destroyed, it can be created again with a new set of CP members. This method is idempotent. It has no effect if the given CP group is already destroyed.

.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=destroygroup]
----

.REST API
[source,sh]
----
> curl -X POST --data "${GROUPNAME}&${PASSWORD}" http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/groups/${CPGROUP_NAME}/remove
OR
> sh cp-subsystem.sh -o force-destroy-group --group ${CPGROUP_NAME} --address 127.0.0.1 --port 5701 --groupname ${GROUPNAME} --password ${PASSWORD}
----

* **Remove a CP Member:**
+
Removes the given unreachable CP member from the active CP members list and all CP groups it belongs to. If any other active CP member is available, it will replace the removed CP member in its CP groups. Otherwise, CP groups which the removed CP member is a member of will shrink and their majority values will be recalculated.
+
WARNING: Before removing a CP member from the CP subsystem, please make sure that it is declared as unreachable by Hazelcast's failure detector and removed from Hazelcast's member list. The behaviour is undefined when a running CP member is removed from the CP subsystem.

.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=removemember]
----

.REST API
[source,sh]
----
> curl -X POST --data "${GROUPNAME}&${PASSWORD}" http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/members/${CPMEMBER_UUID}/remove
OR
> sh cp-subsystem.sh -o remove-member --member ${CPMEMBER_UUID} --address 127.0.0.1 --port 5701 --groupname ${GROUPNAME} --password ${PASSWORD}
----

* **Promote Local Member to a CP Member**
+
Promotes the local Hazelcast member to a CP member. If the local member is already in the active CP members list, then this method will have no effect. When the current member is promoted to CP member, its member UUID is assigned as CP member UUID. The promoted CP member will be added to the CP groups that have missing members, i.e., whose size is smaller than `CPSubsystemConfig.getGroupSize()`.

.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=promotemember]
----

.REST API
[source,sh]
----
> curl -X POST --data "${GROUPNAME}&${PASSWORD}" http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/members
OR
> sh cp-subsystem.sh -o promote-member --address 127.0.0.1 --port 5701 --groupname ${GROUPNAME} --password ${PASSWORD}
----

* **Wipe and Restart CP Subsystem**
+
Wipes & resets the whole CP subsystem and initializes it as if the Hazelcast cluster is starting up initially. This method must be used only when the Metadata CP group loses its majority and cannot make progress anymore.
+
After this method is called, all CP state and data will be wiped and CP members will start with empty state.
+
This method can be invoked only from the Hazelcast master member. Moreover, the Hazelcast cluster must have at least `CPSubsystemConfig.getCPMemberCount()` members.
+
This method must not be called while there are membership changes in the cluster. Before calling this method, please make sure that there is no new member joining and all existing Hazelcast members have seen the same member list.
+
WARNING: This method is **NOT** idempotent and multiple invocations can break the whole system! After calling this API, you must observe the system to see if the restart process is successfully completed or failed before making another call.

.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=restart]
----

.REST API
[source,sh]
----
> curl -X POST --data "${GROUPNAME}&${PASSWORD}" http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/restart
OR
> sh cp-subsystem.sh -o restart --address 127.0.0.1 --port 5701 --groupname ${GROUPNAME} --password ${PASSWORD}
----

==== Session Management API

There are two management API methods for session management.

* **Get CP Group Sessions:**
+
Returns all CP sessions that are currently active in a CP group.

.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=sessions]
----

.REST API
[source,sh]
----
> curl http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/groups/${CPGROUP_NAME}/sessions
OR
> sh cp-subsystem.sh -o get-sessions --group ${CPGROUP_NAME} --address 127.0.0.1 --port 5701

Sample Response:
[{
    "id": 1,
    "creationTime": 1549008095530,
    "expirationTime": 1549008766630,
    "version": 73,
    "endpoint": "[127.0.0.1]:5701",
    "endpointType": "SERVER",
    "endpointName": "hz-member-1"
}, {
    "id": 2,
    "creationTime": 1549008115419,
    "expirationTime": 1549008765425,
    "version": 71,
    "endpoint": "[127.0.0.1]:5702",
    "endpointType": "SERVER",
    "endpointName": "hz-member-2"
}]
----

* **Force Close a Session:**
+
If a Hazelcast instance that owns a CP session crashes, its CP session is not terminated immediately. Instead, the session will be closed after `CPSubsystemConfig.getSessionTimeToLiveSeconds()` passes. If it is known for sure that the session owner is not partitioned and definitely crashed, this method can be used for closing the session and releasing its resources immediately.

.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=closesession]
----

.REST API
[source,sh]
----
> curl -X POST --data "${GROUPNAME}&${PASSWORD}" http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/groups/${CPGROUP_NAME}/sessions/${CP_SESSION_ID}/remove
OR
> sh cp-subsystem.sh -o force-close-session --group ${CPGROUP_NAME} --session-id ${CP_SESSION_ID} --address 127.0.0.1 --port 5701 --groupname ${GROUPNAME} --password ${PASSWORD}
----
