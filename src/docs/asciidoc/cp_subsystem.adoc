
== CP Subsystem

The CP subsystem is a component of a Hazelcast cluster that builds a strongly consistent layer. It is accessed via `HazelcastInstance.getCPSubsystem()`. Its data structures are _CP_ with respect to the http://awoc.wolski.fi/dlib/big-data/Brewer_podc_keynote_2000.pdf[_CAP_ principle], i.e., they always maintain https://aphyr.com/posts/313-strong-consistency-models[linearizability] and prefer consistency over availability during network partitions.

Currently, the CP subsystem contains only the implementations of Hazelcast's concurrency APIs. These APIs do not maintain large states. For this reason, all members of a Hazelcast cluster do not take part in the CP subsystem. The number of members that take part in the CP subsystem is specified with `CPSubsystemConfig.setCPMemberCount(int)`. Let's suppose the number of CP members is configured as **C**. Then, when Hazelcast cluster starts, the first **C** members form the CP subsystem. These members are called the CP members and they can also contain data for the other regular Hazelcast data structures, such as `IMap`, `ISet`.

Data structures in the CP subsystem run in ``CPGroup``s. A _CP group_ consists of an odd number of ``CPMember``s between 3 and 7. Each CP group independently runs the https://raft.github.io/[Raft consensus algorithm]. Operations are committed and executed only after they are successfully replicated to the majority of the CP members in a CP group. For instance, in a CP group of 5 CP members, operations are committed when they are replicated to at least 3 CP members. The size of CP groups is specified via `CPSubsystemConfig.setGroupSize(int)` and each CP group contains the same number of CP members. See the <<cp-subsystem-configuration, CP Subsystem Configuration section>> for configuration details.

Please note that the size of CP groups does not have to be same with the CP member count. Namely, the number of CP members in the CP subsystem can be larger than the configured CP group size. In this case, CP groups are formed by selecting the CP members randomly. Also note that if CP Subsystem Persistence is not enabled, CP Subsystem works only in memory without persisting any state to disk. It means that a crashed CP member is not able to recover by restoring its previous state. Therefore, crashed CP members create a danger for gradually losing the majority of CP groups and eventually cause the total loss of availability of the CP subsystem. To prevent such situations, failed CP members can be removed from the CP subsystem and replaced in CP groups with other available CP members. This flexibility provides a good degree of fault tolerance at run-time. See the <<cp-subsystem-management, CP Subsystem Management section>> for more details. Moreover, CP Subsystem Persistence enables more robustness. When it is enabled, CP members persist their local state to stable storage and can restore their state after crashes. See the <<cp-subsystem-persistence, CP Subsystem Persistence section>> for more details.

The CP subsystem runs 2 CP groups by default. The first one is the _Metadata_ group. It is an internal CP group which is responsible for managing the CP members and CP groups. It is initialized during the cluster startup process if the CP subsystem is enabled via `CPSubsystemConfig.setCPMemberCount(int)` configuration. The second group is the _DEFAULT_ CP group, whose name is given in `CPGroup.DEFAULT_GROUP_NAME`. If a group name is not specified while creating a proxy for a CP data structure, that data structure is mapped to the _DEFAULT_ CP group. For instance, when a CP `IAtomicLong` instance is created by calling `CPSubsystem.getAtomicLong("myAtomicLong")`, it will be initialized on the `DEFAULT` CP group. Besides these **2** predefined CP groups, custom CP groups can be created at run-time. If a CP `IAtomicLong` is created by calling `CPSubsystem.getAtomicLong("myAtomicLong@myGroup")`, first a new CP group is created with the name `myGroup` and then `myAtomicLong` is initialized on this custom CP group.

The current set of CP data structures have quite low memory overheads. Moreover, related to the Raft consensus algorithm, each CP group makes use of internal heartbeat RPCs to maintain the authority of the leader member and help lagging CP members to make progress. Last but not least, the new CP Lock and Semaphore implementations rely on a brand new session mechanism. In a nutshell, a Hazelcast member or client starts a new session on the corresponding CP group when it makes its very first Lock or Semaphore acquire request, and then periodically commits session heartbeats to this CP group to indicate its liveliness. It means that if CP Locks and Semaphores are distributed into multiple CP groups, there will be a session management overhead. See the <<cp-sessions, CP Sessions section>> for more details. For the aforementioned reasons, we recommend you to use a minimal number of CP groups. For most use cases, the `DEFAULT` CP group should be sufficient to maintain all CP data structure instances. Custom CP groups could be created when the throughput of CP subsystem is needed to be improved.

**API Code Sample:**

[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=apisample]
----

[WARNING]
====
The CP data structure proxies differ from the other data structure proxies in two aspects:

* Each time you fetch a proxy via one of the methods in this interface, internally a commit is performed on the Metadata CP group. Hence, the callers should cache  the returned proxies. 
* If you call the `DistributedObject.destroy()` method on a CP data structure proxy, that data structure is terminated on the underlying CP group and cannot be reinitialized until the CP group is force-destroyed. For this reason, please make sure that you are completely done with a CP data structure before destroying its proxy.
====

=== CP Subsystem Discovery

The CP subsystem runs a discovery process in the background on cluster startup. When you enable it by setting a positive value to `CPSubsystemConfig.setCPMemberCount(int)`, say `N`, the first `N` members in the cluster member list initiate the discovery process. Other Hazelcast members skip this step. The CP subsystem discovery process runs out of the box on top of Hazelcast's cluster member list without requiring any custom configuration for different environments. It is completed when each one of the first `N` Hazelcast members initializes its local CP member list and commits it to the Metadata CP group. The Metadata CP group is initialized among those CP members as well. *A soon-to-be CP member terminates itself if any of the following conditions occur before the CP discovery process is completed:*

- Any Hazelcast member leaves the cluster,
- The local Hazelcast member commits a CP member list which is different from other members' committed CP member lists,
- The local Hazelcast member list fails to commit its discovered CP member list for any reason.

When the CP subsystem is restarted via `CPSubsystemManagementService.restart()`, the CP subsystem discovery process is triggered again. However, it does not terminate
Hazelcast members if the discovery fails for the aforementioned reasons, because Hazelcast members are likely to contain data for AP data structures and termination can cause data loss. Hence, you need to observe the cluster and check if the discovery process completes successfully on CP subsystem restart. See <<cp-subsystem-management-apis, CP Subsystem Management APIs section>> for more details.

You can use the `CPSubsystemManagementService.awaitUntilDiscoveryCompleted(timeout, timeUnit)` API to wait until the CP Subsystem discovery process is completed.

=== CP Sessions

For CP data structures which are performing ownership management of the resources, such as Lock or Semaphore, a session is required to keep track of the liveliness of the caller. In this context, the caller means an entity that uses the CP subsystem APIs. It can be either a Hazelcast member or a client. A caller initially creates a session before sending its very first session based request to the CP group, such as a Lock / Semaphore acquire. After creating a session on the CP group, the caller stores its session ID locally and sends it alongside its session based operations. A single session is used for all lock and semaphore proxies of the caller. When a CP group receives a session based operation, it checks the validity of the session using the session ID information available in the operation. A session is valid if it is still open in the CP group. An operation with a valid session ID is accepted as a new session heartbeat. While a caller is idle, in other words, it does not send any session based operation to the CP group for a while, it commits periodic heartbeats to the CP group in the background in order to keep its session alive. This interval is specified in `CPSubsystemConfig.getSessionHeartbeatIntervalSeconds()`.

A session is closed when the caller does not touch the session during a predefined duration. In this case, the caller is assumed to be crashed and all its resources are released automatically. This duration is specified in `CPSubsystemConfig.getSessionTimeToLiveSeconds()`. See the <<cp-subsystem-configuration, CP Subsystem Configuration section>> to learn the recommendations for choosing a reasonable session time-to-live duration.

Sessions offer a trade-off between liveliness and safety. If you set a very small value using `CPSubsystemConfig.setSessionTimeToLiveSeconds(int)`, then a session owner could be considered crashed very quickly and its resources can be released prematurely. On the other hand, if you set a large value, a session could be kept alive for an unnecessarily long duration even if its owner actually crashes.

See the <<cp-subsystem-configuration, CP Subsystem Configuration section>> for more details.

=== FencedLock

`FencedLock` is a linearizable & distributed & reentrant implementation of `j.u.c.locks.Lock`. `FencedLock` is accessed via `CPSubsystem.getLock(String)`. It is CP with respect to the CAP principle. It works on top of the Raft consensus algorithm. It offers linearizability during crash-stop failures and network partitions. If a network partition occurs, it remains available on at most one side of the partition.
`FencedLock` works on top of CP sessions. Please see <<cp-sessions, CP Sessions>> section for more information about CP sessions.

By default, `FencedLock` is reentrant. Once a caller acquires the lock, it can acquire the lock reentrantly as many times as it wants in a linearizable manner. You can configure the reentrancy behavior via `FencedLockConfig`. For instance, reentrancy can be disabled and `FencedLock` can work as a non-reentrant mutex. You can also set a custom reentrancy limit. When the reentrancy limit is already reached, `FencedLock` does not block a lock call. Instead, it fails with `LockAcquireLimitReachedException` or a specified return value. Please check the locking methods to see details about the behavior and <<fencedlock-configuration, FencedLock Configuration section>> for the configuration.

Distributed locks are unfortunately *not equivalent* to single-node mutexes because of the complexities in distributed systems, such as uncertain communication patterns, and independent and partial failures. In an asynchronous network, no lock service can guarantee mutual exclusion, because there is no way to distinguish between a slow and a crashed process. Consider the following scenario, where a Hazelcast client acquires a `FencedLock`, then hits a long GC pause. Since it will not be able to commit session heartbeats while paused, its CP session will be eventually closed. After this moment, another Hazelcast client can acquire this lock. If the first client wakes up again, it may not immediately notice that it has lost ownership of the lock. In this case, multiple clients think they hold the lock. If they attempt to perform an operation on a shared resource, they can break the system. To prevent such situations, you can choose to use an infinite session timeout, but this time probably you are going to deal with liveliness issues. For the scenario above, even if the first client actually crashes, the requests sent by two clients can be reordered in the network and hit the external resource in the reverse order.

There is a simple solution for this problem. Lock holders are ordered by a monotonic fencing token, which increments each time the lock is assigned to a new owner. This fencing token can be passed to external services or resources to ensure sequential execution of the side effects performed by lock holders.

The following diagram illustrates the idea. Client-1 acquires the lock first and receives `1` as its fencing token. Then, it passes this token to the external service, which is our shared resource in this scenario. Just after that, Client-1 hits a long GC pause and eventually loses ownership of the lock because it misses to commit CP session heartbeats. Then, Client-2 chimes in and acquires the lock. Similar to Client-1, Client-2 passes its fencing token to the external service. After that, once Client-1 comes back alive, its write request will be rejected by the external service, and only Client-2 will be able to safely talk to it.

image::FencedLock.png[Fenced Lock]

You can read more about the fencing token idea in Martin Kleppmann's https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html[How to do distributed locking] blog post and Google's https://ai.google/research/pubs/pub27897[Chubby paper]. `FencedLock` integrates this idea with the `j.u.c.locks.Lock` abstraction, excluding `j.u.c.locks.Condition`. `newCondition()` is not implemented and throws `UnsupportedOperationException`.

All of the API methods in the new `FencedLock` abstraction offer exactly-once execution semantics. For instance, even if a `lock()` call is internally retried because of a crashed CP member, the lock is acquired only once. The same rule also applies to the other methods in the API.

=== CP Subsystem Persistence

[navy]*Hazelcast IMDG Enterprise*

==== CP Subsystem Persistence Overview

Without persistence (the default mode of CP Subsystem), CP members lose their
local state on crashes since their state is kept only in memory. Because of
this, crashed CP members cannot be restored and need to be removed from CP
Subsystem via CP Subsystem Management APIs. Without persistence, CP Subsystem
Management APIs enable you to maintain availability of your CP groups as long
as you don't lose majority of CP members at once. Crashed CP members can be
removed and new CP members can be added to CP Subsystem to preserve
availability.

CP Subsystem Persistence enables CP members to be recovered from crash
scenarios. This capability significantly improves the overall reliability of CP
Subsystem. When it is enabled, CP members persist their local state to stable
storage. You can restart crashed CP members and they will restore their local
state and resume working as if they have never crashed. CP Subsystem
Persistence enables you to handle single or multiple CP member crashes, or even
whole cluster crashes and guarantee that committed operations are not lost
after recovery. As long as majority of CP members are available after recovery,
CP Subsystem remains operational.

You can check the code sample below to see how CP Subsystem Persistence works
in general. In this code sample, we configure CP Subsystem with 3 CP members
and also enable CP Subsystem Persistence. We start 3 Hazelcast members with
this configuration and update an `IAtomicLong` instance. Each member creates
a sub-directory for itself inside the default base CP Subsystem Persistence
directory and stores its local CP state there. Then, we terminate all of these
members as if they crash and restart 2 of them back. When we fetch the same
`IAtomicLong` instance from the restarted members and get its current value,
we see that it returns the update that we made before terminating the members.
Please note that we restore majority of CP members to keep CP Subsystem
available after restart.

[source,java]

----
include::{javasource}/cp/CpSubsystemPersistence.java[tag=cppersistence1]
----

Please see <<cp-subsystem-configuration, CP Subsystem Configuration section>>
for the configuration options of CP Subsystem Persistence.

When CP Subsystem Persistence is enabled, *all cluster members* create a
sub-directory under the base persistence directory. This means that AP
Hazelcast members, which are the ones not marked as CP members during the CP
Subsystem discovery process, create their persistence directories as well.
Those members persist only the information that they are not CP members. This
is done because when a Hazelcast member starts with CP Subsystem Persistence
enabled, it checks if there is a CP persistence directory that belongs to itself,
and if it finds one it skips the CP Subsystem discovery process and
initializes its CP member identity from the persisted information. If it was an
AP member before shutdown or crash, it will restore this information.
Otherwise, it could think that the CP Subsystem discovery process is not
executed and initiate it, which would break CP Subsystem.

WARNING: In light of this information, If you have both CP and AP members when
CP Subsystem Persistence is enabled, and if you want to perform a cluster-wide
restart, you need to ensure that AP members are also restarted with their
persistence directories.

==== CP Member Shutdown Behaviour w/ or w/o CP Subsystem Persistence

WARNING: Please read this part carefully to notice the behavioral difference
in the CP member shutdown process when CP Subsystem Persistence is
enabled and disabled.

There is a significant behavioral difference during CP member shutdown when CP
Subsystem Persistence is enabled and disabled. When disabled (the default
mode), during a CP member shutting down, it is replaced with another available
CP member in all of its CP groups, including the Metadata CP group, in order
not to decrease or more importantly not to lose the majority of CP groups. If
there is no available CP member to replace a shutting down CP member in a CP
group, that CP group's size is reduced by 1 and its majority value is
recalculated. CP member shutdown works this way because CP members keep their
local state only in memory when CP Subsystem Persistence is disabled and a
shut-down CP member cannot join back with its previous state, hence it is
better to remove it from its CP groups to not to hurt availability. However,
when CP Subsystem Persistence is enabled, a shutting down CP Member can come
back with its local CP state. Therefore, we don't remove shutting down CP
members from their CP groups when CP Subsystem Persistence is enabled. It is up
to the user to remove those CP members from CP Subsystem if they will not come
back.

In summary, CP member shutdown behaviour is as follows:

- When CP Subsystem Persistence is disabled (the default mode), shutting down
CP members are removed from CP groups and CP group majority values are
recalculated.
- When CP Subsystem Persistence is enabled, shutting down CP members are still
kept in CP groups so they will be part of CP group majority calculations.

==== CP Member Address Limitation

WARNING: Please read this part carefully to notice the temporary limitation
about CP member addresses and CP member restarts. This limitation will be
eliminated in the full release of the CP Subsystem Persistence feature.

As described in <<cp-subsystem-configuration, CP Subsystem Configuration section>>,
CP members persist their local state in sub-directories inside the base CP
Subsystem Persistence directory. When you start a Hazelcast member, it chooses
one of the sub-directories inside the base directory if they exist. By this
way, CP members restore their local state after crashes or shutdowns. If a CP
member crashes, you can move its data to another machine and restore that CP
member there. However, there is a temporary limitation to this capability.
Currently, CP Subsystem Persistence does not support address changes of CP
members. When you move a CP data directory to a new machine, you need to make
sure that the CP member restarts with the same address, because the restarting
member chooses only the CP data sub-directory that contains its address as part
of the persisted CP member identity.

==== CP Subsystem Persistence Behaviour During Wiping out CP Subsystem

If the majority of CP members are permanently lost, CP Subsystem becomes
unavailable. There is no solution to recover from this failure case with strong
consistency guarantee. CP Subsystem Management API contains a method to delete
all CP Subsystem state on the remaining CP members and start from scratch.
`CPSubsystemManagementService.restart()` wipes and resets the whole CP
Subsystem and initializes it as if the Hazelcast cluster is starting up
for the first time. This method deletes persisted CP member states as well.

==== Interaction with Hot Restart Persistence

Hazelcast offers another persistence capability which is called
<<hot-restart-persistence, Hot Restart Persistence>>. Hot Restart Persistence
is used for restarting a cluster with large AP data after a planned cluster
shutdown or a whole cluster-wide crash. Please note that CP Subsystem
Persistence and Hot Restart Persistence are separate features with different
behaviours and reliability guarantees. For instance, CP Subsystem Persistence
guarantees that committed operations will be restored and the linearizability
semantics of the CP Subsystem data structures will be preserved on restarts.
However, Hot Restart Persistence may lose some of the acknowledged updates on
AP data structures, based on how you configure the `fsync` behavior for your
persisted AP data structures. Moreover, if you store AP and CP data in a single
Hazelcast cluster and use both of the persistence features, Hazelcast member
restarts or cluster restarts can fail because of the Hot Restart Persistence
recovery semantics, even if the CP Subsystem Persistence recovery procedure is
successful.

==== CP Persistence Performance Considerations

Performance numbers will be provided in this section with the full release of
the CP Subsystem Persistence feature.

=== Configuration

==== CP Subsystem Configuration

* `cp-member-count`: Number of ``CPMember``s to initialize the `CPSubsystem`. It is `0` by default, meaning that the CP subsystem is disabled. The CP subsystem is enabled when a positive value is set. After the CP subsystem is initialized successfully, more CP members can be added at run-time and the number of active CP members can go beyond the configured CP member count. The number of CP members can be smaller than the total size of the Hazelcast cluster. For instance, you can run 5 CP members in a 20-member Hazelcast cluster.
+
If set, must be greater than or equal to `group-size`.
+
* `group-size`: Number of CP members to run CP groups. If set, it must be an odd number between `3` and `7`. Otherwise, `cp-member-count` is respected.
+
If set, must be smaller than or equal to `cpMemberCount`.
+
* `session-time-to-live-seconds`: Duration for a CP session to be kept alive after the last received heartbeat. The session will be closed if there is no new heartbeat during this duration. Session TTL must be decided wisely. If a very low value is set, CP session of a Hazelcast instance can be closed prematurely if the instance temporarily loses connectivity to the CP subsystem because of a network partition or a GC pause. In such an occasion, all CP resources of this Hazelcast instance, such as `FencedLock` or `ISemaphore`, are released. On the other hand, if a very large value is set, CP resources can remain assigned to an actually crashed Hazelcast instance for too long and liveliness problems can occur. The CP subsystem offers an API `CPSessionManagementService`, to deal with liveliness issues related to CP sessions. In order to prevent premature session expires, session TTL configuration can be set a relatively large value and `CPSessionManagementService.forceCloseSession(String, long)` can be manually called to close CP session of a crashed Hazelcast instance.
+
Must be greater than `session-heartbeat-interval-seconds`, and smaller than or equal to `missing-cp-member-auto-removal-seconds`. Default value is `300` seconds.
+
* `session-heartbeat-interval-seconds`: Interval for the periodically-committed CP session heartbeats. A CP session is started on a CP group with the first session based request of a Hazelcast instance. After that moment, heartbeats are periodically committed to the CP group.
+
Must be smaller than `session-time-to-live-seconds`. Default value is `5` seconds.
+
* `missing-cp-member-auto-removal-seconds`: Duration to wait before automatically removing a missing CP member from the CP subsystem. When a CP member leaves the cluster, it is not automatically removed from the CP subsystem, since it could be still alive and left the cluster because of a network partition. On the other hand, if a missing CP member is actually crashed, it creates a danger for its CP groups, because it will be still part of majority calculations. This situation could lead to losing majority of CP groups if multiple CP members leave the cluster over time.
+
With the default configuration, missing CP members will be automatically removed from the CP subsystem after `4` hours. This feature is very useful in terms of fault tolerance when CP member count is also configured to be larger than group size. In this case, a missing CP member will be safely replaced in its CP groups with other available CP members in the CP subsystem. This configuration also implies that no network partition is expected to be longer than the configured duration.
+
If a missing CP member comes back alive after it is automatically removed from the CP subsystem with this feature, that CP member must be terminated manually.
+
Must be greater than or equal to `session-time-to-live-seconds`. Default value is `14400` seconds (4 hours).
+
* `fail-on-indeterminate-operation-state`: Offers a choice between at-least-once and at-most-once execution of the operations on top of the Raft consensus algorithm. It is disabled by default and offers at-least-once execution guarantee. If enabled, it switches to at-most-once execution guarantee. When you invoke an API method on a CP data structure proxy, it replicates an internal operation to the corresponding CP group. After this operation is committed to majority of this CP group by the Raft leader node, it sends a response for the public API call. If a failure causes loss of the response, then the calling side cannot determine if the operation is committed on the CP group or not. In this case, if this configuration is disabled, the operation is replicated again to the CP group, and hence could be committed multiple times. If it is enabled, the public API call fails with `IndeterminateOperationStateException`.
+
Default value is `false`.
+
* `persistence-enabled`: Specifies whether persistence is globally enabled for CP groups created in CP Subsystem. Set this attribute to `true` if you want CP members to be able to recover their local CP state on crashes.
+
Default value is `false`.
* `base-dir`: Specifies the parent directory where CP data is stored. The default value for `base-dir` is `cp-data`. You can use the default value, or you can specify the value of another folder, but it is mandatory that `base-dir` element has a value. This directory is created automatically if it does not exist.
+
Default value is `cp-data`.
+
`base-dir` is used as the parent directory, and a unique directory is created inside `base-dir` for each CP member which uses the same `base-dir`. That means, `base-dir` is shared among multiple CP members safely. This is especially useful for cloud environments where CP members generally use a shared filesystem.

**Declarative Configuration:**

[source,xml]
----
<hazelcast>
    ...
    <cp-subsystem>
        <cp-member-count>7</cp-member-count>
        <group-size>3</group-size>
        <session-time-to-live-seconds>300</session-time-to-live-seconds>
        <session-heartbeat-interval-seconds>5</session-heartbeat-interval-seconds>
        <missing-cp-member-auto-removal-seconds>14400</missing-cp-member-auto-removal-seconds>
        <fail-on-indeterminate-operation-state>false</fail-on-indeterminate-operation-state>
        <persistence-enabled>true</persistence-enabled>
        <base-dir>/custom-cp-dir</base-dir>
    </cp-subsystem>
    ...
</hazelcast>
----

**Programmatic Configuration:**

[source,java]
----
include::{javasource}/cp/CpSubsystemConfiguration.java[tag=cpconf]
----

==== FencedLock Configuration

* `name`: Name of the `FencedLock`.
* `lock-acquire-limit`: Maximum number of reentrant lock acquires. Once a caller acquires the lock this many times, it will not be able to acquire the lock again, until it makes at least one `unlock()` call.
+
By default, no upper bound is set for the number of reentrant lock acquires, which means that once a caller acquires a `FencedLock`, all of its further `lock()` calls will succeed. However, for instance, if you set `lock-acquire-limit` to `2`, once a caller acquires the lock, it will be able to acquire it once more, but its third `lock()` call will not succeed.
+
If `lock-acquire-limit` is set to 1, then the lock becomes non-reentrant.
+
`0` means there is no upper bound for the number of reentrant lock acquires. Default value is `0`.

**Declarative Configuration:**

[source,xml]
----
<hazelcast>
    ...
    <cp-subsystem>
        ...
        <locks>
            <fenced-lock>
                <name>reentrant-lock</name>
                <lock-acquire-limit>0</lock-acquire-limit>
            </fenced-lock>
            <fenced-lock>
                <name>limited-reentrant-lock</name>
                <lock-acquire-limit>10</lock-acquire-limit>
            </fenced-lock>
            <fenced-lock>
                <name>non-reentrant-lock</name>
                <lock-acquire-limit>1</lock-acquire-limit>
            </fenced-lock>
        </locks>
    </cp-subsystem>
    ...
</hazelcast>
----

**Programmatic Configuration:**

[source,java]
----
include::{javasource}/cp/CpSubsystemConfiguration.java[tag=cplockconf]
----

==== Semaphore Configuration

* `name`: Name of the CP `ISemaphore`.
* `jdk-compatible`: Enables / disables JDK compatibility of the CP `ISemaphore`. When it is JDK compatible, just as in the `j.u.c.Semaphore.release()` method, a permit can be released without acquiring it first, because acquired permits are not bound to threads. However, there is no auto-cleanup of the acquired permits upon Hazelcast server / client failures. If a permit holder fails, its permits must be released manually. When JDK compatibility is disabled, a `HazelcastInstance` must acquire permits before releasing them and it cannot release a permit that it has not acquired. It means, you can acquire a permit from one thread and release it from another thread using the same `HazelcastInstance`, but not different `HazelcastInstance`s. In this mode, acquired permits are automatically released upon failure of the holder `HazelcastInstance`. So there is a minor behavioral difference to the `j.u.c.Semaphore.release()` method.
+
JDK compatibility is disabled by default.

**Declarative Configuration:**

[source,xml]
----
<hazelcast>
    ...
    <cp-subsystem>
        ...
        <semaphores>
            <cp-semaphore>
                <name>jdk-compatible-semaphore</name>
                <jdk-compatible>true</jdk-compatible>
            </cp-semaphore>
            <cp-semaphore>
                <name>another-semaphore</name>
                <jdk-compatible>false</jdk-compatible>
            </cp-semaphore>
        </semaphores>
    </cp-subsystem>
    ...
</hazelcast>
----

**Programmatic Configuration:**

[source,java]
----
include::{javasource}/cp/CpSubsystemConfiguration.java[tag=cpsemaconf]
----

==== Raft Algorithm Configuration

WARNING: These parameters tune specific parameters of Hazelcast's Raft consensus algorithm implementation and are only for power users.

* `leader-election-timeout-in-millis`: Leader election timeout in milliseconds. If a candidate cannot win the majority of votes in time, a new election round is initiated. Default value is `2000` milliseconds.
* `leader-heartbeat-period-in-millis`: Period in milliseconds for a leader to send heartbeat messages to its followers. Default value is `5000` milliseconds.
* `max-missed-leader-heartbeat-count`: Maximum number of missed leader heartbeats to trigger a new leader election. Default value is `5`.
* `append-request-max-entry-count`: Maximum entry count that can be sent in a single batch of append entries request. Default value is `100`.
* `commit-index-advance-count-to-snapshot`: Number of new commits to initiate a new snapshot after the last snapshot. Default value is `10000`.
* `uncommitted-entry-count-to-reject-new-appends`: Maximum number of uncommitted entries in the leader's Raft log before temporarily rejecting the new requests of callers. Default value is `100`.
* `append-request-backoff-timeout-in-millis`: Timeout in milliseconds for append request backoff. After the leader sends an append request to a follower, it will not send a subsequent append request until the follower responds to the former request or this timeout occurs. Default value is `100` milliseconds.

**Declarative Configuration:**

[source,xml]
----
<hazelcast>
    ...
    <cp-subsystem>
        ...
        <raft-algorithm>
            <leader-election-timeout-in-millis>2000</leader-election-timeout-in-millis>
            <leader-heartbeat-period-in-millis>5000</leader-heartbeat-period-in-millis>
            <max-missed-leader-heartbeat-count>5</max-missed-leader-heartbeat-count>
            <append-request-max-entry-count>100</append-request-max-entry-count>
            <commit-index-advance-count-to-snapshot>10000</commit-index-advance-count-to-snapshot>
            <uncommitted-entry-count-to-reject-new-appends>200</uncommitted-entry-count-to-reject-new-appends>
            <append-request-backoff-timeout-in-millis>250</append-request-backoff-timeout-in-millis>
        </raft-algorithm>
        ...
    </cp-subsystem>
    ...
</hazelcast>
----

**Programmatic Configuration:**

[source,java]
----
include::{javasource}/cp/CpSubsystemConfiguration.java[tag=cpraftconf]
----


=== CP Subsystem Management

Unlike the dynamic nature of Hazelcast clusters, the CP subsystem requires manual intervention while expanding/shrinking its size, or when a CP member crashes or becomes unreachable. When a CP member becomes unreachable, it cannot be automatically removed from the CP subsystem because it could be still alive and partitioned away.

Moreover, by default CP subsystem works in memory without persisting any state to disk. It means that a crashed CP member will not be able to recover by reloading its previous state. Therefore, crashed CP members create a danger for gradually losing the majority of CP groups and eventually total loss of the availability of the CP subsystem. To prevent such situations, `CPSubsystemManagementService` offers APIs for dynamic management of the CP members. In addition to that, CP Subsystem Persistence can be enabled to make CP members persist their local CP state to stable storage. Please see <<cp-subsystem-persistence, CP Subsystem Persistence section>> for more details.

The CP subsystem relies on Hazelcast's failure detectors to test the reachability of CP members. Before removing a CP member from the CP subsystem, please make sure that it is declared as unreachable by Hazelcast's failure detector and removed from the Hazelcast's member list.

CP member additions and removals are internally handled by performing a single membership change at a time. When multiple CP members are shutting down concurrently, their shutdown process is executed serially. When a CP membership change is triggered, the Metadata CP group creates a membership change plan for CP groups. Then, the scheduled changes are applied to the CP groups one by one. After all CP group member removals are done, the shutting down CP member is removed from the active CP members list and its shutdown process is completed. A shut-down CP member is automatically replaced with another available CP member in all of its CP groups, including the Metadata CP group, in order not to decrease or more importantly not to lose the majority of CP groups. If there is no available CP member to replace a shutting down CP member in a CP group, that group's size is reduced by 1 and its majority value is recalculated. Please note that this behaviour is when CP Subsystem Persistence is disabled. When CP Subsystem Persistence is enabled, shut-down CP members are not automatically removed from the active CP members list and they are still considered as part of CP groups and majority calculations, because they can come back by restoring their local CP state from stable storage. If you know that a shut-down CP member will not be restarted, you need to remove that member from the CP Subsystem via the `CPSubsystemManagementService.removeCPMember()` API.

A new CP member can be added to the CP subsystem to either increase the number of available CP members for new CP groups or to fulfill the missing slots in the existing CP groups. After the initial Hazelcast cluster startup is done, an existing Hazelcast member can be be promoted to the CP member role. This new CP member automatically joins to CP groups that have missing members, and the majority value of these CP groups is recalculated.

A CP member may crash due to hardware problems or a defect in user code, or it may become unreachable because of connection problems, such as network partitions, network hardware failures, etc. If a CP member is known to be alive but only has temporary communication issues, it will catch up the other CP members and continue to operate normally after its communication issues are resolved. If it is known to be crashed or communication issues cannot be resolved in a short time, it can be preferable to remove this CP member from the CP subsystem, hence from all its CP groups. In this case, the unreachable CP member should be terminated to prevent any accidental communication with the rest of the CP subsystem.

When the majority of a CP group is lost for any reason, that CP group cannot make progress anymore. Even a new CP member cannot join to this CP group, because membership changes also go through the Raft consensus algorithm. For this reason, the only option is to force-destroy the CP group via the `CPSubsystemManagementService.forceDestroyCPGroup()` API. When this API is used, the CP group is terminated non-gracefully, without the Raft algorithm mechanics. Then, all CP data structure proxies that talk to this CP group fail with `CPGroupDestroyedException`. However, if a new proxy is created afterwards, then this CP group will be recreated from the scratch with a new set of CP members. Losing the majority of a CP group can be likened to partition-loss scenario of AP Hazelcast.

Please note that the CP groups that have lost their majority must be force-destroyed immediately, because they can block the Metadata CP group to perform membership changes.

Loss of the majority of Metadata CP group is the doomsday scenario for the CP subsystem. It is a fatal failure and the only solution is to reset the whole CP subsystem state via the `CPSubsystemManagementService.restart()` API. To be able to reset the CP subsystem, the initial size of the CP subsystem must be satisfied, which is defined by `CPSubsystemConfig.getCPMemberCount()`. For instance, assuming that `CPSubsystemConfig.getCPMemberCount()` is 5 and only 1 CP member is currently alive, when `CPSubsystemManagementService.restart()` is called, additional 4 regular Hazelcast members should exist in the cluster. New Hazelcast members can be started to satisfy `CPSubsystemConfig.getCPMemberCount()`.

NOTE: There is a subtle point about graceful shutdown of CP members. If there are `N` CP members in the cluster, `HazelcastInstance.shutdown()` can be called on `N-2` CP members concurrently. Once these `N-2` CP members complete their shutdown, the remaining `2` CP members must be shut down serially. Even though the shutdown API is called concurrently on multiple members, the Metadata CP group handles shutdown requests serially. Therefore, it would be simpler to shut down CP members one by one, by calling `HazelcastInstance.shutdown()` on the next CP member once the current CP member completes its shutdown. The reason behind this limitation is, each shutdown request internally requires a Raft commit to the Metadata CP group. A CP member proceeds to shutdown after it receives a response of its commit to the Metadata CP group. To be able to perform a Raft commit, the Metadata CP group must have its majority available. When there are only `2` CP members left after graceful shutdowns, the majority of the Metadata CP group becomes `2`. If the last `2` CP members shut down concurrently, one of them is likely to perform its Raft commit faster than the other one and leave the cluster before the other CP member completes its Raft commit. In this case, the last CP member waits for a response of its commit attempt on the Metadata group, and times out eventually. This situation causes an unnecessary delay on shutdown process of the last CP member. On the other hand, when the last `2` CP members shut down serially, the `N-1`th member receives response of its commit after its shutdown request is committed also on the last CP member. Then, the last CP member checks its local data to notice that it is the last CP member alive, and proceeds its shutdown without attempting a Raft commit on the Metadata CP group.

==== CP Subsystem Management APIs

You can access the CP subsystem management APIs using the Java API or REST interface. To communicate with the REST interface there are two options; one is to access REST endpoint URL directly or using the `cp-subsystem.sh` shell script, which comes with the Hazelcast package.

NOTE: The `cp-cluster.sh` script uses `curl` command, and `curl` must be installed to be able to use the script.

* **Get Local CP Member:**
+
Returns the local CP member if this Hazelcast member is a part of the CP Subsystem.
+
.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=localmember]
----
+
.REST API
[source,sh]
----
> curl http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/members/local
OR
> sh cp-subsystem.sh -o get-local-member --address 127.0.0.1 --port 5701
+
Sample Response:
{
    "uuid": "6428d7fd-6079-48b2-902c-bdf6a376051e",
    "address": "[127.0.0.1]:5701"
}
----
+
* **Get CP Groups:**
+
Returns the list of active CP groups.
+
.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=cpgroups]
----
+
.REST API
[source,sh]
----
> curl http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/groups
OR
> sh cp-subsystem.sh -o get-groups --address 127.0.0.1 --port 5701
+
Sample Response:
[{
    "name": "METADATA",
    "id": 0
}, {
    "name": "atomics",
    "id": 8
}, {
    "name": "locks",
    "id": 14
}]
----
+
* **Get a single CP Group:**
+
Returns the active CP group with the given name. There can be at most one active CP group with a given name.
+
.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=cpgroup]
----
+
.REST API
[source,sh]
----
> curl http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/groups/${CPGROUP_NAME}
OR
> sh cp-subsystem.sh -o get-group --group ${CPGROUP_NAME} --address 127.0.0.1 --port 5701
+
Sample Response:
{
    "id": {
        "name": "locks",
        "id": 14
    },
    "status": "ACTIVE",
    "members": [{
        "uuid": "33f84b0f-46ba-4a41-9e0a-29ee284c1c2a",
        "address": "[127.0.0.1]:5703"
    }, {
        "uuid": "59ca804c-312c-4cd6-95ff-906b2db13acb",
        "address": "[127.0.0.1]:5704"
    }, {
        "uuid": "777ff6ea-b8a3-478d-9642-47d1db019b37",
        "address": "[127.0.0.1]:5705"
    }, {
        "uuid": "c7856e0f-25d2-4717-9919-88fb3ecb3384",
        "address": "[127.0.0.1]:5702"
    }, {
        "uuid": "c6229b44-8976-4602-bb57-d13cf743ccef",
        "address": "[127.0.0.1]:5701"
    }]
}
----
+
* **Get CP Members:**
+
Returns the list of active CP members in the cluster.
+
.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=cpmembers]
----
+
.REST API
[source,sh]
----
> curl http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/members
OR
> sh cp-subsystem.sh -o get-members --address 127.0.0.1 --port 5701
+
Sample Response:
[{
    "uuid": "33f84b0f-46ba-4a41-9e0a-29ee284c1c2a",
    "address": "[127.0.0.1]:5703"
}, {
    "uuid": "59ca804c-312c-4cd6-95ff-906b2db13acb",
    "address": "[127.0.0.1]:5704"
}, {
    "uuid": "777ff6ea-b8a3-478d-9642-47d1db019b37",
    "address": "[127.0.0.1]:5705"
}, {
    "uuid": "c6229b44-8976-4602-bb57-d13cf743ccef",
    "address": "[127.0.0.1]:5701"
}, {
    "uuid": "c7856e0f-25d2-4717-9919-88fb3ecb3384",
    "address": "[127.0.0.1]:5702"
}]
----
+
* **Force Destroy a CP Group:**
+
Unconditionally destroys the given active CP group without using the Raft algorithm mechanics. This method must be used only when a CP group loses its majority and cannot make progress anymore. Normally, membership changes in CP groups, such as CP member promotion or removal, are done via the Raft consensus algorithm  . However, when a CP group loses its majority, it will not be able to commit any new operation. Therefore, this method ungracefully terminates the remaining members of the given CP group. It also performs a Raft commit to the Metadata CP group in order    to update the status of the destroyed group. Once a CP group ID is destroyed, all CP data structure proxies created before the destroy fails with `CPGroupDestroyedException`.
+
Once a CP group is destroyed, it can be created again with a new set of CP members. This method is idempotent. It has no effect if the given CP group is already destroyed.
+
.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=destroygroup]
----
+
.REST API
[source,sh]
----
> curl -X POST --data "${GROUPNAME}&${PASSWORD}" http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/groups/${CPGROUP_NAME}/remove
OR
> sh cp-subsystem.sh -o force-destroy-group --group ${CPGROUP_NAME} --address 127.0.0.1 --port 5701 --groupname ${GROUPNAME} --password ${PASSWORD}
----
+
* **Remove a CP Member:**
+
Removes the given unreachable CP member from the active CP members list and all CP groups it belongs to. If any other active CP member is available, it will replace the removed CP member in its CP groups. Otherwise, CP groups which the removed CP member is a member of will shrink and their majority values will be recalculated.
+
WARNING: Before removing a CP member from the CP subsystem, please make sure that it is declared as unreachable by Hazelcast's failure detector and removed from Hazelcast's member list. The behavior is undefined when a running CP member is removed from the CP subsystem.
+
.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=removemember]
----
+
.REST API
[source,sh]
----
> curl -X POST --data "${GROUPNAME}&${PASSWORD}" http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/members/${CPMEMBER_UUID}/remove
OR
> sh cp-subsystem.sh -o remove-member --member ${CPMEMBER_UUID} --address 127.0.0.1 --port 5701 --groupname ${GROUPNAME} --password ${PASSWORD}
----
+
* **Promote Local Member to a CP Member**
+
Promotes the local Hazelcast member to a CP member. If the local member is already in the active CP members list, then this method has no effect. When the current member is promoted to a CP member, its member UUID is assigned as CP member UUID. The promoted CP member will be added to the CP groups that have missing members, i.e., whose size is smaller than `CPSubsystemConfig.getGroupSize()`.
+
.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=promotemember]
----
+
.REST API
[source,sh]
----
> curl -X POST --data "${GROUPNAME}&${PASSWORD}" http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/members
OR
> sh cp-subsystem.sh -o promote-member --address 127.0.0.1 --port 5701 --groupname ${GROUPNAME} --password ${PASSWORD}
----
+
* **Wipe and Restart CP Subsystem**
+
Wipes and resets the whole CP subsystem and initializes it as if the Hazelcast cluster is starting up initially. This method must be used only when the Metadata CP group loses its majority and cannot make progress anymore.
+
After this method is called, all CP state and data are wiped and the CP members start with empty state.
+
This method can be invoked only from the Hazelcast master member. Moreover, the Hazelcast cluster must have at least `CPSubsystemConfig.getCPMemberCount()` members.
+
This method must not be called while there are membership changes in the cluster. Before calling this method, please make sure that there is no new member joining and all existing Hazelcast members have seen the same member list.
+
WARNING: This method is **NOT** idempotent and multiple invocations can break the whole system! After calling this API, you must observe the system to see if the restart process is successfully completed or failed before making another call.
+
WARNING: This method deletes all CP data written by CP Subsystem Persistence.
+
.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=restart]
----
+
.REST API
[source,sh]
----
> curl -X POST --data "${GROUPNAME}&${PASSWORD}" http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/restart
OR
> sh cp-subsystem.sh -o restart --address 127.0.0.1 --port 5701 --groupname ${GROUPNAME} --password ${PASSWORD}
----

==== Session Management API

There are two management API methods for session management.

* **Get CP Group Sessions:**
+
Returns all CP sessions that are currently active in a CP group.
+
.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=sessions]
----
+
.REST API
[source,sh]
----
> curl http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/groups/${CPGROUP_NAME}/sessions
OR
> sh cp-subsystem.sh -o get-sessions --group ${CPGROUP_NAME} --address 127.0.0.1 --port 5701
+
Sample Response:
[{
    "id": 1,
    "creationTime": 1549008095530,
    "expirationTime": 1549008766630,
    "version": 73,
    "endpoint": "[127.0.0.1]:5701",
    "endpointType": "SERVER",
    "endpointName": "hz-member-1"
}, {
    "id": 2,
    "creationTime": 1549008115419,
    "expirationTime": 1549008765425,
    "version": 71,
    "endpoint": "[127.0.0.1]:5702",
    "endpointType": "SERVER",
    "endpointName": "hz-member-2"
}]
----
+
* **Force Close a Session:**
+
If a Hazelcast instance that owns a CP session crashes, its CP session is not terminated immediately. Instead, the session is closed after `CPSubsystemConfig.getSessionTimeToLiveSeconds()` passes. If it is known for sure that the session owner is not partitioned and definitely crashed, this method can be used for closing the session and releasing its resources immediately.
+
.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=closesession]
----
+
.REST API
[source,sh]
----
> curl -X POST --data "${GROUPNAME}&${PASSWORD}" http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/groups/${CPGROUP_NAME}/sessions/${CP_SESSION_ID}/remove
OR
> sh cp-subsystem.sh -o force-close-session --group ${CPGROUP_NAME} --session-id ${CP_SESSION_ID} --address 127.0.0.1 --port 5701 --groupname ${GROUPNAME} --password ${PASSWORD}
----
