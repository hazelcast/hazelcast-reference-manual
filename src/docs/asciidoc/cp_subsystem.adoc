
== CP Subsystem

WARNING: **CP Subsystem operates in _the unsafe mode_ by default without
the strong consistency guarantee.** See the
<<cp-subsystem-unsafe-mode, CP Subsystem Unsafe Mode>> section for more
information. You should set a positive number to the CP member count
configuration to enable CP Subsystem and use it with the strong consistency
guarantee. See the <<cp-subsystem-configuration, CP Subsystem Configuration>>
section for details.

CP Subsystem is a component of a Hazelcast cluster that builds a strongly
consistent layer for a set of distributed data structures. Its APIs can be used
for implementing distributed coordination use cases, such as leader election,
distributed locking, synchronization, and metadata management. It is accessed
via `HazelcastInstance.getCPSubsystem()`. Its data structures are _CP_ with
respect to the http://awoc.wolski.fi/dlib/big-data/Brewer_podc_keynote_2000.pdf[_CAP_ principle],
i.e., they always maintain https://aphyr.com/posts/313-strong-consistency-models[linearizability]
and prefer consistency over availability during network partitions. Besides
network partitions, CP Subsystem withstands server and client failures.

Currently, CP Subsystem contains only the implementations of Hazelcast's
concurrency APIs. Since these APIs do not maintain large states, all members of
a Hazelcast cluster do not necessarily take part in CP Subsystem. The number of
Hazelcast members that take part in CP Subsystem is specified with
`CPSubsystemConfig.setCPMemberCount(int)`. Say that it is configured as `N`.
Then, when a Hazelcast cluster starts, the first `N` members form CP Subsystem.
These members are called _CP members_ and they can also contain data for
the other regular _AP_ Hazelcast data structures, such as `IMap`, `ISet`.

Data structures in CP Subsystem run in _CP groups_. Each CP group elects its
own Raft leader and runs the https://raft.github.io/[Raft consensus algorithm]
independently. CP Subsystem runs 2 CP groups by default:

* The first one is _the METADATA CP group_ which is an internal CP group
responsible for managing CP members and CP groups. It is initialized during
cluster startup if CP Subsystem is enabled via `CPSubsystemConfig.setCPMemberCount(int)`.
* The second CP group is _the DEFAULT CP group_, whose name is given in
`CPGroup.DEFAULT_GROUP_NAME`. If a group name is not specified while creating
a CP data structure proxy, that data structure is mapped to _the DEFAULT CP
group_. For instance, when a CP `IAtomicLong` instance is created via
`CPSubsystem.getAtomicLong("myAtomicLong")`, it is initialized on
_the DEFAULT CP group_.

Besides these 2 predefined CP groups, custom CP groups can be created at
run-time while fetching the CP data structure proxies. For instance, if a CP
`IAtomicLong` is created by calling `.getAtomicLong("myAtomicLong@myGroup")`,
first a new CP group is created with the name `myGroup` and then `myAtomicLong`
is initialized on this custom CP group.

This design implies that each CP member can participate to more than one CP
group. CP Subsystem runs a periodic background task to ensure that each CP
member performs the Raft leadership role for roughly equal number of CP groups.
For instance, if there are 3 CP members and 3 CP groups, each CP member becomes
Raft leader for only 1 CP group. If one more CP group is created, then one of
the CP members gets the Raft leader role for 2 CP groups. This is done because
Raft is a leader-based consensus algorithm. A Raft leader node becomes
responsible for handling incoming requests from callers and replicating them to
follower nodes. If a CP member gets the Raft leadership role for too many CP
groups compared to other CP members, it can turn into a bottleneck.

CP member count of CP groups are specified via
`CPSubsystemConfig.setGroupSize(int)`. Please note that this configuration does
not have to be the same with the CP member count. Namely, the number of CP members in
CP Subsystem can be larger than the configured CP group size. CP groups usually
consist of an odd number of CP members between 3 and 7. Operations are
committed and executed only after they are successfully replicated to
the majority of CP members in a CP group. An odd number of CP members is more
advantageous to an even number because of the quorum or majority calculations.
For a CP group of `N` members, the majority is calculated as `N / 2 + 1`. For
instance, in a CP group of 5 CP members, operations are committed when they are
replicated to at least 3 CP members. This CP group can tolerate the failure of 2 CP
members and remain available. However, if we run a CP group with 6 CP members,
it can still tolerate the failure of 2 CP members because the majority of 6 is 4.
Therefore, it does not improve the degree of fault tolerance compared to 5 CP
members.

CP Subsystem achieves horizontal scalability thanks to all of
the aforementioned CP group management capabilities. You can scale out
the throughput and memory capacity by distributing your CP data structures to
multiple CP groups, i.e., manual partitioning / sharding, and distributing
those CP groups over CP members, i.e., choosing a CP group size that is smaller
than the CP member count configuration. Nevertheless, the current set of CP
data structures has quite low memory overheads. Moreover, related to the Raft
consensus algorithm, each CP group makes use of internal heartbeat RPCs to
maintain authority of the Raft leader and help lagging CP group members to make
progress. Last, the new CP lock and semaphore implementations rely on a brand
new session mechanism. In a nutshell, a Hazelcast server or a client starts
a new session on the corresponding CP group when it makes its very first lock
or semaphore acquire request, and then periodically commits session heartbeats
to this CP group in order to indicate its liveliness. It means that if CP
locks and semaphores are distributed to multiple CP groups, there will be
a session management overhead on each CP group. See
the <<cp-sessions, CP Sessions section>> for more details. For these reasons,
we recommend developers to use a minimal number of CP groups. For most use
cases, _the DEFAULT CP group_ should be sufficient to maintain all CP data
structure instances. Custom CP groups is recommended only when you benchmark
your deployment and decide that performance of _the DEFAULT CP group_ is not
sufficient for your workload.

By default, CP Subsystem works only in memory without persisting any state to
disk. It means that a crashed CP member is not able to join to the cluster back
by restoring its previous state. Therefore, crashed CP members create a danger
for gradually losing majority of CP groups and eventually cause the total loss
of availability of CP Subsystem. To prevent such situations, crashed CP members
can be removed from CP Subsystem and replaced in CP groups with other available
CP members. This flexibility provides a good degree of fault tolerance at
run-time. See the <<cp-subsystem-configuration, CP Subsystem Configuration section>>
and <<cp-subsystem-management, CP Subsystem Management section>> for more
details. Moreover, CP Subsystem Persistence enables more robustness. When it is
enabled, CP members persist their local state to stable storage and can restore
their state after crashes. See the <<cp-subsystem-persistence, CP Subsystem Persistence section>>
for more details.

**API Code Sample:**

[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=apisample]
----

[WARNING]
====
The CP data structure proxies differ from the other data Hazelcast structure
proxies in two aspects:

* An internal commit is performed on _the METADATA CP group_ every time you
fetch a proxy from this interface. Hence, the callers should cache the returned proxy
objects.
* If you call the `DistributedObject.destroy()` method on a CP data structure
proxy, that data structure is terminated on the underlying CP group and cannot
be reinitialized until the CP group is force-destroyed via
`CPSubsystemManagementService.forceDestroyCPGroup(String)`. For this reason,
please make sure that you are completely done with a CP data structure before
destroying its proxy.
====

=== CP Discovery Process

CP Subsystem runs a discovery process on cluster startup. When CP Subsystem is
enabled by setting a positive value to
`CPSubsystemConfig.setCPMemberCount(int)`, say `N`, the first `N` members in
the Hazelcast cluster member list initiate this discovery process. Other
Hazelcast members skip this step. The CP discovery process runs out of the box
on top of Hazelcast's cluster member list without requiring any custom
configuration for different environments. It is completed when each one of
the first `N` Hazelcast members initializes its local CP member list and
commits it to _the METADATA CP group_. **A soon-to-be CP member terminates
itself if any of the following conditions occur before the CP discovery process
is completed:**

* Any Hazelcast member leaves the cluster
* The local Hazelcast member commits a CP member list which is different from
other members' committed CP member lists
* The local Hazelcast member fails to commit its discovered CP member list for
any reason.

When CP Subsystem is reset via `CPSubsystemManagementService.restart()`,
the CP discovery process is triggered again. However, it does not terminate
Hazelcast members if the new CP discovery round fails for any of
the aforementioned reasons, because Hazelcast members are likely to contain
data for AP data structures and their termination can cause data loss. Hence,
you need to observe the cluster and check if the CP discovery process
completes successfully on the CP Subsystem reset. See the <<cp-subsystem-management-apis, CP Subsystem Management APIs section>>
for more details.

You can use the `CPSubsystemManagementService.awaitUntilDiscoveryCompleted(timeout, timeUnit)`
API to wait until the CP discovery process is completed.

=== CP Subsystem Persistence

[navy]*Hazelcast IMDG Enterprise*

==== CP Subsystem Persistence Overview

CP Subsystem Persistence enables CP members to recover from crash scenarios.
This capability significantly improves the overall reliability of CP Subsystem.
When it is enabled via `CPSubsystemConfig.setPersistenceEnabled(boolean)`, CP
members persist their local state to stable storage. When you restart the crashed
CP members, they restore their local state and resume working as if they have
never crashed. CP Subsystem Persistence enables you to handle single or
multiple CP member crashes, or even whole cluster crashes and guarantees that
committed operations are not lost after recovery. In other words, CP member
crashes and restarts do not create any consistency problem. As long as majority
of CP members are available after recovery, CP Subsystem remains operational.

Please see the <<cp-subsystem-configuration, CP Subsystem Configuration section>>
for the configuration options of CP Subsystem Persistence.

When CP Subsystem Persistence is enabled, all Hazelcast cluster members create
a sub-directory under the base persistence directory which is specified via
`CPSubsystemConfig.getBaseDir()`. This means that AP Hazelcast members, which
are the ones not marked as CP members during the CP discovery process, create
their persistence directories as well. Those members persist only
the information that they are not CP members. This is done because when
a Hazelcast member starts with CP Subsystem Persistence enabled, it checks if
there is a CP persistence directory belonging to itself. If it founds one, it
skips the CP discovery process and initializes its CP member identity from
the persisted data. If it was an AP member before shutdown or crash, it
restores this information and starts as an AP member. Otherwise, it could think
that the CP discovery process has not been executed and trigger it, which would
break CP Subsystem.

WARNING: In light of this information, if you have both CP and AP members in
your cluster when CP Subsystem Persistence is enabled, and if you want to
perform a cluster-wide restart, you need to ensure that AP members are also
restarted with their CP persistence directories.

You can check the code sample below to see how CP Subsystem Persistence works
in general. In this code sample, we configure CP Subsystem with 3 CP members
and also enable CP Subsystem Persistence. We start 3 Hazelcast members with
this configuration and update a CP `IAtomicLong` instance. Each member creates
a sub-directory for itself inside the default base CP Subsystem Persistence
directory and stores its local CP state there. Then, we terminate two of these
members as if they crash and restart only 1 of them back. When we fetch
the same `IAtomicLong` instance from the restarted members and get its current
value, we see that it returns the update that we made before terminating these
members. Please note that we make sure that we have the majority of CP members
alive to keep CP Subsystem available after restart.

[source,java]

----
include::{javasource}/cp/CpSubsystemPersistence.java[tag=cppersistence1]
----

==== CP Subsystem Persistence Behavior During CP Subsystem Reset

If the majority of CP members are permanently lost, CP Subsystem becomes
unavailable. There is no solution to recover from this failure case with strong
consistency guarantee. CP Subsystem Management API contains a method to delete
all CP Subsystem state on the remaining CP members and start from scratch.
`CPSubsystemManagementService.restart()` wipes and resets the whole CP
Subsystem state and initializes it as if the Hazelcast cluster is starting up
for the first time. This method deletes the persisted CP member states as well.

==== Interaction with Hot Restart Persistence

Hazelcast offers another persistence capability which is called
<<hot-restart-persistence, Hot Restart Persistence>>. Hot Restart Persistence
is used for restarting a cluster with large AP data after a planned cluster
shutdown or a whole cluster-wide crash. Please note that CP Subsystem
Persistence and Hot Restart Persistence are separate features with different
behaviors and reliability guarantees. For instance, CP Subsystem Persistence
guarantees that committed operations will be restored and the linearizability
semantics of the CP Subsystem data structures will be preserved on restarts.
However, Hot Restart Persistence may lose some of the acknowledged updates on
AP data structures, based on how you configure the `fsync` behavior for your
persisted AP data structures. Moreover, if you store AP and CP data in a single
Hazelcast cluster and use both of the persistence features, Hazelcast member
restarts or cluster restarts can fail because of the Hot Restart Persistence
recovery semantics, even if the CP Subsystem Persistence recovery procedure is
successful, or vice-versa.

=== CP Member Shutdown

WARNING: Please read this part carefully to notice the behavioral difference
in the CP member shutdown process when CP Subsystem Persistence is
enabled and disabled.

There is a significant behavioral difference during the CP member shutdown when CP
Subsystem Persistence is enabled and disabled. When disabled (the default mode
in which CP Subsystem works only in memory), a shutting down CP member is
replaced with other available CP members in all of its CP groups in order not
to decrease or more importantly not to lose majorities of CP groups. It is
because CP members keep their local state only in memory when CP Subsystem
Persistence is disabled, hence a shut-down CP member cannot join back with its
CP identity and state, hence it is better to remove it from CP Subsystem to not
to harm availability of CP groups. If there is no other available CP member to
replace a shutting down CP member in a CP group, that CP group's size is
reduced by 1 and its majority value is recalculated. On the other hand, when CP
Subsystem Persistence is enabled, a shut-down CP member can come back by
restoring its CP state. Therefore, it is not automatically removed from CP
Subsystem when CP Subsystem Persistence is enabled. It is up to you to
remove shut-down CP members via
`CPSubsystemManagementService.removeCPMember(String)` if they will not come
back.

In summary, CP member shutdown behavior is as follows:

* When CP Subsystem Persistence is disabled (the default mode), shutting down
CP members are removed from CP Subsystem and the CP group majority values are
recalculated.
* When CP Subsystem Persistence is enabled, shutting down CP members are still
kept in CP Subsystem     so they will be a part of the CP group majority calculations.

Moreover, there is a subtle point about concurrent shutdown of CP members when
CP Subsystem Persistence is disabled. If there are `N` CP members in CP
Subsystem, `HazelcastInstance.shutdown()` can be called on `N-2` CP members
concurrently. Once these `N-2` CP members complete their shutdown,
the remaining `2` CP members must be shut down serially. Even though
the shutdown API can be called concurrently on multiple members, _the METADATA
CP_ group handles shutdown requests serially. Therefore, it would be simpler to
shut down CP members one by one, by calling `HazelcastInstance.shutdown()` on
the next CP member once the current CP member completes its shutdown. This rule
does not apply when CP Subsystem Persistence is enabled so you can shut down
your CP members concurrently if you enabled CP Subsystem Persistence. It is
enough for users to recall this rule while shutting down CP members when CP
Subsystem Persistence is disabled. If interested, you can read the rest of this
paragraph to learn the reasoning behind this rule. Each shutdown request
internally requires a Raft commit to _the METADATA CP group_ when CP Subsystem
Persistence is disabled. A CP member proceeds to shutdown after it receives
a response of this commit. To be able to perform a Raft commit, _the METADATA
CP group_ must have its majority up and running. When only 2 CP members are
left after graceful shutdowns, the majority of _the METADATA CP group_ becomes
2. If the last 2 CP members shut down concurrently, one of them is likely to
perform its Raft commit faster than the other one and leave the cluster before
the other CP member completes its Raft commit. In this case, the last CP member
waits for a response of its commit attempt on _the METADATA CP group_, and
times out eventually. This situation causes an unnecessary delay on the shutdown
process of the last CP member. On the other hand, when the last 2 CP members
shut down serially, the ``N-1``th member receives the response of its commit
after its shutdown request is committed also on the last CP member. Then,
the last CP member checks its local data to notice that it is the last CP
member alive, and proceeds its shutdown without attempting a Raft commit on
_the METADATA CP group_.

=== CP Subsystem's Fault Tolerance Capabilities

CP Subsystem's fault tolerance capabilities are summarized in this section.
For the sake of simplicity, let's assume that both the CP member count and CP
group size configurations are configured as the same and we use only
_the DEFAULT CP group_. **In the list below, "a permanent crash" means that
a CP member either crashes while CP Subsystem Persistence is disabled, hence
it cannot be recovered with its CP identity and data, or it crashes while CP
Subsystem Persistence is enabled but its CP data cannot be recovered, for
instance, due to a total server crash or a disk failure.**

* If a CP member leaves the Hazelcast cluster, it is not automatically removed
from CP Subsystem because CP Subsystem cannot certainly determine if that
member has actually crashed or just disconnected from the cluster. Therefore,
absent CP members are still considered in majority calculations and cause a
danger for the availability of CP Subsystem. If you know for sure that
an absent CP member is crashed, you can remove that CP member from CP Subsystem
via `CPSubsystemManagementService.removeCPMember(String)`. This API call
removes the given CP member from all CP groups and recalculates their majority
values. If there is another available CP member in CP Subsystem, the removed CP
member is replaced with that one, or you can promote an AP member of
the Hazelcast cluster to the CP role via
`CPSubsystemManagementService.promoteToCPMember()`.
* There might be a small window of unavailability after a CP member crash even
if the majority of CP members are still online. For instance, if a crashed CP
member is the Raft leader for some CP groups, those CP groups run a new leader
election round to elect a new leader among remaining CP group members. CP
Subsystem API calls that internally hit those CP groups are retried until they
have new Raft leaders. If a failed CP member has the Raft follower role, it
causes a very minimal disruption since Raft leaders are still able to replicate
and commit operations with the majority of their CP group members.
* If a crashed CP member is restarted after it is removed from CP Subsystem,
its behavior depends on whether CP Subsystem Persistence is enabled or disabled. If
enabled, a restarted CP member is not able to
restore its CP data from disk because after it joins back to the cluster it
notices that it is no longer a CP member. Because of that, it fails its startup
process and prints an error message. The only thing to do in this case is
manually delete its CP persistence directory since its data is no longer
useful. On the other hand, if CP Subsystem Persistence is disabled, a failed CP
member cannot remember anything related to its previous CP identity, hence it
restarts as a new AP member.
* A CP member can encounter a network issue and disconnect from
the cluster. If you remove this CP member from CP Subsystem even though it
is actually alive but only disconnected, you should terminate it
to prevent any accidental communication with the other CP members in
CP Subsystem.
* If a network partition occurs, behavior of CP Subsystem depends on how CP
members are divided in different sides of the network partition and to which
sides Hazelcast clients are connected. Each CP group remains available on
the side that contains the majority of its CP members. If a Raft leader falls
into the minority side, its CP group elects a new Raft leader on the other side
and callers that are talking to the majority side continue to make successful
API calls on CP Subsystem. However, callers that are talking to the minority
side fail with operation timeouts. When the network problem is resolved, CP
members reconnect to each other and CP groups continue their operation
normally.
* CP Subsystem can tolerate failure of the minority of CP members (less than
 `N / 2 + 1`) for availability. If `N / 2 + 1` or more CP members crash, CP
Subsystem loses its availability. If CP Subsystem Persistence is enabled and
the majority of CP members become online by successfully restarting some of
failed CP members, CP Subsystem regains its availability back. **Otherwise, it
means that CP Subsystem has lost its majority irrevocably.** In this case,
the only solution is to wipe-out the whole CP Subsystem state by performing
a force-reset via `CPSubsystemManagementService.restart()`.


**When `CPSubsystemConfig.getCPMemberCount()` is greater than
`CPSubsystemConfig.getGroupSize()`, CP groups are formed by selecting a subset
of CP members. In this case, each CP group can have a different set of CP
members, therefore different fault tolerance and availability conditions.** In
the following list, CP Subsystem's additional fault tolerance capabilities are
discussed for this configuration case.

* When the majority of a _non-METADATA CP group_ permanently crash, that CP
group cannot make progress anymore, even though other CP groups in CP Subsystem
are running fine. Even a new CP member cannot join to this CP group, because
membership changes also go through the Raft consensus algorithm. For this
reason, the only option is to force-destroy this CP group via
`CPSubsystemManagementService.forceDestroyCPGroup(String)`. When this API is
called, the CP group is terminated non-gracefully without the Raft mechanics.
After this API call, all existing CP data structure proxies that talk to this
CP group fail with `CPGroupDestroyedException`. However, if a new proxy is
created afterwards, then this CP group is re-created from scratch with a new
set of CP members. Losing majority of a _non-METADATA CP group_ can be likened
to partition-loss scenario of AP Hazelcast. Please note that _non-METADATA CP
groups_ that have lost their majority must be force-destroyed immediately,
because they can block _the METADATA CP group_ to perform membership changes on
CP Subsystem.
* If the majority of _the METADATA CP group_ permanently crash, unfortunately
it is equivalent to the permanent crash of the majority CP members of the whole
CP Subsystem, even though other CP groups are running fine. In fact, existing
CP groups continue serving to incoming requests, but since the _METADATA CP
group_ is not available anymore, no management tasks can be performed on CP
Subsystem. For instance, a new CP group cannot be created. In this case,
the only solution is to wipe-out the whole CP Subsystem state by performing
a force-reset via `CPSubsystemManagementService.restart()`.

See <<cp-subsystem-management-apis, CP Subsystem Management APIs section>> for
more details.

=== CP Sessions

For CP data structures that involve resource ownership management, such as
Locks or Semaphores, sessions are required to keep track of liveliness of
callers. In this context, __caller__ means an entity that uses CP Subsystem
APIs. It can be either a Hazelcast member or a client. A caller initially
creates a session before sending its very first session based request to the CP
group, such as a Lock / Semaphore acquire. After creating a session on the CP
group, the caller stores its session ID locally and sends it alongside its
session-based operations. A single session is used for all lock and semaphore
proxies of the caller. When a CP group receives a session-based operation, it
checks the validity of the session using the session ID information available
in the operation. A session is valid if it is still open in the CP group.
An operation with a valid session ID is accepted as a new session heartbeat.
While a caller is idle, in other words, it does not send any session based
operation to the CP group for a while, it commits periodic heartbeats to
the CP group in the background in order to keep its session alive. This
interval is specified in
`CPSubsystemConfig.getSessionHeartbeatIntervalSeconds()`.

A session is closed when the caller does not touch the session during a
predefined duration. In this case, the caller is assumed to be crashed and all
its resources are released automatically. This duration is specified in
`CPSubsystemConfig.getSessionTimeToLiveSeconds()`. See
the <<cp-subsystem-configuration, CP Subsystem Configuration section>> for
recommendations to choose a reasonable session time-to-live duration.

Sessions offer a trade-off between liveliness and safety. If you set a very
small value using `CPSubsystemConfig.setSessionTimeToLiveSeconds(int)`, then a
session owner could be considered crashed very quickly and its resources can be
released prematurely. On the other hand, if you set a large value, a session
could be kept alive for an unnecessarily long duration even if its owner
actually crashes. However, it is a safer approach to not to use a small session
time-to-live duration. If a session owner is known to be crashed, its session
could be closed manually via
`CPSessionManagementService.forceCloseSession(String, long)`.

See the <<cp-subsystem-configuration, CP Subsystem Configuration section>> for
more details.

=== FencedLock

`FencedLock` is a linearizable & distributed & reentrant implementation of
`j.u.c.locks.Lock`. `FencedLock` is accessed via `CPSubsystem.getLock(String)`.
It is CP with respect to the CAP principle. It works on top of the Raft
consensus algorithm. It offers linearizability during crash-stop failures and
network partitions. If a network partition occurs, it remains available on at
most one side of the partition.

`FencedLock` works on top of CP sessions. Please see <<cp-sessions, CP Sessions>>
section for more information about CP sessions.

By default, `FencedLock` is reentrant. Once a caller acquires the lock, it can
acquire the lock reentrantly as many times as it wants in a linearizable manner.
You can configure the reentrancy behavior via `FencedLockConfig`. For instance,
reentrancy can be disabled and `FencedLock` can work as a non-reentrant mutex.
You can also set a custom reentrancy limit. When the reentrancy limit is
already reached, `FencedLock` does not block a lock call. Instead, it fails
with `LockAcquireLimitReachedException` or a specified return value. Please
check the locking methods to see details about the behavior and
<<fencedlock-configuration, FencedLock Configuration section>> for
the configuration.

Distributed locks are unfortunately *not equivalent* to single-node mutexes
because of the complexities in distributed systems, such as uncertain
communication patterns, and independent and partial failures.
In an asynchronous network, no lock service can guarantee mutual exclusion,
because there is no way to distinguish between a slow and a crashed process.
Consider the following scenario, where a Hazelcast client acquires
a `FencedLock`, then hits a long GC pause. Since it will not be able to commit
session heartbeats while paused, its CP session will be eventually closed.
After this moment, another Hazelcast client can acquire this lock. If the first
client wakes up again, it may not immediately notice that it has lost ownership
of the lock. In this case, multiple clients think they hold the lock. If they
attempt to perform an operation on a shared resource, they can break
the system. To prevent such situations, you can choose to use an infinite
session timeout, but this time probably you are going to deal with liveliness
issues. For the scenario above, even if the first client actually crashes,
the requests sent by two clients can be reordered in the network and hit
the external resource in the reverse order.

There is a simple solution for this problem. Lock holders are ordered by a
monotonic fencing token, which increments each time the lock is assigned to a
new owner. This fencing token can be passed to external services or resources
to ensure sequential execution of the side effects performed by lock holders.

The following diagram illustrates the idea. Client-1 acquires the lock first
and receives `1` as its fencing token. Then, it passes this token to
the external service, which is our shared resource in this scenario. Just after
that, Client-1 hits a long GC pause and eventually loses ownership of the lock
because it misses to commit CP session heartbeats. Then, Client-2 chimes in and
acquires the lock. Similar to Client-1, Client-2 passes its fencing token to
the external service. After that, once Client-1 comes back alive, its write
request will be rejected by the external service, and only Client-2 will be
able to safely talk to it.

image::FencedLock.png[Fenced Lock]

You can read more about the fencing token idea in Martin Kleppmann's
https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html[How to do distributed locking]
blog post and Google's https://ai.google/research/pubs/pub27897[Chubby paper].
`FencedLock` integrates this idea with the `j.u.c.locks.Lock` abstraction,
excluding `j.u.c.locks.Condition`. `newCondition()` is not implemented and
throws `UnsupportedOperationException`.

All of the API methods in the new `FencedLock` abstraction offer exactly-once
execution semantics. For instance, even if a `lock()` call is internally
retried because of a crashed CP member, the lock is acquired only once.
The same rule also applies to the other methods in the API.

=== Configuration

==== CP Subsystem Configuration

* `cp-member-count`: Number of CP members to initialize CP Subsystem. It is `0`
by default, meaning that CP Subsystem is disabled. CP Subsystem is enabled when
a positive value is set. After CP Subsystem is initialized successfully, more
CP members can be added at run-time and the number of active CP members can go
beyond the configured CP member count. The number of CP members can be smaller
than the total size of the Hazelcast cluster. For instance, you can run `5` CP
members in a Hazelcast cluster of ``20`` members.
+
If set, must be greater than or equal to `group-size`.
+
* `group-size`: Number of CP members to form CP groups. If set, it must be
an odd number between `3` and `7`. Otherwise, `cp-member-count` is respected
while forming CP groups.
+
If set, must be smaller than or equal to `cpMemberCount`.
+
* `session-time-to-live-seconds`: Duration for a CP session to be kept alive
after the last received heartbeat. A CP session is closed if no session
heartbeat is received during this duration. Session TTL must be decided wisely.
If a very low value is set, a CP session can be closed prematurely if its owner
Hazelcast instance temporarily loses connectivity to CP Subsystem because of a
network partition or a GC pause. In such an occasion, all CP resources of this
Hazelcast instance, such as `FencedLock` or `ISemaphore`, are released.
On the other hand, if a very large value is set, CP resources can remain
assigned to an actually crashed Hazelcast instance for too long and liveliness
problems can occur. CP Subsystem offers an API in `CPSessionManagementService`
to deal with liveliness issues related to CP sessions. In order to prevent
premature session expires, session TTL configuration can be set a relatively
large value and `CPSessionManagementService.forceCloseSession(String, long)`
can be manually called to close CP session of a crashed Hazelcast instance.
+
Must be greater than `session-heartbeat-interval-seconds`, and smaller than or
equal to `missing-cp-member-auto-removal-seconds`.
+
Default value is `300` seconds.
+
* `session-heartbeat-interval-seconds`: Interval for the periodically-committed
CP session heartbeats. A CP session is started on a CP group with the first
session-based request of a Hazelcast instance. After that moment, heartbeats
are periodically committed to the CP group.
+
Must be smaller than `session-time-to-live-seconds`.
+
Default value is `5` seconds.
+
* `missing-cp-member-auto-removal-seconds`: Duration to wait before
automatically removing a missing CP member from CP Subsystem. When a CP member
leaves the Hazelcast cluster, it is not automatically removed from CP
Subsystem, since it could be still alive and left the cluster because of
a network partition. On the other hand, if a missing CP member is actually
crashed, it creates a danger for its CP groups, because it will be still part
of majority calculations. This situation could lead to losing majority of CP
groups if multiple CP members leave the cluster over time.
+
With the default configuration, missing CP members are automatically removed
from CP Subsystem after `4` hours. This feature is very useful in terms of
fault tolerance when CP member count is also configured to be larger than group
size. In this case, a missing CP member will be safely replaced in its CP
groups with other available CP members in CP Subsystem. This configuration also
implies that no network partition is expected to be longer than the configured
duration.
+
If a missing CP member comes back alive after it is automatically removed from
CP Subsystem with this feature, that CP member must be terminated manually.
+
Must be greater than or equal to `session-time-to-live-seconds`.
+
Default value is `14400` seconds (`4` hours).
+
* `fail-on-indeterminate-operation-state`: Offers a choice between
at-least-once and at-most-once execution of the operations on top of the Raft
consensus algorithm. It is disabled by default and offers at-least-once
execution guarantee. If enabled, it switches to at-most-once execution
guarantee. When you invoke an API method on a CP data structure proxy, it
replicates an internal operation to the corresponding CP group. After this
operation is committed to majority of this CP group by the Raft leader node, it
sends a response for the public API call. If a failure causes loss of
the response, then the calling side cannot determine if the operation is
committed on the CP group or not. In this case, if this configuration is
disabled, the operation is replicated again to the CP group, and hence could be
committed multiple times. If it is enabled, the public API call fails with
`IndeterminateOperationStateException`.
+
Default value is `false`.
+
* `persistence-enabled`: Specifies whether CP Subsystem Persistence is globally
enabled for CP groups created in CP Subsystem. If enabled, CP members persist
their local CP data to stable storage and can recover from crashes.
+
Default value is `false`.
* `base-dir`: Specifies the parent directory where CP data is stored. You can
use the default value, or you can specify the value of another folder, but it
is mandatory that `base-dir` element has a value. This directory is created
automatically if it does not exist.
+
`base-dir` is used as the parent directory, and a unique directory is created
inside `base-dir` for each CP member which uses the same `base-dir`. That
means, `base-dir` is shared among multiple CP members safely. This is
especially useful for cloud environments where CP members generally use
a shared filesystem.
+
Default value is `cp-data`.
+
* `data-load-timeout-seconds`: Timeout duration for CP members to restore their
data from disk. A CP member fails its startup if it cannot complete its CP data
restore process in the configured duration.
+
Default value is `120` seconds.

**Declarative Configuration:**

[source,xml]
----
<hazelcast>
    ...
    <cp-subsystem>
        <cp-member-count>7</cp-member-count>
        <group-size>3</group-size>
        <session-time-to-live-seconds>300</session-time-to-live-seconds>
        <session-heartbeat-interval-seconds>5</session-heartbeat-interval-seconds>
        <missing-cp-member-auto-removal-seconds>14400</missing-cp-member-auto-removal-seconds>
        <fail-on-indeterminate-operation-state>false</fail-on-indeterminate-operation-state>
        <persistence-enabled>true</persistence-enabled>
        <base-dir>/custom-cp-dir</base-dir>
    </cp-subsystem>
    ...
</hazelcast>
----

**Programmatic Configuration:**

[source,java]
----
include::{javasource}/cp/CpSubsystemConfiguration.java[tag=cpconf]
----

==== FencedLock Configuration

* `name`: Name of the `FencedLock`.
* `lock-acquire-limit`: Maximum number of reentrant lock acquires. Once
a caller acquires the lock this many times, it will not be able to acquire
the lock again, until it makes at least one `unlock()` call.
+
By default, no upper bound is set for the number of reentrant lock acquires,
which means that once a caller acquires a `FencedLock`, all of its further
`lock()` calls will succeed. However, for instance, if you set
`lock-acquire-limit` to `2`, once a caller acquires the lock, it will be able
to acquire it once more, but its third `lock()` call will not succeed.
+
If `lock-acquire-limit` is set to 1, then the lock becomes non-reentrant.
+
`0` means there is no upper bound for the number of reentrant lock acquires.
+
Default value is `0`.

**Declarative Configuration:**

[source,xml]
----
<hazelcast>
    ...
    <cp-subsystem>
        ...
        <locks>
            <fenced-lock>
                <name>reentrant-lock</name>
                <lock-acquire-limit>0</lock-acquire-limit>
            </fenced-lock>
            <fenced-lock>
                <name>limited-reentrant-lock</name>
                <lock-acquire-limit>10</lock-acquire-limit>
            </fenced-lock>
            <fenced-lock>
                <name>non-reentrant-lock</name>
                <lock-acquire-limit>1</lock-acquire-limit>
            </fenced-lock>
        </locks>
    </cp-subsystem>
    ...
</hazelcast>
----

**Programmatic Configuration:**

[source,java]
----
include::{javasource}/cp/CpSubsystemConfiguration.java[tag=cplockconf]
----

==== Semaphore Configuration

* `name`: Name of the CP `ISemaphore`.
* `jdk-compatible`: Enables / disables JDK compatibility of the CP
`ISemaphore`. When it is JDK compatible, just as in
the `j.u.c.Semaphore.release()` method, a permit can be released without
acquiring it first, because acquired permits are not bound to threads. However,
there is no auto-cleanup of the acquired permits upon Hazelcast server / client
failures. If a permit holder fails, its permits must be released manually. When
JDK compatibility is disabled, a `HazelcastInstance` must acquire permits
before releasing them and it cannot release a permit that it has not acquired.
It means, you can acquire a permit from one thread and release it from another
thread using the same `HazelcastInstance`, but not different
``HazelcastInstance``s. In this mode, acquired permits are automatically
released upon failure of the holder `HazelcastInstance`. So there is a minor
behavioral difference to the `j.u.c.Semaphore.release()` method.
+
JDK compatibility is disabled by default.

**Declarative Configuration:**

[source,xml]
----
<hazelcast>
    ...
    <cp-subsystem>
        ...
        <semaphores>
            <cp-semaphore>
                <name>jdk-compatible-semaphore</name>
                <jdk-compatible>true</jdk-compatible>
            </cp-semaphore>
            <cp-semaphore>
                <name>another-semaphore</name>
                <jdk-compatible>false</jdk-compatible>
            </cp-semaphore>
        </semaphores>
    </cp-subsystem>
    ...
</hazelcast>
----

**Programmatic Configuration:**

[source,java]
----
include::{javasource}/cp/CpSubsystemConfiguration.java[tag=cpsemaconf]
----

==== Raft Algorithm Configuration

WARNING: These parameters tune specific parameters of Hazelcast's Raft
consensus algorithm implementation and are only for power users.

* `leader-election-timeout-in-millis`: Leader election timeout in milliseconds.
If a candidate cannot win the majority of the votes in time, a new election
round is initiated.
+
Default value is `2000` milliseconds.
* `leader-heartbeat-period-in-millis`: Duration in milliseconds for a Raft
leader node to send periodic heartbeat messages to its followers in order to
denote its liveliness. Periodic heartbeat messages are actually append entries
requests and can contain log entries for the lagging followers. If a too small
value is set, heartbeat messages are sent from Raft leaders to followers too
frequently and it can cause an unnecessary usage of CPU and network.
+
Default value is `5000` milliseconds.
* `max-missed-leader-heartbeat-count`: Maximum number of missed Raft leader
heartbeats for a follower to trigger a new leader election round. For instance,
if `leader-heartbeat-period-in-millis` is `1` second and this value is set to
``5``, then a follower triggers a new leader election round if `5` seconds pass
after the last heartbeat message of the current Raft leader node. If this
duration is too small, new leader election rounds can be triggered
unnecessarily if the current Raft leader temporarily slows down or a network
congestion occurs. If it is too large, it takes longer to detect failures of
Raft leaders.
+
Default value is `5`.
* `append-request-max-entry-count`: Maximum number of Raft log entries that can
be sent as a batch in a single append entries request. In Hazelcast's Raft
consensus algorithm implementation, a Raft leader maintains a separate
replication pipeline for each follower. It sends a new batch of Raft log
entries to a follower after the follower acknowledges the last append entries
request sent by the leader.
+
Default value is `100`.
* `commit-index-advance-count-to-snapshot`: Number of new commits to initiate
a new snapshot after the last snapshot taken by the local Raft node. This value
must be configured wisely as it effects performance of the system in multiple
ways. If a small value is set, it means that snapshots are taken too frequently
and Raft nodes keep a very short Raft log. If snapshots are large and CP
Subsystem Persistence is enabled, this can create an unnecessary overhead on IO
performance. Moreover, a Raft leader can send too many snapshots to followers
and this can create an unnecessary overhead on network. On the other hand, if
a very large value is set, it can create a memory overhead since Raft log
entries are going to be kept in memory until the next snapshot.
+
Default value is `10000`.
* `uncommitted-entry-count-to-reject-new-appends`: Maximum number of
uncommitted log entries in the leader's Raft log before temporarily rejecting
new requests of callers. Since Raft leaders send log entries to followers in
batches, they accumulate incoming requests in order to improve the throughput.
You can configure this field by considering your degree of concurrency in your
callers. For instance, if you have at most `1000` threads sending requests to
a Raft leader, you can set this field to `1000` so that callers do not get
retry responses unnecessarily.
+
Default value is `100`.
* `append-request-backoff-timeout-in-millis`: Timeout duration in milliseconds
to apply backoff on append entries requests. After a Raft leader sends
an append entries request to a follower, it will not send a subsequent append
entries request either until the follower responds or this timeout occurs.
Backoff durations are increased exponentially if followers remain unresponsive.
+
Default value is `100` milliseconds.

**Declarative Configuration:**

[source,xml]
----
<hazelcast>
    ...
    <cp-subsystem>
        ...
        <raft-algorithm>
            <leader-election-timeout-in-millis>2000</leader-election-timeout-in-millis>
            <leader-heartbeat-period-in-millis>5000</leader-heartbeat-period-in-millis>
            <max-missed-leader-heartbeat-count>5</max-missed-leader-heartbeat-count>
            <append-request-max-entry-count>100</append-request-max-entry-count>
            <commit-index-advance-count-to-snapshot>10000</commit-index-advance-count-to-snapshot>
            <uncommitted-entry-count-to-reject-new-appends>200</uncommitted-entry-count-to-reject-new-appends>
            <append-request-backoff-timeout-in-millis>250</append-request-backoff-timeout-in-millis>
        </raft-algorithm>
        ...
    </cp-subsystem>
    ...
</hazelcast>
----

**Programmatic Configuration:**

[source,java]
----
include::{javasource}/cp/CpSubsystemConfiguration.java[tag=cpraftconf]
----


=== CP Subsystem Unsafe Mode

When CP Subsystem is not enabled, that means
`CPSubsystemConfig.getCPMemberCount()` is `0`, CP data structures operate in
_the unsafe mode_. In this mode, they use Hazelcast's partitioning and lazy
replication mechanisms instead of CP Subsystem's consensus mechanism. For more
information about the lazy replication mechanism of Hazelcast, see
the <<consistency-and-replication-model, Consistency and Replication Model chapter>>.

_The unsafe mode_ provides weaker consistency guarantees compared to when CP
Subsystem is enabled. For example, when you increment an `IAtomicLong` or
acquire a `FencedLock`, just before crash of a member, even though you receive
a success response, the write operation (increment of `IAtomicLong` or acquire
of `FencedLock`) can be lost (which cannot happen when CP Subsystem is enabled).
For this reason, _the unsafe mode_ not recommended for use-cases requiring
strong consistency. It is more suitable for development or testing. You should
take this limitation into consideration if you use CP Subsystem in production
with _the unsafe mode_.

NOTE: CP Subsystem Management APIs are not available in _the unsafe mode_.

NOTE: In _the unsafe mode_, split-brain protection is not supported.

=== CP Subsystem Management

Unlike the dynamic nature of Hazelcast clusters, CP Subsystem requires manual
intervention while expanding/shrinking its size, or when a CP member crashes or
becomes unreachable. When a CP member becomes unreachable, it is not
automatically removed from CP Subsystem because it could be still alive and
partitioned away.

Moreover, by default CP Subsystem works in memory without persisting any state
to disk. It means that a crashed CP member will not be able to recover by
reloading its previous state. Therefore, crashed CP members create a danger for
gradually losing the majority of CP groups and eventually total loss of
the availability of CP Subsystem. To prevent such situations,
`CPSubsystemManagementService` offers a set of APIs. In addition, CP Subsystem
Persistence can be enabled to make CP members persist their local CP state to
stable storage. Please see <<cp-subsystem-persistence, CP Subsystem Persistence section>>
for more details.

CP Subsystem relies on Hazelcast's failure detectors to test reachability of CP
members. Before removing a CP member from CP Subsystem, please make sure that
it is declared as unreachable by Hazelcast's failure detector and removed from
Hazelcast cluster's member list.

CP member additions and removals are internally handled by performing a single
membership change at a time. When multiple CP members are shutting down
concurrently, their shutdown process is executed serially. When a CP membership
change is triggered, _the METADATA CP group_ creates a membership change plan
for CP groups. Then, the scheduled changes are applied to the CP groups one by
one. After all CP group member removals are done, the shutting down CP member
is removed from the active CP members list and its shutdown process is
completed. A shut-down CP member is automatically replaced with another
available CP member in all of its CP groups, including _the METADATA CP group_,
in order not to decrease or more importantly not to lose the majority of CP
groups. If there is no available CP member to replace a shutting down CP member
in a CP group, that group's size is reduced by 1 and its majority value is
recalculated. Please note that this behavior is when CP Subsystem Persistence
is disabled. When CP Subsystem Persistence is enabled, shut-down CP members are
not automatically removed from the active CP members list and they are still
considered as part of CP groups and majority calculations, because they can come
back by restoring their local CP state from stable storage. If you know that a
shut-down CP member will not be restarted, you need to remove that member from
CP Subsystem via `CPSubsystemManagementService.removeCPMember(String)`.

A new CP member can be added to CP Subsystem to either increase the number of
available CP members for new CP groups or to fill the missing slots in existing
CP groups. After the initial Hazelcast cluster startup is done, an existing
Hazelcast member can be be promoted to the CP member role. This new CP member
automatically joins to CP groups that have missing members, and majority values
of these CP groups are recalculated.

==== CP Subsystem Management APIs

You can access the CP Subsystem management APIs using the Java API or REST
interface. To communicate with the REST interface there are two options; one is
to access REST endpoint URL directly or using the `cp-subsystem.sh` shell
script, which comes with the Hazelcast package.

NOTE: The `cp-cluster.sh` script uses `curl` command, and `curl` must be
installed to be able to use the script.

* **Get Local CP Member:**
+
Returns the local CP member if this Hazelcast member is a part of CP Subsystem.
+
.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=localmember]
----
+
.REST API
[source,sh]
----
> curl http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/members/local
OR
> sh cp-subsystem.sh -o get-local-member --address 127.0.0.1 --port 5701
+
Sample Response:
{
    "uuid": "6428d7fd-6079-48b2-902c-bdf6a376051e",
    "address": "[127.0.0.1]:5701"
}
----
+
* **Get CP Groups:**
+
Returns the list of active CP groups.
+
.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=cpgroups]
----
+
.REST API
[source,sh]
----
> curl http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/groups
OR
> sh cp-subsystem.sh -o get-groups --address 127.0.0.1 --port 5701
+
Sample Response:
[{
    "name": "METADATA",
    "id": 0
}, {
    "name": "atomics",
    "id": 8
}, {
    "name": "locks",
    "id": 14
}]
----
+
* **Get a single CP Group:**
+
Returns the active CP group with the given name. There can be at most one
active CP group with a given name.
+
.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=cpgroup]
----
+
.REST API
[source,sh]
----
> curl http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/groups/${CPGROUP_NAME}
OR
> sh cp-subsystem.sh -o get-group --group ${CPGROUP_NAME} --address 127.0.0.1 --port 5701
+
Sample Response:
{
    "id": {
        "name": "locks",
        "id": 14
    },
    "status": "ACTIVE",
    "members": [{
        "uuid": "33f84b0f-46ba-4a41-9e0a-29ee284c1c2a",
        "address": "[127.0.0.1]:5703"
    }, {
        "uuid": "59ca804c-312c-4cd6-95ff-906b2db13acb",
        "address": "[127.0.0.1]:5704"
    }, {
        "uuid": "777ff6ea-b8a3-478d-9642-47d1db019b37",
        "address": "[127.0.0.1]:5705"
    }, {
        "uuid": "c7856e0f-25d2-4717-9919-88fb3ecb3384",
        "address": "[127.0.0.1]:5702"
    }, {
        "uuid": "c6229b44-8976-4602-bb57-d13cf743ccef",
        "address": "[127.0.0.1]:5701"
    }]
}
----
+
* **Get CP Members:**
+
Returns the list of active CP members in the cluster.
+
.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=cpmembers]
----
+
.REST API
[source,sh]
----
> curl http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/members
OR
> sh cp-subsystem.sh -o get-members --address 127.0.0.1 --port 5701
+
Sample Response:
[{
    "uuid": "33f84b0f-46ba-4a41-9e0a-29ee284c1c2a",
    "address": "[127.0.0.1]:5703"
}, {
    "uuid": "59ca804c-312c-4cd6-95ff-906b2db13acb",
    "address": "[127.0.0.1]:5704"
}, {
    "uuid": "777ff6ea-b8a3-478d-9642-47d1db019b37",
    "address": "[127.0.0.1]:5705"
}, {
    "uuid": "c6229b44-8976-4602-bb57-d13cf743ccef",
    "address": "[127.0.0.1]:5701"
}, {
    "uuid": "c7856e0f-25d2-4717-9919-88fb3ecb3384",
    "address": "[127.0.0.1]:5702"
}]
----
+
* **Force Destroy a CP Group:**
+
Unconditionally destroys the given active CP group without using the Raft
algorithm mechanics. This method must be used only when a CP group loses its
majority and cannot make progress anymore. Normally, membership changes in CP
groups, such as CP member promotion or removal, are done via the Raft consensus
algorithm. However, when a CP group permanently loses its majority, it will not
be able to commit any new operation. Therefore, this method ungracefully
terminates the remaining members of the given CP group on the remaining CP
group members. It also performs a Raft commit to _the METADATA CP group_ in
order to update the status of the destroyed group. Once a CP group is
destroyed, all CP data structure proxies created before the destroy fails
with `CPGroupDestroyedException`. However, if a new proxy is created
afterwards, then this CP group is re-created from scratch with a new set of CP
members.
+
This method is idempotent. It has no effect if the given CP group is already
destroyed.
+
.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=destroygroup]
----
+
.REST API
[source,sh]
----
> curl -X POST --data "${GROUPNAME}&${PASSWORD}" http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/groups/${CPGROUP_NAME}/remove
OR
> sh cp-subsystem.sh -o force-destroy-group --group ${CPGROUP_NAME} --address 127.0.0.1 --port 5701 --groupname ${GROUPNAME} --password ${PASSWORD}
----
+
* **Remove a CP Member:**
+
Removes the given unreachable CP member from the active CP members list and all
CP groups it belongs to. If any other active CP member is available, it
replaces the removed CP member in its CP groups. Otherwise, CP groups which
the removed CP member is a member of shrinks and their majority values are
recalculated.
+
WARNING: Before removing a CP member from CP Subsystem, please make sure that
it is declared as unreachable by Hazelcast's failure detector and removed from
Hazelcast's member list. The behavior is undefined when a running CP member is
removed from CP Subsystem.
+
.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=removemember]
----
+
.REST API
[source,sh]
----
> curl -X POST --data "${GROUPNAME}&${PASSWORD}" http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/members/${CPMEMBER_UUID}/remove
OR
> sh cp-subsystem.sh -o remove-member --member ${CPMEMBER_UUID} --address 127.0.0.1 --port 5701 --groupname ${GROUPNAME} --password ${PASSWORD}
----
+
* **Promote Local Member to a CP Member**
+
Promotes the local Hazelcast member to the CP member. If the local member is
already in the active CP members list, i.e., it is already a CP member, then
this method has no effect. When the local member is promoted to the CP role,
its member UUID is assigned as CP member UUID. The promoted CP member will be
added to the CP groups that have missing members, i.e., whose current size is
smaller than `CPSubsystemConfig.getGroupSize()`.
+
.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=promotemember]
----
+
.REST API
[source,sh]
----
> curl -X POST --data "${GROUPNAME}&${PASSWORD}" http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/members
OR
> sh cp-subsystem.sh -o promote-member --address 127.0.0.1 --port 5701 --groupname ${GROUPNAME} --password ${PASSWORD}
----
+
* **Wipe and Reset CP Subsystem**
+
Wipes and resets the whole CP Subsystem state and initializes it as if
the Hazelcast cluster is starting up initially. This method must be used only
when _the METADATA CP group_ loses its majority and cannot make progress
anymore.
+
After this method is called, all CP state and data are wiped and CP members
start with empty state.
+
This method can be invoked only from the Hazelcast master member, which is
the first member in the Hazelcast cluster member list. Moreover, the Hazelcast
cluster must have at least `CPSubsystemConfig.getCPMemberCount()` members.
+
This method must not be called while there are membership changes in
the Hazelcast cluster. Before calling this method, please make sure that there
is no new member joining and all existing Hazelcast members have seen the same
member list.
+
To be able to use this method, the initial CP member count of CP Subsystem,
which is defined by `CPSubsystemConfig.getCPMemberCount()`, must be satisfied.
For instance, if `CPSubsystemConfig.getCPMemberCount()` is 5 and only 1 CP
member is alive, when this method is called, 4 additional AP Hazelcast members
should exist in the cluster, or new Hazelcast members must be started.
+
This method also deletes all data written by CP Subsystem Persistence.
+
This method triggers a new CP discovery process round. However, if the new CP
discovery round fails for any reason, Hazelcast members are not terminated,
because Hazelcast members are likely to contain data for AP data structures
and their termination can cause data loss. Hence, you need to observe
the cluster and check if the CP discovery process completes successfully.
+
WARNING: This method is **NOT** idempotent and multiple invocations can break
the whole system! After calling this API, you must observe the system to see if
the restart process is successfully completed or failed before making another
call.
+
WARNING: This method deletes all CP data written by CP Subsystem Persistence.
+
.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=restart]
----
+
.REST API
[source,sh]
----
> curl -X POST --data "${GROUPNAME}&${PASSWORD}" http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/restart
OR
> sh cp-subsystem.sh -o restart --address 127.0.0.1 --port 5701 --groupname ${GROUPNAME} --password ${PASSWORD}
----

==== Session Management API

There are two management API methods for session management.

* **Get CP Group Sessions:**
+
Returns all CP sessions that are currently active in a CP group.
+
.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=sessions]
----
+
.REST API
[source,sh]
----
> curl http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/groups/${CPGROUP_NAME}/sessions
OR
> sh cp-subsystem.sh -o get-sessions --group ${CPGROUP_NAME} --address 127.0.0.1 --port 5701
+
Sample Response:
[{
    "id": 1,
    "creationTime": 1549008095530,
    "expirationTime": 1549008766630,
    "version": 73,
    "endpoint": "[127.0.0.1]:5701",
    "endpointType": "SERVER",
    "endpointName": "hz-member-1"
}, {
    "id": 2,
    "creationTime": 1549008115419,
    "expirationTime": 1549008765425,
    "version": 71,
    "endpoint": "[127.0.0.1]:5702",
    "endpointType": "SERVER",
    "endpointName": "hz-member-2"
}]
----
+
* **Force Close a Session:**
+
If a Hazelcast instance that owns a CP session crashes, its CP session is not
terminated immediately. Instead, the session is closed after
`CPSubsystemConfig.getSessionTimeToLiveSeconds()` passes. If it is known for
sure that the session owner is not partitioned and definitely crashed, this
method can be used for closing the session and releasing its resources
immediately.
+
.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=closesession]
----
+
.REST API
[source,sh]
----
> curl -X POST --data "${GROUPNAME}&${PASSWORD}" http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/groups/${CPGROUP_NAME}/sessions/${CP_SESSION_ID}/remove
OR
> sh cp-subsystem.sh -o force-close-session --group ${CPGROUP_NAME} --session-id ${CP_SESSION_ID} --address 127.0.0.1 --port 5701 --groupname ${GROUPNAME} --password ${PASSWORD}
----
