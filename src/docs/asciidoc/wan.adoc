
== WAN

[blue]*Hazelcast IMDG Enterprise Feature*

=== WAN Replication

There could be cases where you need to synchronize multiple Hazelcast clusters to the same state. Hazelcast WAN Replication allows you to keep multiple Hazelcast clusters in sync by replicating their state over WAN environments such as the Internet.

Imagine you have different data centers in New York, London and Tokyo each running an independent Hazelcast cluster. Every cluster
would be operating at native speed in their own LAN (Local Area Network), but you also want some or all record sets in
these clusters to be replicated to each other: updates in the Tokyo cluster should also replicate to London and New York and updates
in the New York cluster are to be synchronized to the Tokyo and London clusters.

This chapter explains how you can replicate the state of your clusters over Wide Area Network (WAN) through Hazelcast WAN Replication.

NOTE: You can download the white paper **Hazelcast on AWS: Best Practices for Deployment** at
http://hazelcast.com/resources/hazelcast-on-aws-best-practices-for-deployment/[Hazelcast.com].


==== Defining WAN Replication

Hazelcast supports two different operation modes of WAN Replication:

* **Active-Passive:** This mode is mostly used for failover scenarios where you want to replicate an active cluster to one
  or more passive clusters, for the purpose of maintaining a backup.
* **Active-Active:** Every cluster is equal, each cluster replicates to all other clusters. This is normally used to connect
  different clients to different clusters for the sake of the shortest path between client and server.
  
There are two different ways of defining the WAN replication endpoints:

- Static endpoints
- Discovery SPI

You can use at most one of these when defining a single WAN publisher.

===== Defining WAN Replication Using Static Endpoints

Below is an example of declarative configuration of WAN Replication from New York cluster to target the London cluster:

[source,xml]
----
<hazelcast>
...
  <wan-replication name="my-wan-cluster-batch">
      <wan-publisher group-name="london">
          <class-name>com.hazelcast.enterprise.wan.replication.WanBatchReplication</class-name>
          <queue-full-behavior>THROW_EXCEPTION</queue-full-behavior>
          <queue-capacity>1000</queue-capacity>
          <properties>
              <property name="batch.size">500</property>
              <property name="batch.max.delay.millis">1000</property>
              <property name="snapshot.enabled">false</property>
              <property name="response.timeout.millis">60000</property>
              <property name="ack.type">ACK_ON_OPERATION_COMPLETE</property>
              <property name="endpoints">10.3.5.1:5701, 10.3.5.2:5701</property>
              <property name="group.password">london-pass</property>
              <property name="discovery.period">20</property>
              <property name="executorThreadCount">2</property>
          </properties>
      </wan-publisher>
  </wan-replication>
...
</hazelcast>
----

Following are the definitions of configuration elements:

* `name`: Name of your WAN Replication. This name is referenced in IMap or ICache configuration when you add WAN Replication for these data structures (using the element <wan-replication-ref> in the configuration of IMap or ICache).
* `group-name`: Configures target cluster's group name.
* `class-name`: Name of the class implementation for the WAN replication.
* `queue-full-behavior`: Policy to be applied when WAN Replication event queues are full. Please see the <<queue-full-behavior, Queue Full Behavior section>>.
* `queue-capacity`: Size of the queue of events. Its default value is 10000. Please see the <<queue-capacity, Queue Capacity section>>.
* `batch.size`: Maximum size of events that are sent to the target cluster in a single batch. Its default value is 500. Please see the <<batch-size, Batch Size section>>.
* `batch.max.delay.millis`: Maximum amount of time, in milliseconds, to be waited before sending a batch of events in case `batch.size` is not reached. Its default value is 1000 milliseconds. Please see the <<batch-maximum-delay, Batch Maximum Delay section>>.
* `snapshot.enabled`: When set to `true`, only the latest events (based on key) are selected and sent in a batch. Its default value is `false`.
* `response.timeout.millis`: Time, in milliseconds, to be waited for the acknowledgment of a sent WAN event to target cluster. Its default value is 60000 milliseconds. Please see the <<response-timeout, Response Timeout section>>.
* `ack.type`: Acknowledgment type for each target cluster. Please see the <<acknowledgment-types, Acknowledgment Types section>>.
* `endpoints`: IP addresses and ports of the cluster members for which the WAN replication is implemented. These endpoints are not necessarily the entire target cluster and WAN does not perform the discovery of other members in the target cluster. It only expects that these IP addresses (or at least some of them) are available.
* `group.password`: Configures target cluster's group password.

Other relevant properties are:
 
* `discovery.period`: Period in seconds in which WAN tries to reestablish connections to failed endpoints. Default is 10 seconds.
* `executorThreadCount`: The number of threads that the `WanBatchReplication` executor will have. The executor is used to send WAN events to the endpoints and ideally you want to have one thread per endpoint. If this property is omitted and you have specified the `endpoints` property, this will be the case. If necessary you can manually define the number of threads that the executor will use. Once the executor has been initialized there is thread affinity between the discovered endpoints and the executor threads - all events for a single endpoint will go through a single executor thread, preserving event order. It is important to determine which number of executor threads is a good value. Failure to do so can lead to performance issues - either contention on a too small number of threads or wasted threads that will not be performing any work. 

And the following is the equivalent programmatic configuration snippet:

[source,java]
----
include::{javasource}/wan/SampleWANReplicationConfiguration.java[tag=wrc]
----

Using this configuration, the cluster running in New York will replicate to Tokyo and London. The Tokyo and London clusters should
have similar configurations if you want to run in Active-Active mode.

If the New York and London cluster configurations contain the `wan-replication` element and the Tokyo cluster does not, it means
New York and London are active endpoints and Tokyo is a passive endpoint.

===== Defining WAN Replication Using Discovery SPI

In addition to defining target cluster endpoints with static IP addresses, you can configure WAN to work with the discovery SPI and determine the endpoint IP addresses at runtime. This allows you to use WAN with endpoints on various cloud infrastructures (such as Amazon EC2 or GCP Compute) where the IP address is not known in advance. Typically you will use a readily available discovery SPI plugin such as https://github.com/hazelcast/hazelcast-aws[Hazelcast AWS EC2 discovery plugin], https://github.com/hazelcast/hazelcast-gcp[Hazelcast GCP discovery plugin], or similar. For more advanced cases, you can provide your own discovery SPI implementation with custom logic for determining the WAN target endpoints such as looking up the endpoints in some service registry or even reading the endpoint addresses from a file.

NOTE: When using the discovery SPI, WAN will always connect to the public address of the members returned by the discovery SPI implementation. This is opposite to the cluster membership mechanism using the discovery SPI where a member connects to a different member in the same cluster through its private address. Should you prefer for WAN to use the private address of the discovered member as well, please use the `discovery.useEndpointPrivateAddress` publisher property (see below).

 
Following is an example of setting up the WAN replication with the EC2 discovery plugin. You must have the https://github.com/hazelcast/hazelcast-aws[Hazelcast AWS EC2 discovery plugin] on the classpath.

[source,xml]
----
<hazelcast>
...
  <wan-replication name="my-wan-cluster-batch">
      <wan-publisher group-name="london">
          <class-name>com.hazelcast.enterprise.wan.replication.WanBatchReplication</class-name>
          <queue-full-behavior>THROW_EXCEPTION</queue-full-behavior>
          <queue-capacity>1000</queue-capacity>
          <properties>
              <property name="batch.size">500</property>
              <property name="batch.max.delay.millis">1000</property>
              <property name="snapshot.enabled">false</property>
              <property name="response.timeout.millis">60000</property>
              <property name="ack.type">ACK_ON_OPERATION_COMPLETE</property>
              <property name="group.password">london-pass</property>
              <property name="discovery.period">20</property>
              <property name="maxEndpoints">5</property>
              <property name="executorThreadCount">5</property>
          </properties>
          <discovery-strategies>
              <discovery-strategy enabled="true" class="com.hazelcast.aws.AwsDiscoveryStrategy">
                  <properties>
                      <property name="access-key">test-access-key</property>
                      <property name="secret-key">test-secret-key</property>
                      <property name="region">test-region</property>
                      <property name="iam-role">test-iam-role</property>
                      <property name="host-header">ec2.test-host-header</property>
                      <property name="security-group-name">test-security-group-name</property>
                      <property name="tag-key">test-tag-key</property>
                      <property name="tag-value">test-tag-value</property>
                      <property name="connection-timeout-seconds">10</property>
                      <property name="hz-port">5701-5708</property>
                  </properties>
              </discovery-strategy>
          </discovery-strategies>
      </wan-publisher>
  </wan-replication>
...
</hazelcast>
----

The `hz-port` property defines the port or the port range on which the target endpoint is running. The default port range 5701-5708 is used if this property is not defined. This is needed because the Amazon API which the AWS plugin uses does not provide the port on which Hazelcast is running, only the IP address. For some other discovery SPI implementations, this might not be necessary and it might discover the port as well, e.g., by looking up in a service registry.

The other properties are the same as when using the `aws` element. In case of EC2 discovery you can configure the WAN replication using the `aws` element. You may use either of these, but not both at the same time.

[source,xml]
----
<hazelcast>
...
  <wan-replication name="my-wan-cluster-batch">
      <wan-publisher group-name="london">
          <class-name>com.hazelcast.enterprise.wan.replication.WanBatchReplication</class-name>
          <queue-full-behavior>THROW_EXCEPTION</queue-full-behavior>
          <queue-capacity>1000</queue-capacity>
          <properties>
              <property name="batch.size">500</property>
              <property name="batch.max.delay.millis">1000</property>
              <property name="snapshot.enabled">false</property>
              <property name="response.timeout.millis">60000</property>
              <property name="ack.type">ACK_ON_OPERATION_COMPLETE</property>
              <property name="group.password">london-pass</property>
              <property name="discovery.period">20</property>
              <property name="discovery.useEndpointPrivateAddress">false</property>
              <property name="maxEndpoints">5</property>
              <property name="executorThreadCount">5</property>
          </properties>
          <aws enabled="true">
              <access-key>my-access-key</access-key>
              <secret-key>my-secret-key</secret-key>
              <iam-role>dummy</iam-role>
              <region>us-west-1</region>
              <host-header>ec2.amazonaws.com</host-header>
              <security-group-name>hazelcast-sg</security-group-name>
              <tag-key>type</tag-key>
              <tag-value>hz-members</tag-value>
          </aws>
      </wan-publisher>
  </wan-replication>
...
</hazelcast>
----

You can refer to the <<aws-element, aws element>> and the <<configuring-client-for-aws, Configuring Client for AWS>> sections for the descriptions of above AWS configuration elements. Following are the definitions of additional configuration properties:

- `discovery.period`: Period in seconds in which WAN tries to discover new endpoints and reestablish connections to failed endpoints. Default is 10 seconds.
- `maxEndpoints`: Maximum number of endpoints that WAN will connect to when using a discovery mechanism to define endpoints. Default is `Integer.MAX_VALUE`. This property has no effect when static endpoint IPs are defined using the `endpoints` property.
- `executorThreadCount`: Number of threads that the `WanBatchReplication` executor will have. The executor is used to send WAN events to the endpoints and ideally you want to have one thread per endpoint. If this property is omitted and you have specified the `endpoints` property, this will be the case. If, on the other hand, you are using WAN with the discovery SPI and you have not specified this property, the executor will be sized to the initial number of discovered endpoints. This can lead to performance issues if the number of endpoints changes in the future - either contention on a too small number of threads or wasted threads that will not be performing any work. To prevent this you can manually define the executor thread count. Once the executor has been initialized there is thread affinity between the discovered endpoints and the executor threads - all events for a single endpoint will go through a single executor thread, preserving event order.
- `discovery.useEndpointPrivateAddress`: Determines whether the WAN connection manager should connect to the endpoint on the private address returned by the discovery SPI. By default this property is `false` which means the WAN connection manager will always use the public address.

You can also define the WAN publisher with discovery SPI using the programmatic configuration: 

[source,java]
----
include::{javasource}/wan/SampleWANReplicationDiscoveryConfiguration.java[tag=wrdc]
----


==== WanBatchReplication Implementation

Hazelcast offers `WanBatchReplication` implementation for the WAN replication.

As you see in the above configuration examples, this implementation is configured using the `class-name` element (in the declarative configuration) or the method `setClassName` (in the programmatic configuration).

The implementation `WanBatchReplication` waits until:

- a pre-defined number of replication events are generated, (please refer to the <<batch-size, Batch Size section>>).
- or a pre-defined amount of time is passed (please refer to the <<batch-maximum-delay, Batch Maximum Delay section>>).

NOTE: `WanNoDelayReplication` implementation has been removed. You can still achieve this behavior by setting the batch size to `1` while configuring your WAN replication.


==== Configuring WAN Replication for IMap and ICache

Yon can configure the WAN replication for Hazelcast's IMap and ICache data structures. To enable WAN replication for an IMap or ICache instance, you can use the `wan-replication-ref` element. Each IMap and ICache instance can have different WAN replication configurations.

**Enabling WAN Replication for IMap:**

Imagine you have different distributed maps, however only one of those maps should be replicated to a target cluster. To achieve this, configure the map that you want
replicated by adding the `wan-replication-ref` element in the map configuration as shown below.

[source,xml]
----
<hazelcast>
  <wan-replication name="my-wan-cluster">
    ...
  </wan-replication>
  <map name="my-shared-map">
    <wan-replication-ref name="my-wan-cluster">
       <merge-policy>com.hazelcast.map.merge.PassThroughMergePolicy</merge-policy>
       <republishing-enabled>false</republishing-enabled>
    </wan-replication-ref>
  </map>
  ...
</hazelcast>
----

The following is the equivalent programmatic configuration:

[source,java]
----
include::{javasource}/wan/EnablingWRforMap.java[tag=wrmap]
----

You see that we have `my-shared-map` configured to replicate itself to the cluster targets defined in the earlier
`wan-replication` element.

`wan-replication-ref` has the following elements;

- `name`: Name of `wan-replication` configuration. IMap or ICache instance uses this `wan-replication` configuration. 
- `merge-policy`: Resolve conflicts that are occurred when target cluster already has the replicated entry key.
- `republishing-enabled`: When enabled, an incoming event to a member is forwarded to target cluster of that member. Enabling the event republishing is useful in a scenario where cluster A replicates to cluster B and cluster B replicates to cluster C. You do not need to enable republishing when all your clusters replicate to each other. 

When using Active-Active Replication, multiple clusters can simultaneously update the same entry in a distributed data structure.
You can configure a merge policy to resolve these potential conflicts, as shown in the above example configuration (using the `merge-policy` sub-element under the `wan-replication-ref` element).

Hazelcast provides the following merge policies for IMap:

- `com.hazelcast.map.merge.PutIfAbsentMapMergePolicy`: Incoming entry merges from the source map to the target map if it does not exist in the target map.
- `com.hazelcast.map.merge.HigherHitsMapMergePolicy`: Incoming entry merges from the source map to the target map if the source entry has more hits than the target one.
- `com.hazelcast.map.merge.PassThroughMergePolicy`: Incoming entry merges from the source map to the target map unless the incoming entry is not null.
- `com.hazelcast.map.merge.LatestUpdateMapMergePolicy`: Incoming entry merges from the source map to the target map if the source entry has been updated more recently than the target entry. Please note that this merge policy can only be used when the clusters' clocks are in sync.

NOTE When using WAN replication, please note that only key based events are replicated to the target cluster. Operations like `clear`, `destroy` and `evictAll` are NOT replicated.


**Enabling WAN Replication for ICache:**

The following is a declarative configuration example for enabling WAN Replication for ICache:

[source,xml]
----
<wan-replication name="my-wan-cluster">
   ...
</wan-replication>
<cache name="my-shared-cache">
   <wan-replication-ref name="my-wan-cluster">
      <merge-policy>com.hazelcast.cache.merge.PassThroughCacheMergePolicy</merge-policy>
      <republishing-enabled>true</republishing-enabled>
   </wan-replication-ref>
</cache>
----

The following is the equivalent programmatic configuration:


[source,java]
----
include::{javasource}/wan/EnablingWRforCache.java[tag=wrcache]
----


NOTE: Caches that are created dynamically do not support WAN replication functionality. Cache configurations should be defined either declaratively (by XML) or programmatically on both source and target clusters.


Hazelcast provides the following merge policies for ICache:

- `com.hazelcast.cache.merge.HigherHitsCacheMergePolicy`: Incoming entry merges from the source cache to the target cache if the source entry has more hits than the target one.
- `com.hazelcast.cache.merge.PassThroughCacheMergePolicy`: Incoming entry merges from the source cache to the target cache unless the incoming entry is not null.


==== Batch Size

The maximum size of events that are sent in a single batch can be changed 
depending on your needs. Default value for batch size is `500`.

Batch size can be set for each target cluster by modifying related `WanPublisherConfig`.

Below is the declarative configuration for changing the value of the property:

[source,xml]
----
...
 <wan-replication name="my-wan-cluster">
    <wan-publisher group-name="london">
        ...
        <properties>
            ...
            <property name="batch.size">1000</property>
            ...
        </properties>
        ...
    </wan-publisher>
 </wan-replication>
...
----

And, following is the equivalent programmatic configuration:

[source,java]
----
...
 WanReplicationConfig wanConfig = config.getWanReplicationConfig("my-wan-cluster");
 WanPublisherConfig publisherConfig = new WanPublisherConfig();
 ...
 Map<String, Comparable> props = publisherConfig.getProperties();
 props.put("batch.size", 1000);
 wanConfig.addWanPublisherConfig(publisherConfig);
...
----

NOTE: `WanNoDelayReplication` implementation has been removed. You can still achieve this behavior by setting the batch size to `1` while configuring your WAN replication.


==== Batch Maximum Delay

When using `WanBatchReplication` if the number of WAN replication events generated does not reach <<batch-size, Batch Size>>,
they are sent to the target cluster after a certain amount of time is passed. You can set this duration in milliseconds using this batch maximum delay configuration. Default value of for this duration is 1 second (1000 milliseconds).

Maximum delay can be set for each target cluster by modifying related `WanPublisherConfig`.

You can change this property using the declarative configuration as shown below.

[source,xml]
----
...
 <wan-replication name="my-wan-cluster">
    <wan-publisher group-name="london">
        ...
        <properties>
            ...
            <property name="batch.max.delay.millis">2000</property>
            ... 
        </properties>
        ...
    </wan-publisher>
 </wan-replication>
...
----

And, the following is the equivalent programmatic configuration:

[source,java]
----
...
 WanReplicationConfig wanConfig = config.getWanReplicationConfig("my-wan-cluster");
 WanPublisherConfig publisherConfig = new WanPublisherConfig();
 ...
 Map<String, Comparable> props = publisherConfig.getProperties();
 props.put("batch.max.delay.millis", 2000);
 wanConfig.addWanPublisherConfig(publisherConfig);
...
----


==== Response Timeout

After a replication event is sent to the target cluster, the source member waits for an acknowledgement of the delivery of the event to the target.
If the confirmation is not received inside a timeout duration window, the event is resent to the target cluster. Default value of this duration is `60000` milliseconds.

You can change this duration depending on your network latency for each target cluster by modifying related `WanPublisherConfig`.

Below is an example of declarative configuration:

[source,xml]
----
...
  <wan-replication name="my-wan-cluster">
    <wan-publisher group-name="london">
      <properties>
        <property name="response.timeout.millis">5000</property>
      </properties>
    </wan-publisher>
  </wan-replication>
...
----

And, the following is the equivalent programmatic configuration:


[source,java]
----
...
 WanReplicationConfig wanConfig = config.getWanReplicationConfig("my-wan-cluster");
 WanPublisherConfig publisherConfig = new WanPublisherConfig();
 ...
 Map<String, Comparable> props = publisherConfig.getProperties();
 props.put("response.timeout.millis", 5000);
 wanConfig.addWanPublisherConfig(publisherConfig);
...
----


==== Queue Capacity

For huge clusters or high data mutation rates, you might need to increase the replication queue size. The default queue
size for replication queues is `10000`. This means, if you have heavy put/update/remove rates, you might exceed the queue size
so that the oldest, not yet replicated, updates might get lost. Note that a separate queue is used for each WAN Replication configured for IMap and ICache.
 
Queue capacity can be set for each target cluster by modifying related `WanPublisherConfig`.

You can change this property using the declarative configuration as shown below.

[source,xml]
----
...
 <wan-replication name="my-wan-cluster">
    <wan-publisher group-name="london">
        ...
        <queue-capacity>15000</queue-capacity>
        ...
    </target-cluster>
 </wan-replication>
...
----

And, the following is the equivalent programmatic configuration:

[source,java]
----
...
 WanReplicationConfig wanConfig = config.getWanReplicationConfig("my-wan-cluster");
 WanPublisherConfig publisherConfig = new WanPublisherConfig();
 ...
 publisherConfig.setQueueCapacity(15000);
 wanConfig.addWanPublisherConfig(publisherConfig);
...
----

Note that you can clear a member's WAN replication event queue. It can be initiated through Management Center's https://docs.hazelcast.org/docs/management-center/latest/manual/html/index.html#monitoring-wan-replication[Clear Queues action] or Hazelcast’s REST API. Below is the URL for its REST call:

```
http://member_ip:port/hazelcast/rest/mancenter/wan/clearWanQueues
```

This may be useful, for instance, to release the consumed heap if you know that the target cluster is being shut down, decommissioned, put out of use and it will never come back. Or, when a failure happens and queues are not replicated anymore, you could clear the queues using this clearing action.

==== Queue Full Behavior

In the previous Hazelcast releases, WAN replication was dropping the new events if WAN replication event queues are full.
This behavior is configurable starting with Hazelcast 3.6. 

There are following supported behaviors:
 
- `DISCARD_AFTER_MUTATION`: If you select this option, the new WAN events generated by the member are dropped and not replicated to the target cluster
when the WAN event queues are full.   
- `THROW_EXCEPTION`: If you select this option, the WAN queue size is checked before each supported mutating operation (like `IMap.put()`, `ICache.put()`).
If one the queues of target cluster is full, `WANReplicationQueueFullException` is thrown and the operation is not allowed.
- `THROW_EXCEPTION_ONLY_IF_REPLICATION_ACTIVE`: Its effect is similar to that of `THROW_EXCEPTION`. But, it  throws exception only when WAN replication is active. It discards the new events if WAN replication is stopped.


The following is an example configuration:

[source,xml]
----
<wan-replication name="my-wan-cluster">
  <wan-publisher group-name="test-cluster-1">
    ...
    <queue-full-behavior>DISCARD_AFTER_MUTATION</queue-full-behavior>
  </wan-publisher>
</wan-replication>
----

NOTE: `queue-full-behavior` configuration is optional. Its default value is `DISCARD_AFTER_MUTATION`.


==== Event Filtering API

Starting with Hazelcast 3.6, WAN replication allows you to intercept WAN replication events before they are placed to
WAN event replication queues by providing a filtering API. Using this API, you can monitor WAN replication events of each data structure
separately.

You can attach filters to your data structures using  `filter` property of `wan-replication-ref` configuration inside `hazelcast.xml` as shown in the following example configuration. You can also configure it using the programmatic configuration.

[source,xml]
----
<hazelcast>
  <map name="testMap">
    <wan-replication-ref name="test">
      <filters>
        <filter-impl>com.example.SampleFilter</filter-impl>
        <filter-impl>com.example.SampleFilter2</filter-impl>
      </filters>
    </wan-replication-ref>
  </map>
</hazelcast>
----

As shown in the above configuration, you can define more than one filter. Filters are called in the order that they are introduced.
A WAN replication event is only eligible to publish if it passes all the filters.

Map and Cache have different filter interfaces: `MapWanEventFilter` and `CacheWanEventFilter`. Both of these interfaces have the method `filter` which takes the following parameters:

- `mapName`/`cacheName`: Name of the related data structure.
- `entryView`: https://docs.hazelcast.org/docs/latest/javadoc/com/hazelcast/core/EntryView.html[EntryView]
or https://docs.hazelcast.org/docs/latest/javadoc/com/hazelcast/cache/CacheEntryView.html[CacheEntryView] depending on the data structure.
- `eventType`: Enum type - `UPDATED(1)`, `REMOVED(2)` or `LOADED(3)` - depending on the event.

NOTE: `LOADED` events are filtered out and not replicated to target cluster.


==== Acknowledgment Types

Starting with Hazelcast 3.6, WAN replication supports different acknowledgment (ACK) types for each target cluster group.
You can choose from 2 different ACK type depending on your consistency requirements. The following ACK types are supported:
 
- `ACK_ON_RECEIPT`: A batch of replication events is considered successful as soon as it is received by the target cluster. This option does not guarantee that the received event is actually applied but it is faster.
- `ACK_ON_OPERATION_COMPLETE`: This option guarantees that the event is received by the target cluster and it is applied. It is more time consuming. But it is the best way if you have strong consistency requirements.

Following is an example configuration:

[source,xml]
----
<wan-replication name="my-wan-cluster">
  <wan-publisher group-name="test-cluster-1">
    ...
    <properties>
        <property name="ack.type">ACK_ON_OPERATION_COMPLETE</property>
    </properties>
  </wan-publisher>
</wan-replication>
----

NOTE: `ack.type` configuration is optional. Its default value is `ACK_ON_RECEIPT`.


==== Synchronizing WAN Target Cluster

Starting with Hazelcast 3.7 you can initiate a synchronization operation on an IMap for a specific target cluster. 
Synchronization operation sends all the data of an IMap to a target cluster to align the state of target IMap with source IMap.
Synchronization is useful if two remote clusters lost their synchronization due to WAN queue overflow or in restart scenarios.

Synchronization can be initiated through https://docs.hazelcast.org/docs/management-center/latest/manual/html/index.html#wan-sync[Management Center] and Hazelcast’s REST API. 

Below is the URL for the REST call;

```
http://member_ip:port/hazelcast/rest/mancenter/wan/sync/map
```

You need to add parameters to the request in the following order separated by "&";

* Name of the WAN replication configuration
* Target group name
* Map name to be synchronized

Assume that you have configured an IMap with a WAN replication configuration as follows:

[source,xml]
----
<wan-replication name="my-wan-cluster">
      <wan-publisher group-name="istanbul">
          <class-name>com.hazelcast.enterprise.wan.replication.WanBatchReplication</class-name>
            ...
      </wan-publisher>
<wan-replication>
...
<map name="my-map">
    <wan-replication-ref name="my-wan-cluster">
       <merge-policy>com.hazelcast.map.merge.PassThroughMergePolicy</merge-policy>
    </wan-replication-ref>
</map>
----

Then, a sample CURL command to initiate the synchronization for "my-map" would be as follows:

```
curl -H "Content-type: text/plain" -X POST -d "my-wan-cluster&istanbul&my-map" --URL http://127.0.0.1:5701/hazelcast/rest/mancenter/wan/sync/map
```

[NOTE]
.Synchronizing All Maps
====

You can also use the following URL in your REST call if you want to synchronize all the maps in source and target cluster:

`http://member_ip:port/hazelcast/rest/mancenter/wan/sync/allmaps`
====

NOTE: Synchronization for a target cluster operates only with the data residing in the memory. Therefore, evicted entries will not be
synchronized, not even if `MapLoader` is configured.


==== WAN Replication Failure Detection and Recovery

The failure detection and recovery mechanisms in WAN handle failures during WAN replication and they closely interact with the list of endpoints that WAN is replicating to. There might be some small differences when using static endpoints or the discovery SPI but here we will outline the general mechanism of failure detection and recovery.

===== WAN Target Endpoint List
 
The WAN connection manager maintains a list of public addresses that it can replicate to at any moment. This list may change over time as failures are detected or as new addresses are discovered when using the discovery SPI. The connection manager does not eagerly create connections to these addresses as they are added to the list to avoid overloading the endpoint with connections from all members using the same configuration. It will try and connect to the endpoint just before WAN events are about to be transmitted. This means that if there are no updates on the map or cache using WAN replication, there will be no WAN events and the connection will not be established to the endpoint.

When more than one endpoint is configured, traffic will be load balanced between them using the partition, so that the same partitions are always sent to the same target member, ensuring ordering by partition.
 
===== WAN Failure Detection
 
If using the Hazelcast IMDG Enterprise edition class `WanBatchReplication` (see the <<defining-wan-replication, Defining WAN replication section>>), the WAN replication will catch any exceptions when sending the WAN events to the endpoint. In the case of an exception, the endpoint will be removed from the endpoint list to which WAN replicates and the WAN events will be resent to a different address. The replication will be retried until it is successful.

===== WAN Endpoint Recovery

Periodically the WAN connection manager will try and "rediscover" new endpoints. The period is 10 seconds by default but configurable with the `discovery.period` property (see the <<defining-wan-replication, Defining WAN replication section>>).

The discovered endpoints depend on the configuration used to define WAN replication. If using static WAN endpoints (by using the `endpoints` property), the discovered endpoints are always the same and are equal to the values defined in the configuration. If using discovery SPI with WAN, the discovered endpoints may be different each time.

When the discovery returns a list of endpoints (addresses), the WAN target endpoint list is updated. Newly discovered endpoints are added and endpoints which are no longer in the discovered list are removed. Newly discovered endpoints may include addresses to which WAN replication has previously failed. This means that once a new WAN event is about to be sent, a connection will be reestablished to the previously failed endpoint and WAN replication will be retried. The endpoint can later be again removed from the target endpoint list if the replication again encounters failure.

==== WAN Replication Additional Information

Each cluster in WAN topology has to have a unique `group-name` property for a proper handling of forwarded events.
 
Starting with Hazelcast 3.6, WAN replication backs up its event queues to other members to prevent event loss in case of member failures.
WAN replication's backup mechanism depends on the related data structures' backup operations. Note that, WAN replication is supported for IMap and ICache.
That means, as far as you set a backup count for your IMap or ICache instances, WAN replication events generated by these instances are also replicated.
 
There is no additional configuration to enable/disable WAN replication event backups.

=== Delta WAN Synchronization

Hazelcast clusters connected over WAN can go out-of-sync because of various reasons such as member failures and concurrent updates. To overcome the out-of-sync issue, Hazelcast has the default <<synchronizing-wan-target-cluster, WAN synchronization>> feature, through which the maps in different clusters are synced by transferring all entries from the source to the target cluster. This may be not efficient since some of the entries have remained unchanged on both clusters and do not require to be transferred. Also, for the entries to be transferred, they need to be copied to on-heap on the source cluster. This may cause spikes in the heap usage, especially if using large off-heap stores.

Besides the default WAN synchronization, Hazelcast provides Delta WAN Synchronization which uses https://en.wikipedia.org/wiki/Merkle_tree[Merkle tree] for the same purpose. It is a data structure used for efficient comparison of the difference in the contents of large data structures. The precision of this comparison is defined by Merkle tree's depth. Merkle tree hash exchanges can detect inconsistencies in the map data and synchronize only the different entries when using WAN synchronization, instead of sending all the map entries.

NOTE: Currently, Delta WAN Synchronization is implemented only for Hazelcast IMap. It will also be implemented for ICache in the future releases.

==== Requirements

To be able to use Delta WAN synchronization, the following must be met:

* Source and target cluster versions must be at least Hazelcast 3.11.
* Both clusters must have the same number of partitions.
* Both clusters must use the same partitioning strategy.
* Both clusters must have the Merkle tree structure enabled.


==== Using Delta WAN Synchronization

To be able to use Delta WAN synchronization for a Hazelcast data structure: 

. Configure the WAN synchronization mechanism for your WAN publisher so that it uses the Merkle tree: If configuring declaratively, you can use the `consistency-check-strategy` sub-element of the `wan-sync` element. If configuring programmatically, you can use the setter of the https://docs.hazelcast.org/docs/3.11/javadoc/com/hazelcast/config/WanSyncConfig.html[`WanSyncConfig`] object.
. Bind that WAN synchronization configuration to the data structure (currently IMap): Simply set the WAN replication reference of your map to the name of the WAN replication configuration which uses the Merkle tree.

Following is a declarative configuration example of the above:

[source,xml]
----
<map name="myMap">
    <wan-replication-ref name="wanReplicationScheme">
        ...
    </wan-replication-ref>
    ...
</map>
...
...
<wan-replication name="wanReplicationScheme">
    <wan-publisher group-name="groupName">
        <class-name>...</class-name>
        <wan-sync>
            <consistency-check-strategy>MERKLE_TREES</consistency-check-strategy>
        </wan-sync>
        ...
    </wan-publisher>
</wan-replication>
----

Here, the element `consistency-check-strategy` sets the strategy for checking the consistency of data between the source and target clusters. You must initiate the WAN synchronization (via Management Center or REST API as explained in <<synchronizing-wan-target-cluster, Synchronizing WAN clusters>>) to let this strategy reconcile the inconsistencies. The element `consistency-check-strategy` has currently two values:

* `NONE`: Means that there will be no consistency checks. This is the default value.
* `MERKLE_TREES`: Means that WAN synchronization will use Merkle tree structure.

==== Configuring Delta WAN Synchronization

You can configure Delta WAN Synchronization declaratively using the `merkle-tree` element or programmatically using the https://docs.hazelcast.org/docs/3.11/javadoc/com/hazelcast/config/MerkleTreeConfig.html[`MerkleTreeConfig`] object.

Following is a declarative configuration example showing how to enable Delta WAN Synchronization, bind it to a Hazelcast data structure (an IMap in the below case) and specify its depth.

[source,xml]
----
<merkle-tree enabled="true">
    <mapName>someMap</mapName>
    <depth>5</depth>
</merkle-tree>
----

Here are the descriptions of sub-elements and attributes:

* `enabled`: Specifies whether the Merkle tree structure is enabled. Its default value is `true`.
* `mapName`: Specifies the name of the map for which the Merkle tree structure is used.
* `depth`: Specifies the depth of Merkle tree. Valid values are between 2 and 27 (exclusive). Its default value is `10`.
** A larger depth means that a data synchronization mechanism is able to pinpoint a smaller subset of the data structure (e.g., IMap) contents in which a change has occurred. This causes the synchronization mechanism to be more efficient. However, keep in mind that a large depth means that the Merkle tree will consume more memory. As the comparison mechanism is iterative, a larger depth also prolongs the comparison duration. Therefore, it is recommended not to have large tree depths if the latency of the comparison operation is high. 
** A smaller depth means that the Merkle tree is shallower and the data synchronization mechanism transfers larger chunks of the data structure (e.g., IMap) in which a possible change has happened. As you can imagine, a shallower Merkle tree will consume less memory.

Following is a declarative example including the Merkle tree configuration.

[source,xml]
----
<map name="myMap">
    <wan-replication-ref name="wanReplicationScheme">
        ...
    </wan-replication-ref>
    ...
</map>
...
<merkle-tree enabled="true">
    <mapName>myMap</mapName>
    <depth>10</depth>
</merkle-tree>
...
<wan-replication name="wanReplicationScheme">
    <wan-publisher group-name="groupName">
        <class-name>...</class-name>
        <wan-sync>
            <consistency-check-strategy>MERKLE_TREES</consistency-check-strategy>
        </wan-sync>
        ...
    </wan-publisher>
</wan-replication>
----


NOTE: If you do not specifically configure the `merkle-tree` in your Hazelcast configuration, Hazelcast uses the default Merkle tree structure values (i.e., it is enabled by default and its default depth is 10) when there is a WAN publisher using the Merkle tree (i.e., `consistency-check-strategy` for a WAN replication configuration is set as `MERKLE_TREES` and there is a data structure using that WAN replication configuration).


NOTE: Merkle trees are created for each partition holding IMap data. Therefore, increasing the partition count also
increases the efficiency of the Delta WAN Synchronization.

==== The Process

Synchronizing the maps based on Merkle trees consists of two phases:

1. _Consistency check_: Process of exchanging and comparing the hashes stored in the Merkle tree structures in the
source and target clusters. The check starts with the root node and continues recursively with the children with different
hash codes. Both sides send the children of the nodes that the other side sent, hence the comparison is done by `depth/2`
steps. After this check, the tree leaves holding different entries are identified.
2. _Synchronization_: Process of transferring the entries belong to the leaves identified by the _consistency
check_ from the source to target cluster. On the target cluster the configured merge policy is applied for each entry that
is in both the source and target clusters.

NOTE: If you only need the differences between the clusters, you can trigger the consistency check without performing
synchronization.

==== Memory Consumption

Since Merkle trees are built for each partition and each map, the memory overhead of the trees with high entry count and deep
trees can be significant. The trees are maintained on-heap, therefore - besides the memory consumption - garbage collection could be another
concern. Make sure the configuration is tested with realistic data size before deployed in production.

The table below shows a few examples for what the memory overhead could be.

.Merkle trees memory overhead for a member
|===
|Entries Stored |Partitions Owned |Entries per Leaf |Depth |Memory Overhead

|1M
|271
|7
|10
|57 MB

|1M
|271
|1
|13
|97 MB

|10M
|271
|72
|10
|412 MB

|10M
|271
|9
|13
|453 MB

|10M
|5001
|4
|10
|577 MB

|10M
|5001
|1
|12
|899 MB

|25M
|5001
|10
|10
|1983 MB

|25M
|5001
|1
|13
|2735 MB

|===

==== Defining the Depth

The efficiency of the Delta WAN Synchronization (WAN synchronization based on Merkle trees) is determined by the average number of entries per the tree
leaves that is proportionate to the number of entries in the map. The bigger this average the more entries are getting
synchronized for the same difference. Raising the depth decreases this average at the cost of increasing the memory overhead.

This average can be calculated for a map as `avgEntriesPerLeaf = mapEntryCount / totalLeafCount`, where `totalLeafCount =
partitionCount * 2^depth-1^`. The ideal value is 1, however this may come at significant memory overhead as shown in the
table above.

In order to specify the tree depth, a trade-off between memory consumption and effectiveness might be needed.

Even if the map is huge and the Merkle trees are configured to be relatively shallow, the Merkle tree based synchronization
may be leveraged if only a small subset of the whole map is expected to be synchronized. The table below illustrates the
efficiency of the Merkle tree based synchronization compared to the default synchronization mechanism.


.Efficiency examples
|===
|Map entry count |Difference count |Depth |Memory consumption |Avg entries / leaf |Entries synced |Efficiency

|10M
|5M
|11
|684 MB
|2
|10M
|0%

|10M
|5M
|12
|899 MB
|1
|5M
|100%

|10M
|1M
|10
|577 MB
|4
|4M
|150%

|10M
|10K
|8
|496 MB
|16
|160K
|6150%

|10M
|10K
|12
|899 MB
|1
|1K
|99900%

|===

As shown in the last two rows, the Merkle tree based synchronization transfers significantly less entries than what the
default mechanism does even with 8 deep trees. The efficiency with depth 12 is even better but consumes much more memory.

NOTE: The averages in the table are calculated with 5001 partitions.

NOTE: The average entries per leaf number above assumes perfect distribution of the entries amongst the leaves. Since this is
typically not true in real-life scenarios the efficiency can be slightly worse. The statistics section below describes how to
get the actual average for the leaves involved in the synchronization.

==== REST API

The two phases of the Merkle tree based synchronization can be triggered by a REST call, as it can be done with the
default synchronization.

The URL for the consistency check REST call:

```
http://member_ip:port/hazelcast/rest/mancenter/wan/consistencyCheck/map
```

The URL for the synchronization REST call - the same as it is for the default synchronization:

```
http://member_ip:port/hazelcast/rest/mancenter/wan/sync/map
```

You need to add parameters to the request in both cases in the following order separated by "&";

* Name of the WAN replication configuration
* Target group name
* Map name to be synchronized

NOTE: You can also use the following URL in your REST call if you want to synchronize all the maps in source and target cluster:
`http://member_ip:port/hazelcast/rest/mancenter/wan/sync/allmaps`

NOTE: Consistency check can be triggered only for one map.

==== Statistics

The consistency check and the synchronization both write statistics into the diagnostics subsystem and send it
 to the Management Center. The following reported fields can be used to reason about the efficiency of the configuration.

Consistency check reports the number of the

* Merkle tree nodes checked,
* Merkle tree nodes found to be different,
* entries needed to be synchronized to make the clusters consistent.

Synchronization reports the

* duration of the synchronization,
* number of the entries synchronized,
* average number of the entries per tree leaves in the synchronized leaves.

=== Hazelcast WAN Replication with Solace

image::Plugin_New.png[Solace Plugin, 84, 22]


This section explains how you can integrate Hazelcast's WAN replication with http://www.solacesystems.com/[Solace] messaging platform. With this integration, you can publish and consume WAN replication events to/from Solace appliances. 

Solace combines with Hazelcast to drive efficiencies in the processing of global data workloads. This joint solution enables multi-cloud and hybrid-cloud replication of Hazelcast clusters. Please see https://hazelcast.com/partner/solace/[Hazelcast-Solace partnership] for more information.
 
==== Enabling Integration
 
To publish and consume WAN replication events on Solace appliances, Hazelcast WAN replication has the following classes:
 
* `SolaceWanPublisher`
* `SolaceWanConsumer`
 
You can register these classes using the configuration elements `<wan-publisher>` and `<wan-consumer>` while configuring your WAN replication.
`SolaceWanPublisher` and `SolaceWanConsumer` are included in the `hazelcast-solace` library which can be found in following Maven coordinates:

[source,xml]
----
  <dependency>
    <groupId>com.hazelcast</groupId>
    <artifactId>hazelcast-solace</artifactId>
    <version>1.0.0</version>
  </dependency>
----
 
NOTE: The hazelcast-solace artifact is not available in central Maven repository, but in Hazelcast's repository on the CloudBees platform. Please refer to the <<installing-hazelcast-imdg-enterprise, Installing Hazelcast IMDG Enterprise section>> for information on how to configure this repository.
 
 
Please see the following sections for configuration details.
 
==== Configuring Publisher
 
Following is an example declarative configuration for the publisher side:
 
[source,xml]
----
<wan-replication name="AtoB">
    <wan-publisher group-name="clusterB">
        <class-name>com.hazelcast.enterprise.wan.solace.SolaceWanPublisher</class-name>
        <properties>
            <property name="host">192.168.2.66</property>
            <property name="vpn.name">YOUR_VPN_NAME</property>
            <property name="username">admin</property>
            <property name="password">YOUR_PASSWORD</property>
            <property name="topic.base.name">BaseTopic</property>
            <property name="queue.name">Q/hz/clusterA</property>
            <property name="initial.queue.mapping.enabled">true</property>
        </properties>
    </wan-publisher>
</wan-replication>
----
 
Descriptions of the properties are as follows:
 
* `class-name`: Full class name of Solace WAN publisher, i.e., `com.hazelcast.enterprise.wan.solace.SolaceWanPublisher`
* `host`: IP address of the Solace host machine. It can be in the format "IP address:Port number". It is a mandatory property.
* `vpn.name`: Name of the Solace VPN. It is an optional property.
* `username`: Username for the Solace host. It is a mandatory property.
* `password`: Password for the Solace host. It is an optional property.
* `topic.base.name`: Base topic name to use while publishing events to Solace appliance. If not defined, the members will publish to topics using the format "T/hz/[cluster-group-name]/partitionId". If defined, the format "[topic.base.name]/partitionId" will be used. It is an optional property.
* `queue.name`: Name of the queue to be used in topic-to-queue mapping. This property is only valid if the property `initial.queue.mapping.enabled` is set to "true". Default queue name is "Q/hz/[cluster-group-name].
* `initial.queue.mapping.enabled`: Decides if a default topic-to-queue mapping should be performed on the publisher side. When enabled, it tries to provide a queue, whose name is defined by the property `queue.name`, and all topics that are generated by this instance is mapped to this queue. When disabled, you should perform this mapping manually. It is enabled by default.
 
==== Configuring Consumer
 
Following is an example declarative configuration for the consumer side:
 
[source,xml]
----
<wan-replication name="AtoB">
    <wan-consumer>
        <class-name>com.hazelcast.enterprise.wan.solace.SolaceWanConsumer</class-name>
        <persist-wan-replicated-data>false</persist-wan-replicated-data>
        <properties>
            <property name="host">192.168.2.66</property>
            <property name="vpn.name">YOUR_VPN_NAME</property>
            <property name="username">admin</property>
            <property name="password">YOUR_PASSWORD</property>
            <property name="queue.name">Q/hz/clusterA</property>
        </properties>
    </wan-consumer>
</wan-replication>
----
 
Descriptions of the properties are as follows:
 
* `class-name`: Full class name of Solace WAN consumer, i.e., `com.hazelcast.enterprise.wan.solace.SolaceWanConsumer`
* `persist-wan-replicated-data`:  When true, an incoming event over WAN replication can be persisted to a data store, otherwise it will not be persisted. Default value is false.
* `host`: IP address of the Solace host machine. It can be in the format "IP address:Port number". It is a mandatory property.
* `vpn.name`: Name of the Solace VPN. It is an optional property.
* `username`: Username for the Solace host. It is a mandatory property.
* `password`: Password for the Solace host. It is an optional property.
* `queue.name`: Name of the queue to be polled by the consumer. It is a mandatory property.
 
 
==== Additional Information
 
You can find code samples at https://github.com/hazelcast/hazelcast-code-samples/tree/master/enterprise/wan-replication/src/main/java/com/hazelcast/wan/solace[hazelcast-code-samples] repository.
 
NOTE: You can download the white paper **Hazelcast WAN Replication with Solace** from https://hazelcast.com/resources/hazelcast-wan-replication-solace/[Hazelcast.com].










